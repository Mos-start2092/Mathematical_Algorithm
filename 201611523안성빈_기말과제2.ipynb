{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수학적 알고리즘 기말과제 (2)\n",
    "## 201611523 안성빈 (통계학과)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과제의 목적\n",
    "- 숫자 손글씨 데이터 MNIST를 분류하는 딥러닝 모델을 생성한다.\n",
    "- CNN 기법을 사용한다.\n",
    "- Train data 로 모델을 훈련하여 Test data 모델의 성능을 비교한다.\n",
    "- 오차역전파에서 모멘텀을 사용한 경우와 SGD를 사용한 경우를 비교해본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.MNIST데이터 불러오기\n",
    "#### MNIST데이터는 숫자 손글씨를 28x28의 격자로 나눈 뒤 수치화한 데이터이다.\n",
    "#### Data_size는 총 10000개이다\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 이미지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행: 10000, 열: 784\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if(sys.version_info > (3,0)):\n",
    "    writemode='wb'\n",
    "else:\n",
    "    writemode='w'\n",
    "\n",
    "zipped_mnist=[f for f in os.listdir('./') if f.endswith('ubyte.gz')]\n",
    "for z in zipped_mnist:\n",
    "    with gzip.GzipFile(z,mode='rb') as decompressed, open(z[:-3],writemode) as outfile:\n",
    "        outfile.write(decompressed.read())\n",
    "\n",
    "        \n",
    "        \n",
    "def load_mnist_image(path, kind='train'):\n",
    "    \"\"\"`path`에서 MNIST 데이터 불러오기\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels.idx1-ubyte_' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images.idx3-ubyte_' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images = (images / 255.)  * 2\n",
    " \n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "X_data, y_data = load_mnist_image('', kind='t10k')\n",
    "print('행: %d, 열: %d' % (X_data.shape[0], X_data.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "fug,ax=plt.subplots(nrows=2,ncols=5,sharex=True,sharey=True)\n",
    "ax=ax.flatten()\n",
    "for i in range(10):\n",
    "    img=X_data[y_data==i][0].reshape(28,28)\n",
    "    ax[i].imshow(img,cmap=\"Greys\")\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN을 하기 위해서 (1,28,28)형태로 데이터를 불러오는 함수를 생성\n",
    "### 모델훈련을 위해 Target값을 One-Hot encoding 해주는 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"`path`에서 MNIST 데이터 불러오기\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels.idx1-ubyte_' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images.idx3-ubyte_' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels),1, 28,28)\n",
    "        \n",
    " \n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "def _change_one_hot_label(X):\n",
    "    T = np.zeros((X.size, 10))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "\n",
    "\n",
    "X_data, y_data = load_mnist('', kind='t10k')\n",
    "t_data=_change_one_hot_label(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,  38, 254, 109,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,  87, 252,  82,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0, 135, 241,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,  45, 244, 150,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,  84, 254,  63,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 202, 223,  11,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,  32, 254, 216,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,  95, 254, 195,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0, 140, 254,  77,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  57, 237, 205,   8,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0, 124, 255, 165,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0, 171, 254,  81,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          24, 232, 215,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         120, 254, 159,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         151, 254, 142,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         228, 254,  66,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  61,\n",
       "         251, 254,  66,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 141,\n",
       "         254, 205,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  10, 215,\n",
       "         254, 121,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   5, 198,\n",
       "         176,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0]]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 훈련을 위해 필요한 Class 및 함수 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기존의 신경망에서 필요한 함수 및 class생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu class\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):      # Relu함수의 forward 학습을 위한 함수생성\n",
    "        self.mask = (x <= 0)    \n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0     # self.mask를 인덱스로 활용하여 음수인 경우 0으로 값 대체\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):  # Relu함수의 backward 학습을 위한 함수생성\n",
    "        dout[self.mask] = 0    # forward 과정에서 음수였던 값들에 해당하는 인덱스(self.mask)의 업데이트는 0\n",
    "        dx = dout              \n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:                 # convolution 형태가 아닌 가중치 학습을 위한 class를 생성\n",
    "    def __init__(self, W, b): # 모델의 성능향상을 위해서 편향도 고려한 모델을 생성한다.\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)            #데이터를 재배열해준다\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b    # 가중치 곱\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # 가중치 및 편향 업데이트를 위한 값들을 생성\n",
    "        dx = np.dot(dout, self.W.T)              \n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):        # 소프트 맥스함수 생성\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):  # cross-entropy함수 생성\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    #원-핫 데이터를 정답 레이블로 전환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(One-Hot 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 합성곱 층을 위한 class 및 함수 생성 \n",
    "#### (이때 수업시간에 다루지 않은 패딩과 스트라이드라는 개념을 활용한다.)\n",
    "- 패딩: 출력목적을 위해 입력 데이터 주변을 특정 값으로 채운다 (수업시간 기준 패딩=0)\n",
    "- 스트라이드: 필터(Moving Window)를 적용하는 위치의 간격 (수업시간 기준 스트라이드=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward학습을 위해 이미지 -> 2차원 행렬로 전환해주는 im2col\n",
    "### backward 학습을 위해 행렬 -> 이미지로 전환해주는 col2im  함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.-> 역전파에 활용\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱을 위한 Convolution class생성\n",
    "- 수업시간과 다르게 모델의 성능향상을 위해 편향을 포함한 모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 중간 데이터（backward 시 사용）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 기울기\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape                          # 가중치의 shape저장\n",
    "        N, C, H, W = x.shape                                  #  입력 데이터의 shape저장\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride) # 합성곱 이후의 data size\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride) # 합성곱 이후의 data size\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)        #im2col로 데이터 전개\n",
    "        col_W = self.W.reshape(FN, -1).T                      # 필터도 계산을 위해 reshape\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b                     # 가중치 곱 + 편향\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)  # resahpe을 통해 최종 합성곱데이터 생성\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape                          \n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)        # 역전파를 위해 받아들이는 값을 연산을 위해 reshape한다\n",
    "        \n",
    "        # 가중치 및 편향을 업데이트하기 위한 값들을 받아준다.\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW) # 가중치의 형태에 맞게끔 변환\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)  # col2im을 통해 원래의 데이터 형태로 변환\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pooling을 위한 class 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):  # 해당 과제에서는 2,2로 pooling을 사용한다\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)  #pooling 이후의 data_size\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)  #pooling 이후의 data_size\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)  # 연산을 위해 im2col함수를 이용한 변환\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)               \n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)                                  # max값을 추출하는 pooling을 사용\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)       # reshape하여 데이터 형태를 적합시킴\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)                                \n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w                             # 역전파를 위한 pool_size생성\n",
    "        dmax = np.zeros((dout.size, pool_size))                           # 받아주는 행렬 생성\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten() # 역전파위한 값 생성\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,))                    # reshape을 통한 데이터 형태 변환\n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 역전파에 사용할 SGD class와 Momentum class생성\n",
    "- 학습률은 0.01로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "\n",
    "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "            \n",
    "class Momentum:\n",
    "\n",
    "    \"\"\"모멘텀 SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.CNN \n",
    "## 앞서 생성한 class 와 함수를 이용하여 CNN을 구현한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST -> 28x28）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트\n",
    "    output_size : 출력 크기（MNIST -> 10）\n",
    "    activation : 활성화 함수 - 'ReLU' \n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01） # 빠른 성능향상을 위한 정규화\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':20, 'filter_size':9, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화  # 표준편차를 0.01로 하였다\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        # 각 layer를 계층식으로 OrderDict()에 차례로 추가하여사용한다\n",
    "        # OrderDict() 는 순서가 있는 딕셔너리이다.\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss() # 마지막 SofrmaxWithLoss 계층은 따로 저장해둔다(계산 편리성 위해)\n",
    "\n",
    "###########################################################################################################\n",
    "########################################### Forward  ######################################################\n",
    "\n",
    "    def predict(self, x):                 \n",
    "        for layer in self.layers.values(): # layer에 추가한 순서대로 forward를 실행해 다음 계층에 전달\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수 \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):          # 정확도를 측정하는 함수 생성\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)                     # 정답이면 acc을 추가\n",
    "        \n",
    "        return acc / x.shape[0]                        # 최종 데이터 size로 나누어 정확도 측정\n",
    "\n",
    "################################################################################################\n",
    "######################################### Backward ##############################################\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # loss 값\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)    # 각 계층의 backward를 통한 값 추출\n",
    "\n",
    "        layers = list(self.layers.values())      # 기울기의 값을 list로 저장\n",
    "        layers.reverse()                         # 역전파(역순으로 진행)이기 때문에 역순으로 배열 \n",
    "        \n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)           \n",
    "\n",
    "        # 가중치와 편향 각각 역전파 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 신경망 훈련을 해주는 클래스 Trainer 생성\n",
    "### SGD와 Momentum 각각 10번 씩 총 20번의 train이 필요하여 class 를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
    "        Parameters\n",
    "        ----------\n",
    "        network : 훈련 모델\n",
    "        x_train : 훈련시에 사용되는 train의 feature data\n",
    "        t_train : 훈련시에 사용되는 정답(Target) data\n",
    "        x_test  : 훈련한 모델로 예측을 진행할 test의 feature data\n",
    "        t_test  : x_test의 결과와 비교할 실제 test의 정답data\n",
    "        epochs  : 총 데이터를 활용한 반복 횟수\n",
    "        mini_batch_size   : batch_size\n",
    "        optimizer : 역전파(최적화)에 사용하는 방법론 -> SGD와 Mometum이 있다.\n",
    "    \"\"\"\n",
    "    # 초기화\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
    "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
    "        self.network = network\n",
    "        self.verbose = verbose\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
    "\n",
    "        # optimzer- SGD or 모멘텀 선택\n",
    "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum}    \n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "        \n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size /mini_batch_size, 1)   # minibatch사이즈에 따른 훈련 반복수\n",
    "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)  # bsize만큼 사용할 index 랜덤추출\n",
    "        x_batch = self.x_train[batch_mask]                               # 해당 인덱스의 data추출\n",
    "        t_batch = self.t_train[batch_mask]                               # 해당 인덱스의 data추출\n",
    "        \n",
    "        grads = self.network.gradient(x_batch, t_batch)                  # batch_data를 훈련하여 가중치 get\n",
    "        self.optimizer.update(self.network.params, grads)                # 선택한 방법으로 역전파\n",
    "                                                                         # 모멘텀 or SGD\n",
    "        loss = self.network.loss(x_batch, t_batch)                       # train에 따른 loss\n",
    "        self.train_loss_list.append(loss)                                # train의 경향을 보기위해 loss list생성\n",
    "        if self.verbose: print(\"train loss:\" + str(loss))                 \n",
    "        \n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1                                      # train의 epoch을 알기위해 epoch을 count\n",
    "            \n",
    "            x_train_sample, t_train_sample = self.x_train, self.t_train \n",
    "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
    "            if not self.evaluate_sample_num_per_epoch is None:\n",
    "                t = self.evaluate_sample_num_per_epoch\n",
    "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
    "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
    "                                                                               \n",
    "            train_acc = self.network.accuracy(x_train_sample, t_train_sample) # train_acc 추출\n",
    "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)    # test_acc  추출\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):                                       #train을 진행\n",
    "            self.train_step()\n",
    "\n",
    "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.훈련을 통한 SGD 와 Momentum의 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련전에 데이터를 train과 test로 분리\n",
    "- sklearn의 train_test_split 함수를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (8000, 1, 28, 28)\n",
      "x_test.shape: (2000, 1, 28, 28)\n",
      "t_train.shape: (8000, 10)\n",
      "t_test.shape: (2000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,t_train,t_test=train_test_split(X_data,t_data,test_size=0.2,shuffle=True)\n",
    "print(\"x_train.shape:\",x_train.shape)\n",
    "print(\"x_test.shape:\",x_test.shape)\n",
    "print(\"t_train.shape:\",t_train.shape)\n",
    "print(\"t_test.shape:\",t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD를 활용한 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2711148347121686\n",
      "=== epoch:1, train acc:0.165, test acc:0.141 ===\n",
      "train loss:2.2173345693276088\n",
      "train loss:2.206564492807784\n",
      "train loss:2.2007742806066353\n",
      "train loss:2.171185598877772\n",
      "train loss:2.1663306827428115\n",
      "train loss:2.1803098941331376\n",
      "train loss:2.1668538769422923\n",
      "train loss:2.1237238196448938\n",
      "train loss:2.0120952930479787\n",
      "train loss:2.112880797012223\n",
      "train loss:1.9970524689408236\n",
      "train loss:2.017669479094069\n",
      "train loss:1.8846590738959401\n",
      "train loss:1.9038665744192493\n",
      "train loss:1.8891925165879864\n",
      "train loss:1.7793646764983004\n",
      "train loss:1.7891977411781148\n",
      "train loss:1.7841102927351742\n",
      "train loss:1.7108322982112885\n",
      "train loss:1.6717025105614578\n",
      "train loss:1.6372920323605769\n",
      "train loss:1.6795123641000012\n",
      "train loss:1.6022587529451722\n",
      "train loss:1.5685294994221732\n",
      "train loss:1.514360313190589\n",
      "train loss:1.4318529299826972\n",
      "train loss:1.366767383433854\n",
      "train loss:1.240025329114733\n",
      "train loss:1.2314084825015248\n",
      "train loss:1.203466150043496\n",
      "train loss:1.1010455987971133\n",
      "train loss:1.0867216175881764\n",
      "train loss:1.120617870663827\n",
      "train loss:0.9768811592508019\n",
      "train loss:0.9348142804160414\n",
      "train loss:0.9052108352668549\n",
      "train loss:0.8778487678669269\n",
      "train loss:0.7840293745444212\n",
      "train loss:0.9084213497611827\n",
      "train loss:0.9024798995820997\n",
      "train loss:0.8075888575128526\n",
      "train loss:0.766804862496794\n",
      "train loss:0.7560200377612951\n",
      "train loss:0.7243763668476407\n",
      "train loss:0.7699935519115351\n",
      "train loss:0.7610763418320151\n",
      "train loss:0.8753521945005059\n",
      "train loss:0.639300009802205\n",
      "train loss:0.7884811285967434\n",
      "train loss:0.7060789713742507\n",
      "train loss:0.6590814893076156\n",
      "train loss:0.5836448896211759\n",
      "train loss:0.5899415343951562\n",
      "train loss:0.6278460083688057\n",
      "train loss:0.6019772337137307\n",
      "train loss:0.6355670297464939\n",
      "train loss:0.42166335311520753\n",
      "train loss:0.5590622548188712\n",
      "train loss:0.5975978260362822\n",
      "train loss:0.546916200426537\n",
      "train loss:0.42942230029171247\n",
      "train loss:0.5634056214043137\n",
      "train loss:0.4099398603154416\n",
      "train loss:0.4731575366127888\n",
      "train loss:0.5300825339567212\n",
      "train loss:0.47130682005535285\n",
      "train loss:0.5349329904626505\n",
      "train loss:0.4173380805317726\n",
      "train loss:0.5547924324761362\n",
      "train loss:0.3947541867658829\n",
      "train loss:0.4459966855352671\n",
      "train loss:0.404820613713712\n",
      "train loss:0.40959983366514846\n",
      "train loss:0.6033431391663848\n",
      "train loss:0.5137273304776739\n",
      "train loss:0.5208324804979971\n",
      "train loss:0.44628664675844315\n",
      "train loss:0.524199925987375\n",
      "train loss:0.4191766181506872\n",
      "train loss:0.5523621709994816\n",
      "=== epoch:2, train acc:0.868, test acc:0.863 ===\n",
      "train loss:0.3615825188369042\n",
      "train loss:0.4192734004968937\n",
      "train loss:0.45018791396636026\n",
      "train loss:0.33509191124633264\n",
      "train loss:0.30401956076209724\n",
      "train loss:0.39241610024952694\n",
      "train loss:0.4211995034202451\n",
      "train loss:0.40566205719185966\n",
      "train loss:0.27906516417133287\n",
      "train loss:0.32539810101469935\n",
      "train loss:0.40873785016220304\n",
      "train loss:0.4155688359761459\n",
      "train loss:0.4544154992545855\n",
      "train loss:0.38114558262392323\n",
      "train loss:0.33097919351896943\n",
      "train loss:0.26723486959416376\n",
      "train loss:0.3426917819234132\n",
      "train loss:0.3254446619334174\n",
      "train loss:0.36354135945330623\n",
      "train loss:0.3869272312485368\n",
      "train loss:0.2982714869645696\n",
      "train loss:0.33191186470564055\n",
      "train loss:0.3029512664944543\n",
      "train loss:0.5150945949618816\n",
      "train loss:0.3192313219375962\n",
      "train loss:0.28491004212923704\n",
      "train loss:0.30658248198143095\n",
      "train loss:0.3663255153817902\n",
      "train loss:0.33998940128684585\n",
      "train loss:0.3063059693808226\n",
      "train loss:0.31497122765576013\n",
      "train loss:0.3608464189230445\n",
      "train loss:0.35712856496916456\n",
      "train loss:0.2921391314634813\n",
      "train loss:0.4870993136982096\n",
      "train loss:0.34946787558095727\n",
      "train loss:0.27147470538115837\n",
      "train loss:0.4342431660537576\n",
      "train loss:0.3045170955265627\n",
      "train loss:0.37432437211919434\n",
      "train loss:0.3827388684011388\n",
      "train loss:0.32669396347739144\n",
      "train loss:0.34841099978705453\n",
      "train loss:0.29509086456346334\n",
      "train loss:0.28712282283963075\n",
      "train loss:0.27039494345951387\n",
      "train loss:0.37205410907160785\n",
      "train loss:0.27559390196683037\n",
      "train loss:0.3491179494678221\n",
      "train loss:0.21335584741704194\n",
      "train loss:0.20401962536460616\n",
      "train loss:0.28110134720141444\n",
      "train loss:0.35106339201838727\n",
      "train loss:0.3316778111448697\n",
      "train loss:0.31729194088237156\n",
      "train loss:0.17998167668378098\n",
      "train loss:0.4114788183454183\n",
      "train loss:0.3289798493288664\n",
      "train loss:0.33532492199429575\n",
      "train loss:0.30364280480778616\n",
      "train loss:0.175617023796295\n",
      "train loss:0.3225871300487071\n",
      "train loss:0.19666748220634245\n",
      "train loss:0.4429722521865847\n",
      "train loss:0.3406808917118333\n",
      "train loss:0.41751777086757985\n",
      "train loss:0.24899302345268332\n",
      "train loss:0.3839715965056882\n",
      "train loss:0.310054489377816\n",
      "train loss:0.2553290530749407\n",
      "train loss:0.20547667235841247\n",
      "train loss:0.16590168688012402\n",
      "train loss:0.25852845978784633\n",
      "train loss:0.3680522626859408\n",
      "train loss:0.16596826914159452\n",
      "train loss:0.1722528441652345\n",
      "train loss:0.44543498236495216\n",
      "train loss:0.2548518388398676\n",
      "train loss:0.37928649662958763\n",
      "train loss:0.2843978220529031\n",
      "=== epoch:3, train acc:0.901, test acc:0.908 ===\n",
      "train loss:0.27353670779009365\n",
      "train loss:0.28860540235423526\n",
      "train loss:0.16880872837629546\n",
      "train loss:0.2328140097867006\n",
      "train loss:0.3402846350869909\n",
      "train loss:0.2387824826610385\n",
      "train loss:0.29291413185218085\n",
      "train loss:0.2617614454563246\n",
      "train loss:0.1696524146803476\n",
      "train loss:0.2902669037409695\n",
      "train loss:0.2975790955719055\n",
      "train loss:0.21703962178933656\n",
      "train loss:0.2193035019033489\n",
      "train loss:0.2643555890199545\n",
      "train loss:0.27037884197835205\n",
      "train loss:0.25894730849473385\n",
      "train loss:0.24458363300242694\n",
      "train loss:0.2615175147859088\n",
      "train loss:0.21907583775602316\n",
      "train loss:0.2760167135985288\n",
      "train loss:0.3812949780161092\n",
      "train loss:0.26319371338090436\n",
      "train loss:0.24135242707333468\n",
      "train loss:0.27956126586890806\n",
      "train loss:0.19995043057188133\n",
      "train loss:0.26678915268152287\n",
      "train loss:0.3364238689874209\n",
      "train loss:0.30965337858098485\n",
      "train loss:0.1380355594259722\n",
      "train loss:0.1936850102498583\n",
      "train loss:0.3964492994302685\n",
      "train loss:0.3436873712051035\n",
      "train loss:0.34061159290773824\n",
      "train loss:0.18730105794446558\n",
      "train loss:0.2808816178614098\n",
      "train loss:0.26705242065838614\n",
      "train loss:0.24449978746445453\n",
      "train loss:0.16263968074793755\n",
      "train loss:0.1749675033394538\n",
      "train loss:0.25144381918890213\n",
      "train loss:0.255135398843141\n",
      "train loss:0.29858172957204926\n",
      "train loss:0.12696997452847342\n",
      "train loss:0.31381330197631047\n",
      "train loss:0.25448944056187556\n",
      "train loss:0.29583524571758835\n",
      "train loss:0.16223521198948418\n",
      "train loss:0.147309254194008\n",
      "train loss:0.23050800872313368\n",
      "train loss:0.2776487625768853\n",
      "train loss:0.29460788460581605\n",
      "train loss:0.2382699879272658\n",
      "train loss:0.20781556452639235\n",
      "train loss:0.16627087797696177\n",
      "train loss:0.2109322357561372\n",
      "train loss:0.19188362640812798\n",
      "train loss:0.2783563847462505\n",
      "train loss:0.3181139997006525\n",
      "train loss:0.19771635967749307\n",
      "train loss:0.1197946542118337\n",
      "train loss:0.18341992095208146\n",
      "train loss:0.1426630893741433\n",
      "train loss:0.20059086207599763\n",
      "train loss:0.17630445170811615\n",
      "train loss:0.1677891738704021\n",
      "train loss:0.1775189593647854\n",
      "train loss:0.1585385634397897\n",
      "train loss:0.19907448887369203\n",
      "train loss:0.15861986599587244\n",
      "train loss:0.2221858620373129\n",
      "train loss:0.2669600240520198\n",
      "train loss:0.2904360205458743\n",
      "train loss:0.21085148712380486\n",
      "train loss:0.12873424327893826\n",
      "train loss:0.25745050025973415\n",
      "train loss:0.20951634026011612\n",
      "train loss:0.23088091651873366\n",
      "train loss:0.23917059072290012\n",
      "train loss:0.301121403297778\n",
      "train loss:0.34042466026614465\n",
      "=== epoch:4, train acc:0.912, test acc:0.887 ===\n",
      "train loss:0.21375766412681274\n",
      "train loss:0.16574575672558078\n",
      "train loss:0.19253785666331644\n",
      "train loss:0.25672371041581915\n",
      "train loss:0.19189282757966283\n",
      "train loss:0.17150945518748223\n",
      "train loss:0.1633617693989073\n",
      "train loss:0.15709156451633965\n",
      "train loss:0.19194471057142842\n",
      "train loss:0.2906660888700026\n",
      "train loss:0.28417649241941123\n",
      "train loss:0.18449933924016887\n",
      "train loss:0.13310142388522153\n",
      "train loss:0.19750321342837376\n",
      "train loss:0.21229702714892723\n",
      "train loss:0.3159616321166422\n",
      "train loss:0.32235228425100326\n",
      "train loss:0.2077429755991936\n",
      "train loss:0.2293922805578448\n",
      "train loss:0.2526124224963805\n",
      "train loss:0.20523378829721353\n",
      "train loss:0.26098911389821433\n",
      "train loss:0.15146258724799697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.22819547349601488\n",
      "train loss:0.21941101715835462\n",
      "train loss:0.2114316877202611\n",
      "train loss:0.17426398156164782\n",
      "train loss:0.2671420763110114\n",
      "train loss:0.13367274535867096\n",
      "train loss:0.30184062564217196\n",
      "train loss:0.3203062570696704\n",
      "train loss:0.2663100099261822\n",
      "train loss:0.23137744158733597\n",
      "train loss:0.22501383931536065\n",
      "train loss:0.1918533166947138\n",
      "train loss:0.19406380424330533\n",
      "train loss:0.28008308720077346\n",
      "train loss:0.17039679258793272\n",
      "train loss:0.19513327206475772\n",
      "train loss:0.17831260015381545\n",
      "train loss:0.14916448997927992\n",
      "train loss:0.21755554915626782\n",
      "train loss:0.20714084116164105\n",
      "train loss:0.11075305235533961\n",
      "train loss:0.21135681547637353\n",
      "train loss:0.15383297426532513\n",
      "train loss:0.23564339032911\n",
      "train loss:0.178501660348971\n",
      "train loss:0.2060019681371282\n",
      "train loss:0.15967360826303595\n",
      "train loss:0.20453957073435736\n",
      "train loss:0.12150154089572855\n",
      "train loss:0.14256533507069757\n",
      "train loss:0.08591251034077653\n",
      "train loss:0.22372313851813697\n",
      "train loss:0.1350907634505307\n",
      "train loss:0.2184474559450965\n",
      "train loss:0.1780360484241411\n",
      "train loss:0.10488507335737543\n",
      "train loss:0.10581788102224056\n",
      "train loss:0.17798390541347672\n",
      "train loss:0.17829164497392472\n",
      "train loss:0.27592878502736073\n",
      "train loss:0.1499213477947683\n",
      "train loss:0.10158831237980767\n",
      "train loss:0.21972845148842426\n",
      "train loss:0.183605360908319\n",
      "train loss:0.13753490834702886\n",
      "train loss:0.1799794722398903\n",
      "train loss:0.09072585423619793\n",
      "train loss:0.2042838227230637\n",
      "train loss:0.18708387346480077\n",
      "train loss:0.11569542812718854\n",
      "train loss:0.2751559816816252\n",
      "train loss:0.2297090692183343\n",
      "train loss:0.11056210451632036\n",
      "train loss:0.20896075605651734\n",
      "train loss:0.1603014417997005\n",
      "train loss:0.199277248721703\n",
      "train loss:0.190575612886166\n",
      "=== epoch:5, train acc:0.929, test acc:0.911 ===\n",
      "train loss:0.2738451631653786\n",
      "train loss:0.1646625417066299\n",
      "train loss:0.1409146726794419\n",
      "train loss:0.18743259706079957\n",
      "train loss:0.2589574269880167\n",
      "train loss:0.19666965960952862\n",
      "train loss:0.2789578858008601\n",
      "train loss:0.2582735897308382\n",
      "train loss:0.10428871005386771\n",
      "train loss:0.16569838663422737\n",
      "train loss:0.2087123274470935\n",
      "train loss:0.14125576205528156\n",
      "train loss:0.11317075593765448\n",
      "train loss:0.2701391880543382\n",
      "train loss:0.2436104880370604\n",
      "train loss:0.24521258083629907\n",
      "train loss:0.13069911404886073\n",
      "train loss:0.12019467500508937\n",
      "train loss:0.22632386306418767\n",
      "train loss:0.21335483555796564\n",
      "train loss:0.1845356469961763\n",
      "train loss:0.15199160214982774\n",
      "train loss:0.1388004230404169\n",
      "train loss:0.21002651737880146\n",
      "train loss:0.12838782743502727\n",
      "train loss:0.17639490540661779\n",
      "train loss:0.1758354795581069\n",
      "train loss:0.1799217503536035\n",
      "train loss:0.1707817706031993\n",
      "train loss:0.24633848777310088\n",
      "train loss:0.14934666645844802\n",
      "train loss:0.09676192966050412\n",
      "train loss:0.2561954243561715\n",
      "train loss:0.21638534310578095\n",
      "train loss:0.19176251433724484\n",
      "train loss:0.1912542777885223\n",
      "train loss:0.2163133137184349\n",
      "train loss:0.214588847181974\n",
      "train loss:0.1892249830856468\n",
      "train loss:0.21969237181420684\n",
      "train loss:0.10397267297452815\n",
      "train loss:0.1658245150137496\n",
      "train loss:0.16080191360887053\n",
      "train loss:0.3245170018754812\n",
      "train loss:0.20630250648950088\n",
      "train loss:0.10767569734274726\n",
      "train loss:0.12538834044933236\n",
      "train loss:0.1722871022437186\n",
      "train loss:0.11307705994966062\n",
      "train loss:0.13019762520923217\n",
      "train loss:0.10323820626019854\n",
      "train loss:0.24009116920650758\n",
      "train loss:0.2150728443872257\n",
      "train loss:0.147161001233557\n",
      "train loss:0.15501555244193002\n",
      "train loss:0.22765261669036227\n",
      "train loss:0.13223616802847074\n",
      "train loss:0.2269039331700175\n",
      "train loss:0.17116911826693568\n",
      "train loss:0.17232366738650828\n",
      "train loss:0.1369452725124515\n",
      "train loss:0.18486664672419081\n",
      "train loss:0.1744670255016482\n",
      "train loss:0.16715702450109823\n",
      "train loss:0.09769856838106682\n",
      "train loss:0.14787867886914166\n",
      "train loss:0.25584017886895144\n",
      "train loss:0.08737364079357966\n",
      "train loss:0.19420549867649722\n",
      "train loss:0.1351100308749872\n",
      "train loss:0.14538070491034186\n",
      "train loss:0.2876329365460921\n",
      "train loss:0.11739456392408544\n",
      "train loss:0.30271280601144507\n",
      "train loss:0.0870276699369354\n",
      "train loss:0.2448546346180568\n",
      "train loss:0.10895197376747351\n",
      "train loss:0.19573202953893282\n",
      "train loss:0.0820356886198638\n",
      "train loss:0.13012139809296777\n",
      "=== epoch:6, train acc:0.942, test acc:0.925 ===\n",
      "train loss:0.27791547514102644\n",
      "train loss:0.18844062191894664\n",
      "train loss:0.18299365446147328\n",
      "train loss:0.20945655039100652\n",
      "train loss:0.1529650738425263\n",
      "train loss:0.19746733373485875\n",
      "train loss:0.13122489155762754\n",
      "train loss:0.11118727627399805\n",
      "train loss:0.21232596852106073\n",
      "train loss:0.10614642227570087\n",
      "train loss:0.17062226072487982\n",
      "train loss:0.09027218654479902\n",
      "train loss:0.14389748905012203\n",
      "train loss:0.1552636225672514\n",
      "train loss:0.16767700858939158\n",
      "train loss:0.19371377734724782\n",
      "train loss:0.18750094813405982\n",
      "train loss:0.12950116025792413\n",
      "train loss:0.08765913926063877\n",
      "train loss:0.16495086180764543\n",
      "train loss:0.17493107465337995\n",
      "train loss:0.12635508143540575\n",
      "train loss:0.1833081220742531\n",
      "train loss:0.2569555624423889\n",
      "train loss:0.16643844133979016\n",
      "train loss:0.14213602421004926\n",
      "train loss:0.1548814579885822\n",
      "train loss:0.09483613412956854\n",
      "train loss:0.16875814221918642\n",
      "train loss:0.14516025256268622\n",
      "train loss:0.18374054186701128\n",
      "train loss:0.2033077566159199\n",
      "train loss:0.1034889926587217\n",
      "train loss:0.09704373633067206\n",
      "train loss:0.152709691287248\n",
      "train loss:0.08523878964313221\n",
      "train loss:0.11136709306267495\n",
      "train loss:0.11612767864416967\n",
      "train loss:0.0779241026198906\n",
      "train loss:0.15894048917398573\n",
      "train loss:0.17580878518459742\n",
      "train loss:0.2498061470047654\n",
      "train loss:0.20634112075954747\n",
      "train loss:0.18311619457623837\n",
      "train loss:0.0663944642657383\n",
      "train loss:0.11419836090226619\n",
      "train loss:0.22982920603079576\n",
      "train loss:0.10084970627343204\n",
      "train loss:0.20302488189657172\n",
      "train loss:0.14905969745785577\n",
      "train loss:0.1885471864471874\n",
      "train loss:0.18995844487545066\n",
      "train loss:0.14941038367549084\n",
      "train loss:0.2468861734921573\n",
      "train loss:0.08722717535088442\n",
      "train loss:0.095758687891136\n",
      "train loss:0.06065982978261343\n",
      "train loss:0.2295259983253484\n",
      "train loss:0.08139912017068099\n",
      "train loss:0.1295716912144767\n",
      "train loss:0.0894870468327754\n",
      "train loss:0.12620525402098717\n",
      "train loss:0.09650295899418719\n",
      "train loss:0.10733056734247974\n",
      "train loss:0.1052466518641891\n",
      "train loss:0.25089893620362386\n",
      "train loss:0.11344529147573651\n",
      "train loss:0.14220456199251583\n",
      "train loss:0.2185323718226977\n",
      "train loss:0.07571129340801072\n",
      "train loss:0.055819709471894005\n",
      "train loss:0.12137825401117881\n",
      "train loss:0.150288056221372\n",
      "train loss:0.17017987493072317\n",
      "train loss:0.11699484970099631\n",
      "train loss:0.14881237986644855\n",
      "train loss:0.17219109293659635\n",
      "train loss:0.16957992623221016\n",
      "train loss:0.11411690109893845\n",
      "train loss:0.1940439108336421\n",
      "=== epoch:7, train acc:0.952, test acc:0.932 ===\n",
      "train loss:0.20321225532490617\n",
      "train loss:0.16592161677012396\n",
      "train loss:0.12260994063283992\n",
      "train loss:0.16132997856867515\n",
      "train loss:0.15151703869687175\n",
      "train loss:0.18485690453825307\n",
      "train loss:0.09621902104699295\n",
      "train loss:0.09748454263917893\n",
      "train loss:0.17549381999515873\n",
      "train loss:0.09414301002820798\n",
      "train loss:0.10577317276393\n",
      "train loss:0.10712753526136744\n",
      "train loss:0.1824802664509507\n",
      "train loss:0.19414207959886207\n",
      "train loss:0.14899521023800383\n",
      "train loss:0.10437744847065689\n",
      "train loss:0.08522128833504951\n",
      "train loss:0.19289934601498218\n",
      "train loss:0.1195783730959502\n",
      "train loss:0.12622715180084643\n",
      "train loss:0.11048983510544261\n",
      "train loss:0.09229382803970941\n",
      "train loss:0.16081699870172145\n",
      "train loss:0.19198362863008944\n",
      "train loss:0.1982303438773955\n",
      "train loss:0.12468968822575757\n",
      "train loss:0.24213543504545434\n",
      "train loss:0.1253756364973862\n",
      "train loss:0.21046259375233764\n",
      "train loss:0.13369186858420934\n",
      "train loss:0.10164872353648252\n",
      "train loss:0.1750027415235695\n",
      "train loss:0.08063480257099967\n",
      "train loss:0.153411916500424\n",
      "train loss:0.16537894287481245\n",
      "train loss:0.0884155588006083\n",
      "train loss:0.08741035190610137\n",
      "train loss:0.07820774537001685\n",
      "train loss:0.15716393367917578\n",
      "train loss:0.1333084006408206\n",
      "train loss:0.13973858943246406\n",
      "train loss:0.1387780560225284\n",
      "train loss:0.06884847115279163\n",
      "train loss:0.06016640359164439\n",
      "train loss:0.11908256173924908\n",
      "train loss:0.1564775440391665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.09038383289277416\n",
      "train loss:0.14204442072762885\n",
      "train loss:0.1520528992143977\n",
      "train loss:0.08473790774988935\n",
      "train loss:0.10532595713639345\n",
      "train loss:0.25207348240407085\n",
      "train loss:0.09569410464220071\n",
      "train loss:0.06606086135755401\n",
      "train loss:0.13666978590151854\n",
      "train loss:0.1256602865870244\n",
      "train loss:0.08268764052355007\n",
      "train loss:0.11779336261439574\n",
      "train loss:0.09653859044168805\n",
      "train loss:0.10601042919371687\n",
      "train loss:0.13980008171492359\n",
      "train loss:0.06987229313518269\n",
      "train loss:0.12207525630250216\n",
      "train loss:0.09084419416644246\n",
      "train loss:0.13973007384852135\n",
      "train loss:0.13627921280845767\n",
      "train loss:0.0923378378287853\n",
      "train loss:0.06508514081695085\n",
      "train loss:0.05054364865189607\n",
      "train loss:0.1371038649254305\n",
      "train loss:0.06561500681564712\n",
      "train loss:0.14877089488414838\n",
      "train loss:0.18476607513044432\n",
      "train loss:0.18942523896447894\n",
      "train loss:0.124633246479888\n",
      "train loss:0.0919266087793762\n",
      "train loss:0.05273637721160352\n",
      "train loss:0.09612855347942663\n",
      "train loss:0.12022585031736038\n",
      "train loss:0.11953132597302946\n",
      "=== epoch:8, train acc:0.956, test acc:0.935 ===\n",
      "train loss:0.06887622299277436\n",
      "train loss:0.1486291224179494\n",
      "train loss:0.21512442541859692\n",
      "train loss:0.10667293011602547\n",
      "train loss:0.0856994974529386\n",
      "train loss:0.09018038758600735\n",
      "train loss:0.09653476529406228\n",
      "train loss:0.09294649070453645\n",
      "train loss:0.12290778011146107\n",
      "train loss:0.1186887461503645\n",
      "train loss:0.19545793769596942\n",
      "train loss:0.08814795432426994\n",
      "train loss:0.07556983364663114\n",
      "train loss:0.1657130727261605\n",
      "train loss:0.09634988977409398\n",
      "train loss:0.08642990560561475\n",
      "train loss:0.04375353145814187\n",
      "train loss:0.09232859199389898\n",
      "train loss:0.12334178338092977\n",
      "train loss:0.10217650434090109\n",
      "train loss:0.12996360011169195\n",
      "train loss:0.15544778811586343\n",
      "train loss:0.12943287492566427\n",
      "train loss:0.12509631534956805\n",
      "train loss:0.13554169742177902\n",
      "train loss:0.14123081457764353\n",
      "train loss:0.09470508145515694\n",
      "train loss:0.07719546404248744\n",
      "train loss:0.06632950667334919\n",
      "train loss:0.19290079309086983\n",
      "train loss:0.033141358887295094\n",
      "train loss:0.06447203532490602\n",
      "train loss:0.07865211602161053\n",
      "train loss:0.11460766051775312\n",
      "train loss:0.15000260143141514\n",
      "train loss:0.13209361512863294\n",
      "train loss:0.10702393150416434\n",
      "train loss:0.18188852927752047\n",
      "train loss:0.09455303275465869\n",
      "train loss:0.15549835114021934\n",
      "train loss:0.10350736046575724\n",
      "train loss:0.09676116278470294\n",
      "train loss:0.12286490915410281\n",
      "train loss:0.08946686169106068\n",
      "train loss:0.0752965378094485\n",
      "train loss:0.12602542669698194\n",
      "train loss:0.15324352784453546\n",
      "train loss:0.08338907168507313\n",
      "train loss:0.09870519341419999\n",
      "train loss:0.21221614932743957\n",
      "train loss:0.0819522558947701\n",
      "train loss:0.04773788798065373\n",
      "train loss:0.06266667796914323\n",
      "train loss:0.08690520573811293\n",
      "train loss:0.09301400399915614\n",
      "train loss:0.13075876016343407\n",
      "train loss:0.1413072765522443\n",
      "train loss:0.10321060032154658\n",
      "train loss:0.07617475666385363\n",
      "train loss:0.05611579001125589\n",
      "train loss:0.16924942266884105\n",
      "train loss:0.09192773358937634\n",
      "train loss:0.06499197079991333\n",
      "train loss:0.10043095477373944\n",
      "train loss:0.057468155350118434\n",
      "train loss:0.22875161490080365\n",
      "train loss:0.083375082107333\n",
      "train loss:0.10074636317544064\n",
      "train loss:0.09794875448564305\n",
      "train loss:0.039095004688049065\n",
      "train loss:0.14748754400568884\n",
      "train loss:0.1080585869035062\n",
      "train loss:0.09758537858124196\n",
      "train loss:0.08822093234524359\n",
      "train loss:0.07063403558515134\n",
      "train loss:0.10201284057108247\n",
      "train loss:0.12099887589183979\n",
      "train loss:0.06810773356324758\n",
      "train loss:0.10993284370669704\n",
      "train loss:0.14500585083889556\n",
      "=== epoch:9, train acc:0.968, test acc:0.939 ===\n",
      "train loss:0.10128146394496525\n",
      "train loss:0.1355346083263346\n",
      "train loss:0.1676845735395143\n",
      "train loss:0.05629089895007315\n",
      "train loss:0.10521980714083373\n",
      "train loss:0.10031013623631344\n",
      "train loss:0.09182998775291674\n",
      "train loss:0.07522210632352717\n",
      "train loss:0.09338149186699672\n",
      "train loss:0.12092190743782526\n",
      "train loss:0.16372410058577114\n",
      "train loss:0.057659925593013896\n",
      "train loss:0.10903754231719871\n",
      "train loss:0.08523147202782613\n",
      "train loss:0.07436343068842095\n",
      "train loss:0.09147556446202568\n",
      "train loss:0.08678627501094521\n",
      "train loss:0.15372606226553553\n",
      "train loss:0.09942772904189232\n",
      "train loss:0.10873511716651806\n",
      "train loss:0.09470642256909406\n",
      "train loss:0.05923857706848759\n",
      "train loss:0.04671239773334861\n",
      "train loss:0.05075917137592079\n",
      "train loss:0.05363433417025542\n",
      "train loss:0.05183229646529072\n",
      "train loss:0.08910175034136136\n",
      "train loss:0.07915399874571696\n",
      "train loss:0.09202694332569462\n",
      "train loss:0.16136778018117798\n",
      "train loss:0.06447315186236124\n",
      "train loss:0.09861576809525203\n",
      "train loss:0.0685264706019783\n",
      "train loss:0.08893868293072332\n",
      "train loss:0.05714849416153436\n",
      "train loss:0.0635779855697664\n",
      "train loss:0.06754967603715713\n",
      "train loss:0.1132116714289147\n",
      "train loss:0.1054566785020715\n",
      "train loss:0.08094415250346274\n",
      "train loss:0.08900269505519566\n",
      "train loss:0.08889028151857012\n",
      "train loss:0.10860785479245179\n",
      "train loss:0.06036561944451613\n",
      "train loss:0.04809057645549028\n",
      "train loss:0.06142692049383455\n",
      "train loss:0.06745349277510279\n",
      "train loss:0.20898197882883007\n",
      "train loss:0.06247455466957216\n",
      "train loss:0.030156244765251885\n",
      "train loss:0.1203634663926486\n",
      "train loss:0.05823735391839963\n",
      "train loss:0.08969836546357769\n",
      "train loss:0.09310552629424855\n",
      "train loss:0.13930297989378124\n",
      "train loss:0.08157740724833723\n",
      "train loss:0.14319556989590554\n",
      "train loss:0.09826946842218484\n",
      "train loss:0.16307252747917073\n",
      "train loss:0.047900491617349675\n",
      "train loss:0.07490318412933847\n",
      "train loss:0.16584452571969496\n",
      "train loss:0.07088681586350189\n",
      "train loss:0.20102748461021075\n",
      "train loss:0.08916309922038008\n",
      "train loss:0.07367723679841395\n",
      "train loss:0.12816456394767956\n",
      "train loss:0.039860194549145155\n",
      "train loss:0.0721846064613767\n",
      "train loss:0.13079325498283306\n",
      "train loss:0.05043367739902258\n",
      "train loss:0.08496786685436021\n",
      "train loss:0.09113198154080117\n",
      "train loss:0.05690943814665897\n",
      "train loss:0.11549173490310005\n",
      "train loss:0.11976884911774004\n",
      "train loss:0.05939488935718258\n",
      "train loss:0.0414464172476811\n",
      "train loss:0.11112470010398462\n",
      "train loss:0.09344119563304541\n",
      "=== epoch:10, train acc:0.967, test acc:0.948 ===\n",
      "train loss:0.11458102850062644\n",
      "train loss:0.09496100244714761\n",
      "train loss:0.12262769186310202\n",
      "train loss:0.06298787008579287\n",
      "train loss:0.07709002242379887\n",
      "train loss:0.11332865171593767\n",
      "train loss:0.08495446047039698\n",
      "train loss:0.08496174116548343\n",
      "train loss:0.04547180413669774\n",
      "train loss:0.05522706557677351\n",
      "train loss:0.054772720003028644\n",
      "train loss:0.1303334331922222\n",
      "train loss:0.05907670830179914\n",
      "train loss:0.09370220056922772\n",
      "train loss:0.07551316090742459\n",
      "train loss:0.07754319681866181\n",
      "train loss:0.048906265674387855\n",
      "train loss:0.09057591925148216\n",
      "train loss:0.07267449906695925\n",
      "train loss:0.08278372963025343\n",
      "train loss:0.06691297980500646\n",
      "train loss:0.05502173331446466\n",
      "train loss:0.1553613979591846\n",
      "train loss:0.09340651454577203\n",
      "train loss:0.09856209977157203\n",
      "train loss:0.13756415284625081\n",
      "train loss:0.07300533667080317\n",
      "train loss:0.1389297355508053\n",
      "train loss:0.037969883968374286\n",
      "train loss:0.039641055671669055\n",
      "train loss:0.05901194392192756\n",
      "train loss:0.10741939147072084\n",
      "train loss:0.15577239780320917\n",
      "train loss:0.07487162687816168\n",
      "train loss:0.06793166130322927\n",
      "train loss:0.0905027978716032\n",
      "train loss:0.1152179753477192\n",
      "train loss:0.057780710008476566\n",
      "train loss:0.07872783489434039\n",
      "train loss:0.10312366243583268\n",
      "train loss:0.06580845952986603\n",
      "train loss:0.05631367927442705\n",
      "train loss:0.044591550551785375\n",
      "train loss:0.14012663895837021\n",
      "train loss:0.060293215246533124\n",
      "train loss:0.08740101680480895\n",
      "train loss:0.032109176516023116\n",
      "train loss:0.049910951334191454\n",
      "train loss:0.12974205934289293\n",
      "train loss:0.139504026499185\n",
      "train loss:0.09892722597154906\n",
      "train loss:0.07330847616524838\n",
      "train loss:0.10175740599758776\n",
      "train loss:0.1231866580382523\n",
      "train loss:0.057634416002154375\n",
      "train loss:0.0877770260905741\n",
      "train loss:0.061713675564292\n",
      "train loss:0.06825325406986227\n",
      "train loss:0.08919105482905829\n",
      "train loss:0.05106453759450249\n",
      "train loss:0.04887591562533491\n",
      "train loss:0.062169707448282194\n",
      "train loss:0.10139529309691842\n",
      "train loss:0.13732680208434048\n",
      "train loss:0.0794631805711847\n",
      "train loss:0.056741587112001356\n",
      "train loss:0.0855259774859957\n",
      "train loss:0.07574033567594211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.11188507658620558\n",
      "train loss:0.12155505461696176\n",
      "train loss:0.04396160056628098\n",
      "train loss:0.022098407096092374\n",
      "train loss:0.051976225130610294\n",
      "train loss:0.07195421292869844\n",
      "train loss:0.07819582241438759\n",
      "train loss:0.14431036700829172\n",
      "train loss:0.06783310600168596\n",
      "train loss:0.03996506019087844\n",
      "train loss:0.07171533556090948\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.961\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "\n",
    "network = CNN(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 20, 'filter_size': 9, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='SGD', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "SGD_train_acc=trainer.train_acc_list\n",
    "SGD_test_acc=trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum을 활용한 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2939692321075547\n",
      "=== epoch:1, train acc:0.146, test acc:0.142 ===\n",
      "train loss:2.231078793287649\n",
      "train loss:2.1950729801921254\n",
      "train loss:2.1314569129600143\n",
      "train loss:2.0825821425887945\n",
      "train loss:1.9898483437594177\n",
      "train loss:1.8423504390720453\n",
      "train loss:1.7203951917156461\n",
      "train loss:1.6207334915822824\n",
      "train loss:1.370470944244701\n",
      "train loss:1.0636116272866747\n",
      "train loss:0.9801939643520607\n",
      "train loss:0.8393636431839309\n",
      "train loss:0.6906899221530084\n",
      "train loss:0.555797853787717\n",
      "train loss:0.5689694700344713\n",
      "train loss:0.5258731051822114\n",
      "train loss:0.5752195639329848\n",
      "train loss:0.719943172437114\n",
      "train loss:0.4173774693574292\n",
      "train loss:0.745183678147799\n",
      "train loss:0.6104409846934777\n",
      "train loss:0.7039730245514925\n",
      "train loss:0.4311215551282158\n",
      "train loss:0.6698941244115649\n",
      "train loss:0.41087298852373727\n",
      "train loss:0.3487639984251303\n",
      "train loss:0.4770399256369781\n",
      "train loss:0.5661871977129045\n",
      "train loss:0.548155044936001\n",
      "train loss:0.285074221589173\n",
      "train loss:0.40781224687528117\n",
      "train loss:0.4259870046667744\n",
      "train loss:0.3830168146208067\n",
      "train loss:0.2832446748219299\n",
      "train loss:0.5011166009861143\n",
      "train loss:0.46140491559207253\n",
      "train loss:0.31432231596483434\n",
      "train loss:0.5799067681070761\n",
      "train loss:0.47030874849403403\n",
      "train loss:0.39917422344110115\n",
      "train loss:0.2945130718643762\n",
      "train loss:0.42141910642828\n",
      "train loss:0.4120551060035504\n",
      "train loss:0.21445269877158893\n",
      "train loss:0.24814202966416635\n",
      "train loss:0.22784456258763658\n",
      "train loss:0.17611593326610442\n",
      "train loss:0.25673178956564535\n",
      "train loss:0.20881021844157793\n",
      "train loss:0.3069145817893723\n",
      "train loss:0.2411641703078959\n",
      "train loss:0.2814859693806837\n",
      "train loss:0.27410354717473834\n",
      "train loss:0.27467403156079606\n",
      "train loss:0.12485484880881703\n",
      "train loss:0.24102833445198626\n",
      "train loss:0.21886263307545759\n",
      "train loss:0.18424022314559477\n",
      "train loss:0.4417151600328841\n",
      "train loss:0.23048656648763705\n",
      "train loss:0.28042598786804396\n",
      "train loss:0.06364922791843862\n",
      "train loss:0.2099999146098611\n",
      "train loss:0.17442968627780547\n",
      "train loss:0.15694108870214163\n",
      "train loss:0.1906198349825305\n",
      "train loss:0.20734787818891381\n",
      "train loss:0.05324349939830165\n",
      "train loss:0.14811272641976697\n",
      "train loss:0.1799700069414527\n",
      "train loss:0.10563102002257833\n",
      "train loss:0.19299154993098033\n",
      "train loss:0.13732842384416985\n",
      "train loss:0.10229660538431748\n",
      "train loss:0.18512505576169494\n",
      "train loss:0.2542433470044463\n",
      "train loss:0.1667804782035281\n",
      "train loss:0.18667070079800063\n",
      "train loss:0.23110575468803726\n",
      "train loss:0.1893409921239561\n",
      "=== epoch:2, train acc:0.954, test acc:0.927 ===\n",
      "train loss:0.17964341820256094\n",
      "train loss:0.14541599539465927\n",
      "train loss:0.08427559487332464\n",
      "train loss:0.15728984545598984\n",
      "train loss:0.36017184596125057\n",
      "train loss:0.07534722101208007\n",
      "train loss:0.24508173754462093\n",
      "train loss:0.10455837664347781\n",
      "train loss:0.0704637878141925\n",
      "train loss:0.15707420224643645\n",
      "train loss:0.17916103352217885\n",
      "train loss:0.1268611944843207\n",
      "train loss:0.10445111626751903\n",
      "train loss:0.1312442952250281\n",
      "train loss:0.20414495643635738\n",
      "train loss:0.20776847058781633\n",
      "train loss:0.18627247464901595\n",
      "train loss:0.14723730930193646\n",
      "train loss:0.08851958830173395\n",
      "train loss:0.09322731371634835\n",
      "train loss:0.13822412597959716\n",
      "train loss:0.1719649332819354\n",
      "train loss:0.0974806326453186\n",
      "train loss:0.10797268855014927\n",
      "train loss:0.09766745903784843\n",
      "train loss:0.1133372160210299\n",
      "train loss:0.11803951348300172\n",
      "train loss:0.06865745151002593\n",
      "train loss:0.1653857248766305\n",
      "train loss:0.08640579439337817\n",
      "train loss:0.07483316966883574\n",
      "train loss:0.11743028789915189\n",
      "train loss:0.16649344268196317\n",
      "train loss:0.14588610511811204\n",
      "train loss:0.09434671533935529\n",
      "train loss:0.08119624886961051\n",
      "train loss:0.23489541212812703\n",
      "train loss:0.06162594661669949\n",
      "train loss:0.09399679407782446\n",
      "train loss:0.10042121333072186\n",
      "train loss:0.1327280296667404\n",
      "train loss:0.10980769609434952\n",
      "train loss:0.16183887448482598\n",
      "train loss:0.048191953536979054\n",
      "train loss:0.1477453342581528\n",
      "train loss:0.08730671260883315\n",
      "train loss:0.171989127746311\n",
      "train loss:0.10842893092170344\n",
      "train loss:0.12634933694769315\n",
      "train loss:0.14066858000609264\n",
      "train loss:0.11766890836842776\n",
      "train loss:0.03167378670376796\n",
      "train loss:0.10987738311979539\n",
      "train loss:0.15423586679102974\n",
      "train loss:0.147569341982628\n",
      "train loss:0.14071591803265351\n",
      "train loss:0.06162659694332417\n",
      "train loss:0.10461068158391967\n",
      "train loss:0.08525284284053641\n",
      "train loss:0.031032455855563818\n",
      "train loss:0.10216759729122007\n",
      "train loss:0.1880351058419823\n",
      "train loss:0.07349639311851186\n",
      "train loss:0.033812379924449874\n",
      "train loss:0.05911473739419607\n",
      "train loss:0.1088484918269516\n",
      "train loss:0.10986856905618303\n",
      "train loss:0.24915673580231673\n",
      "train loss:0.06873115146248324\n",
      "train loss:0.08970711400185136\n",
      "train loss:0.041757896321516\n",
      "train loss:0.08103957884928788\n",
      "train loss:0.2227139666999896\n",
      "train loss:0.07786519710781632\n",
      "train loss:0.058320675509820064\n",
      "train loss:0.031154834131575337\n",
      "train loss:0.06342011569710372\n",
      "train loss:0.04833141078194551\n",
      "train loss:0.1417277184161537\n",
      "train loss:0.17794202766452485\n",
      "=== epoch:3, train acc:0.965, test acc:0.949 ===\n",
      "train loss:0.10607513155533187\n",
      "train loss:0.10729516370034849\n",
      "train loss:0.07217333113892445\n",
      "train loss:0.12893185523048378\n",
      "train loss:0.04882044267107981\n",
      "train loss:0.0908798643656686\n",
      "train loss:0.08585754730775606\n",
      "train loss:0.07758644698058728\n",
      "train loss:0.0789528089760245\n",
      "train loss:0.18622849018837248\n",
      "train loss:0.04272608145877413\n",
      "train loss:0.10869633697452173\n",
      "train loss:0.1285789784230195\n",
      "train loss:0.05867769823173985\n",
      "train loss:0.07771406385831912\n",
      "train loss:0.15017406690201873\n",
      "train loss:0.0512811997134641\n",
      "train loss:0.19972607266445055\n",
      "train loss:0.08131297086323938\n",
      "train loss:0.03949319810459796\n",
      "train loss:0.037676185745819504\n",
      "train loss:0.11269677827697806\n",
      "train loss:0.14517963377178866\n",
      "train loss:0.12084688686282058\n",
      "train loss:0.0445310206610026\n",
      "train loss:0.06669749420398567\n",
      "train loss:0.082788711449715\n",
      "train loss:0.07311265056219879\n",
      "train loss:0.10298990663412345\n",
      "train loss:0.036289100224890326\n",
      "train loss:0.07011743621098318\n",
      "train loss:0.051660959814909285\n",
      "train loss:0.07956974927119921\n",
      "train loss:0.04282016684159041\n",
      "train loss:0.08151476327129238\n",
      "train loss:0.01717027913980156\n",
      "train loss:0.06273646819847589\n",
      "train loss:0.13424542206752604\n",
      "train loss:0.07269546982433134\n",
      "train loss:0.037892787890372204\n",
      "train loss:0.11127667508182927\n",
      "train loss:0.02299455172429461\n",
      "train loss:0.11171720959783726\n",
      "train loss:0.036546561034178804\n",
      "train loss:0.04142887155488908\n",
      "train loss:0.024246451098436813\n",
      "train loss:0.027725880562277413\n",
      "train loss:0.031157755882286177\n",
      "train loss:0.024411661732852395\n",
      "train loss:0.06896370064625655\n",
      "train loss:0.08843087617378749\n",
      "train loss:0.053692457745455224\n",
      "train loss:0.017589286645572556\n",
      "train loss:0.0326928300548517\n",
      "train loss:0.020636690058859132\n",
      "train loss:0.02999544639855347\n",
      "train loss:0.08194049197998765\n",
      "train loss:0.05686911837608281\n",
      "train loss:0.09605619022187183\n",
      "train loss:0.055524984061736074\n",
      "train loss:0.0145384926472967\n",
      "train loss:0.05607109063918757\n",
      "train loss:0.047676662351136365\n",
      "train loss:0.03413116164282362\n",
      "train loss:0.07439235342613709\n",
      "train loss:0.03997692270610048\n",
      "train loss:0.06380474339831195\n",
      "train loss:0.010971490813530948\n",
      "train loss:0.04620716079166216\n",
      "train loss:0.024078970572295303\n",
      "train loss:0.03866484626943775\n",
      "train loss:0.05976531605261858\n",
      "train loss:0.1261775353126281\n",
      "train loss:0.022728302084553485\n",
      "train loss:0.06341948356862313\n",
      "train loss:0.06731019300745332\n",
      "train loss:0.05364316547321307\n",
      "train loss:0.030195424450477466\n",
      "train loss:0.0511989146943887\n",
      "train loss:0.02884859419237569\n",
      "=== epoch:4, train acc:0.988, test acc:0.963 ===\n",
      "train loss:0.08743877260758565\n",
      "train loss:0.03759412130603695\n",
      "train loss:0.047332964211743496\n",
      "train loss:0.046559502782013755\n",
      "train loss:0.030535978193020136\n",
      "train loss:0.1360502606096261\n",
      "train loss:0.03357687711612977\n",
      "train loss:0.012973940688275969\n",
      "train loss:0.0348457875249577\n",
      "train loss:0.015943794080760233\n",
      "train loss:0.03212328325381928\n",
      "train loss:0.057688909268666\n",
      "train loss:0.046588390848417885\n",
      "train loss:0.03204687977463806\n",
      "train loss:0.015514323907993486\n",
      "train loss:0.035366516578958555\n",
      "train loss:0.016498672121270624\n",
      "train loss:0.013787640518972027\n",
      "train loss:0.03805650509820344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02241218595691966\n",
      "train loss:0.041147053696944706\n",
      "train loss:0.03877151965991092\n",
      "train loss:0.0424133793046013\n",
      "train loss:0.03972314723867331\n",
      "train loss:0.029579786212771728\n",
      "train loss:0.013783714928711248\n",
      "train loss:0.031091596977180006\n",
      "train loss:0.052605218187982444\n",
      "train loss:0.02790358772409424\n",
      "train loss:0.007091936051932166\n",
      "train loss:0.02818788568694692\n",
      "train loss:0.03459890456849169\n",
      "train loss:0.06027836920550673\n",
      "train loss:0.01514214250592815\n",
      "train loss:0.04606420880558587\n",
      "train loss:0.025466000048981693\n",
      "train loss:0.1244614473899302\n",
      "train loss:0.038600305026050385\n",
      "train loss:0.0625882514213109\n",
      "train loss:0.011786115956084597\n",
      "train loss:0.01841843711215134\n",
      "train loss:0.023144964647679275\n",
      "train loss:0.020492839140400534\n",
      "train loss:0.0435574083248457\n",
      "train loss:0.008406330108570594\n",
      "train loss:0.015356333660147256\n",
      "train loss:0.010525284401915724\n",
      "train loss:0.06482282033774459\n",
      "train loss:0.030718116895894928\n",
      "train loss:0.08920201642342013\n",
      "train loss:0.04056080568026564\n",
      "train loss:0.05899900065046912\n",
      "train loss:0.03659820873919417\n",
      "train loss:0.024360104458344148\n",
      "train loss:0.029809239702054954\n",
      "train loss:0.0375914252332426\n",
      "train loss:0.025143377241752463\n",
      "train loss:0.023758790428457643\n",
      "train loss:0.03773737030297292\n",
      "train loss:0.02074999296515759\n",
      "train loss:0.07582620560060832\n",
      "train loss:0.041076779138462756\n",
      "train loss:0.01758886544953285\n",
      "train loss:0.012932766894930463\n",
      "train loss:0.02687729396849249\n",
      "train loss:0.0626835755197573\n",
      "train loss:0.008565726736856846\n",
      "train loss:0.05400628696753036\n",
      "train loss:0.04143905919124742\n",
      "train loss:0.07635232585199031\n",
      "train loss:0.09281098187453668\n",
      "train loss:0.02212586622159972\n",
      "train loss:0.04200964718146835\n",
      "train loss:0.03960952975874836\n",
      "train loss:0.06414841755947556\n",
      "train loss:0.034094401355396264\n",
      "train loss:0.02318261020279403\n",
      "train loss:0.01525523941135111\n",
      "train loss:0.046870051912787626\n",
      "train loss:0.02323298732142252\n",
      "=== epoch:5, train acc:0.983, test acc:0.962 ===\n",
      "train loss:0.03747444207234372\n",
      "train loss:0.03132137427751526\n",
      "train loss:0.0062291634962885435\n",
      "train loss:0.05720340696832024\n",
      "train loss:0.06100538403445979\n",
      "train loss:0.051451723233904255\n",
      "train loss:0.021589264317427984\n",
      "train loss:0.016153589940226604\n",
      "train loss:0.015615567327489999\n",
      "train loss:0.04698389590428016\n",
      "train loss:0.012155607890888901\n",
      "train loss:0.025146401972204354\n",
      "train loss:0.04206202746070721\n",
      "train loss:0.027903204612159373\n",
      "train loss:0.04724457897904824\n",
      "train loss:0.08141248647317123\n",
      "train loss:0.027550238579939684\n",
      "train loss:0.04615028050834311\n",
      "train loss:0.057384834081339375\n",
      "train loss:0.03097820025173976\n",
      "train loss:0.04992380009210952\n",
      "train loss:0.06304219001894223\n",
      "train loss:0.0420464985294063\n",
      "train loss:0.02900782532215715\n",
      "train loss:0.014038611389350692\n",
      "train loss:0.029773533088927495\n",
      "train loss:0.0846007199272146\n",
      "train loss:0.05610543608647407\n",
      "train loss:0.03537459925193248\n",
      "train loss:0.015562496318350973\n",
      "train loss:0.04353318411852847\n",
      "train loss:0.03256367999003542\n",
      "train loss:0.014500347161424162\n",
      "train loss:0.055921182438142585\n",
      "train loss:0.01364872548120968\n",
      "train loss:0.01997560862715476\n",
      "train loss:0.0155782212426178\n",
      "train loss:0.011729420471297235\n",
      "train loss:0.008841239282950211\n",
      "train loss:0.016003914554163966\n",
      "train loss:0.06293220831687699\n",
      "train loss:0.013312666126821603\n",
      "train loss:0.0314879980169658\n",
      "train loss:0.04003799967047648\n",
      "train loss:0.039425267343287655\n",
      "train loss:0.02790940125496169\n",
      "train loss:0.02545841475030268\n",
      "train loss:0.033878442855442246\n",
      "train loss:0.011091045860703195\n",
      "train loss:0.023074256149170292\n",
      "train loss:0.024613757620574647\n",
      "train loss:0.0041303757299471795\n",
      "train loss:0.03035471120661918\n",
      "train loss:0.03396355051356778\n",
      "train loss:0.061057677205084496\n",
      "train loss:0.02347141336293139\n",
      "train loss:0.03808085048348798\n",
      "train loss:0.0067439627402306156\n",
      "train loss:0.06545506101766035\n",
      "train loss:0.022078058227536418\n",
      "train loss:0.013827010092987122\n",
      "train loss:0.02097910359242168\n",
      "train loss:0.014295022755512754\n",
      "train loss:0.02232067970848763\n",
      "train loss:0.062011245853691783\n",
      "train loss:0.07177384212799981\n",
      "train loss:0.00571126847467933\n",
      "train loss:0.029155276310812685\n",
      "train loss:0.03082056694417274\n",
      "train loss:0.012448617122712079\n",
      "train loss:0.024916126447077268\n",
      "train loss:0.04139028321304817\n",
      "train loss:0.021466917721281988\n",
      "train loss:0.007927372860968906\n",
      "train loss:0.012953052341169167\n",
      "train loss:0.02302278959797048\n",
      "train loss:0.0063735631975661644\n",
      "train loss:0.03121285179863862\n",
      "train loss:0.0639811603384575\n",
      "train loss:0.014468519888785199\n",
      "=== epoch:6, train acc:0.99, test acc:0.97 ===\n",
      "train loss:0.02255361182897285\n",
      "train loss:0.03322202578762539\n",
      "train loss:0.02493907233756111\n",
      "train loss:0.014325844491812385\n",
      "train loss:0.008263360488074566\n",
      "train loss:0.058426652031348565\n",
      "train loss:0.013796456345998482\n",
      "train loss:0.014614016862322984\n",
      "train loss:0.01324932051455493\n",
      "train loss:0.013546298592126133\n",
      "train loss:0.004655019825999369\n",
      "train loss:0.009794719405155332\n",
      "train loss:0.01136935759021692\n",
      "train loss:0.08142193918203519\n",
      "train loss:0.0134923228773535\n",
      "train loss:0.01199642008779155\n",
      "train loss:0.009404035961953437\n",
      "train loss:0.010552424295696538\n",
      "train loss:0.020099078665625103\n",
      "train loss:0.009643943086905568\n",
      "train loss:0.009180219872140788\n",
      "train loss:0.015676756601684716\n",
      "train loss:0.003811254773580894\n",
      "train loss:0.014192581523960336\n",
      "train loss:0.01411534044353627\n",
      "train loss:0.029278059398026825\n",
      "train loss:0.01689214158547241\n",
      "train loss:0.013310655265162225\n",
      "train loss:0.01804624157549665\n",
      "train loss:0.009836096514311596\n",
      "train loss:0.010401272210750706\n",
      "train loss:0.004095713530228384\n",
      "train loss:0.008815762107740757\n",
      "train loss:0.02597428767159777\n",
      "train loss:0.028830430826192677\n",
      "train loss:0.006540052708644252\n",
      "train loss:0.005567295805501224\n",
      "train loss:0.032357953858937034\n",
      "train loss:0.002662331529210424\n",
      "train loss:0.04692008077714342\n",
      "train loss:0.01646961801808454\n",
      "train loss:0.055050454493210416\n",
      "train loss:0.03269356231894739\n",
      "train loss:0.02682360454294906\n",
      "train loss:0.029075184504121086\n",
      "train loss:0.002453359921406767\n",
      "train loss:0.006968717470987219\n",
      "train loss:0.050127742523194796\n",
      "train loss:0.023689784370827312\n",
      "train loss:0.017341788637035126\n",
      "train loss:0.04427766555643501\n",
      "train loss:0.012261316583180202\n",
      "train loss:0.015428478832482275\n",
      "train loss:0.02710218447281425\n",
      "train loss:0.013801671720839149\n",
      "train loss:0.009722539345134817\n",
      "train loss:0.002775200114058417\n",
      "train loss:0.016245372750130622\n",
      "train loss:0.010267916001175817\n",
      "train loss:0.034173610658779136\n",
      "train loss:0.01178485107738398\n",
      "train loss:0.009661560407330317\n",
      "train loss:0.005336493141100871\n",
      "train loss:0.02609103132778826\n",
      "train loss:0.011870809457037598\n",
      "train loss:0.02452940054696435\n",
      "train loss:0.006034012286432564\n",
      "train loss:0.044696404348307434\n",
      "train loss:0.020175159941524502\n",
      "train loss:0.007011501225295328\n",
      "train loss:0.04967929311402287\n",
      "train loss:0.03900902201210095\n",
      "train loss:0.01609541283528769\n",
      "train loss:0.028029271780255362\n",
      "train loss:0.028015067625937817\n",
      "train loss:0.008403480018295679\n",
      "train loss:0.00663319189781111\n",
      "train loss:0.02470946462635597\n",
      "train loss:0.07755672661172124\n",
      "train loss:0.007700031393074059\n",
      "=== epoch:7, train acc:0.993, test acc:0.977 ===\n",
      "train loss:0.019502424103091626\n",
      "train loss:0.010664860380174364\n",
      "train loss:0.013227195461873483\n",
      "train loss:0.038353677947400876\n",
      "train loss:0.0038493233666176247\n",
      "train loss:0.01730383817241573\n",
      "train loss:0.007959125157538199\n",
      "train loss:0.01580394135805123\n",
      "train loss:0.023057664997835618\n",
      "train loss:0.02320340657955428\n",
      "train loss:0.011191611014486331\n",
      "train loss:0.026940263473907423\n",
      "train loss:0.010442185056442111\n",
      "train loss:0.010645038633613404\n",
      "train loss:0.010341108198397543\n",
      "train loss:0.01422484885814233\n",
      "train loss:0.004352950244777273\n",
      "train loss:0.01058655053492505\n",
      "train loss:0.02691144974051141\n",
      "train loss:0.006227307424110346\n",
      "train loss:0.004403699451970713\n",
      "train loss:0.01278826659074347\n",
      "train loss:0.020251911928211234\n",
      "train loss:0.002218778696614508\n",
      "train loss:0.0075704619468020765\n",
      "train loss:0.015285995311004927\n",
      "train loss:0.017601154869766635\n",
      "train loss:0.02490365398412496\n",
      "train loss:0.029434393849180056\n",
      "train loss:0.010275531192983155\n",
      "train loss:0.030267371723751456\n",
      "train loss:0.028299473521793263\n",
      "train loss:0.020830738376991068\n",
      "train loss:0.024275152287003415\n",
      "train loss:0.018502470390663724\n",
      "train loss:0.03281733980813168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0064706828420334535\n",
      "train loss:0.01233182016368514\n",
      "train loss:0.03444596942903201\n",
      "train loss:0.0223416882578353\n",
      "train loss:0.009598243810797725\n",
      "train loss:0.014720613708564761\n",
      "train loss:0.01372686747987093\n",
      "train loss:0.006605395749254177\n",
      "train loss:0.01997836275756359\n",
      "train loss:0.021041650169047306\n",
      "train loss:0.005790729507806006\n",
      "train loss:0.008690602468190754\n",
      "train loss:0.011318572459809521\n",
      "train loss:0.009912141215888386\n",
      "train loss:0.006979996566364711\n",
      "train loss:0.00602793431392969\n",
      "train loss:0.006108538805714021\n",
      "train loss:0.01286862630969635\n",
      "train loss:0.02431427975826219\n",
      "train loss:0.01083860395148886\n",
      "train loss:0.012982256270444019\n",
      "train loss:0.010063048808143347\n",
      "train loss:0.01612683302665208\n",
      "train loss:0.007505455037663486\n",
      "train loss:0.009703940067563827\n",
      "train loss:0.004037131319338594\n",
      "train loss:0.003359658224920635\n",
      "train loss:0.009336037460395297\n",
      "train loss:0.004626089586978497\n",
      "train loss:0.018123695988531353\n",
      "train loss:0.011515900749285357\n",
      "train loss:0.03340694876698246\n",
      "train loss:0.013537179217965682\n",
      "train loss:0.030744930599941277\n",
      "train loss:0.0033016699053956366\n",
      "train loss:0.008486014113662059\n",
      "train loss:0.006723628754974661\n",
      "train loss:0.007380684257484999\n",
      "train loss:0.004604850451408782\n",
      "train loss:0.012151825382620973\n",
      "train loss:0.013434087613509664\n",
      "train loss:0.01477117381646393\n",
      "train loss:0.032615146073876694\n",
      "train loss:0.0031761082810637053\n",
      "=== epoch:8, train acc:0.989, test acc:0.975 ===\n",
      "train loss:0.007672030613327807\n",
      "train loss:0.015109373523339277\n",
      "train loss:0.0269478604838751\n",
      "train loss:0.017201164875733248\n",
      "train loss:0.009474794206690089\n",
      "train loss:0.016837657131281852\n",
      "train loss:0.006450967335014372\n",
      "train loss:0.010363625074876466\n",
      "train loss:0.0035980173882582166\n",
      "train loss:0.009111008700736162\n",
      "train loss:0.012560031556972748\n",
      "train loss:0.021750736615134044\n",
      "train loss:0.013775858191478146\n",
      "train loss:0.01426759964045276\n",
      "train loss:0.018164843780425488\n",
      "train loss:0.009965715597875093\n",
      "train loss:0.01588767846131745\n",
      "train loss:0.02130538140432504\n",
      "train loss:0.017528909883250284\n",
      "train loss:0.010600225597473993\n",
      "train loss:0.00429047091914812\n",
      "train loss:0.012541606848927482\n",
      "train loss:0.010679969670518161\n",
      "train loss:0.013385858897694171\n",
      "train loss:0.014032779173054422\n",
      "train loss:0.010193133092862555\n",
      "train loss:0.03242860843764933\n",
      "train loss:0.008490326070674578\n",
      "train loss:0.005346235327947893\n",
      "train loss:0.005348616567397329\n",
      "train loss:0.0024981217584337636\n",
      "train loss:0.0032461832260016203\n",
      "train loss:0.030217171952052008\n",
      "train loss:0.03949603005575663\n",
      "train loss:0.008650531227759831\n",
      "train loss:0.007139888503672461\n",
      "train loss:0.008980140259079119\n",
      "train loss:0.0038166803334193705\n",
      "train loss:0.0037494193671767893\n",
      "train loss:0.01775739481185947\n",
      "train loss:0.007154630566296948\n",
      "train loss:0.005121757355365417\n",
      "train loss:0.0029419317883302965\n",
      "train loss:0.0071660584846877755\n",
      "train loss:0.0035453865849370977\n",
      "train loss:0.004689436158780931\n",
      "train loss:0.009070677946634454\n",
      "train loss:0.002012507507961746\n",
      "train loss:0.019098603405813444\n",
      "train loss:0.000708120227740352\n",
      "train loss:0.009074493341564526\n",
      "train loss:0.0092795120482563\n",
      "train loss:0.004047985674775562\n",
      "train loss:0.003211475316366704\n",
      "train loss:0.005868430953571734\n",
      "train loss:0.006268522135684478\n",
      "train loss:0.0011972673380957607\n",
      "train loss:0.0062689472605451\n",
      "train loss:0.00363497206864466\n",
      "train loss:0.012023787428654141\n",
      "train loss:0.006825958182451468\n",
      "train loss:0.027142634081292492\n",
      "train loss:0.016759097831535504\n",
      "train loss:0.0015594021332960212\n",
      "train loss:0.00795762920708036\n",
      "train loss:0.0009720170405442834\n",
      "train loss:0.004805846990647167\n",
      "train loss:0.013824088639219356\n",
      "train loss:0.005225446098005199\n",
      "train loss:0.00352478184651454\n",
      "train loss:0.018526093111417716\n",
      "train loss:0.007496074219503546\n",
      "train loss:0.0032158650651088407\n",
      "train loss:0.003210240887813994\n",
      "train loss:0.004765740427068166\n",
      "train loss:0.0025966679321082625\n",
      "train loss:0.026148068272471493\n",
      "train loss:0.005921866360802946\n",
      "train loss:0.008499046617952137\n",
      "train loss:0.0044034486204638686\n",
      "=== epoch:9, train acc:0.993, test acc:0.978 ===\n",
      "train loss:0.008204300938564477\n",
      "train loss:0.006353653275536199\n",
      "train loss:0.0032742154364346684\n",
      "train loss:0.012249339352731616\n",
      "train loss:0.008315927040950194\n",
      "train loss:0.011550355683362473\n",
      "train loss:0.00415309988907965\n",
      "train loss:0.0033649663137151754\n",
      "train loss:0.011347317492450106\n",
      "train loss:0.004796791475399731\n",
      "train loss:0.0029206901480557067\n",
      "train loss:0.016792203040270903\n",
      "train loss:0.017362041034100894\n",
      "train loss:0.023782267215897695\n",
      "train loss:0.005058394868677556\n",
      "train loss:0.0068852746329256\n",
      "train loss:0.004936977899688354\n",
      "train loss:0.0058259869596408356\n",
      "train loss:0.011168018161929527\n",
      "train loss:0.004489474747696451\n",
      "train loss:0.01069472982794073\n",
      "train loss:0.009704851565009519\n",
      "train loss:0.00479522322048169\n",
      "train loss:0.007208816947468249\n",
      "train loss:0.00510893895294709\n",
      "train loss:0.00955282507213543\n",
      "train loss:0.02637045163036736\n",
      "train loss:0.013662353410880047\n",
      "train loss:0.00701252306485352\n",
      "train loss:0.003931192351549325\n",
      "train loss:0.006529612626351905\n",
      "train loss:0.004990182621777319\n",
      "train loss:0.006102180008192954\n",
      "train loss:0.004381845125107192\n",
      "train loss:0.006733145202688819\n",
      "train loss:0.0028927612845837157\n",
      "train loss:0.01210776050365245\n",
      "train loss:0.016233470589234463\n",
      "train loss:0.00626448751959955\n",
      "train loss:0.00798371341093864\n",
      "train loss:0.02281056167598573\n",
      "train loss:0.003293434725184246\n",
      "train loss:0.012880465799678758\n",
      "train loss:0.010842010170213056\n",
      "train loss:0.0012296169006334068\n",
      "train loss:0.008736641218298404\n",
      "train loss:0.00594895436417969\n",
      "train loss:0.0032754358068099645\n",
      "train loss:0.011512571737431818\n",
      "train loss:0.013416864713891316\n",
      "train loss:0.013866221763436859\n",
      "train loss:0.0049694026493866065\n",
      "train loss:0.003948989445850061\n",
      "train loss:0.003993570392595096\n",
      "train loss:0.011983985622411281\n",
      "train loss:0.009790206231723789\n",
      "train loss:0.004652318635208299\n",
      "train loss:0.003603485696124274\n",
      "train loss:0.005319994169685472\n",
      "train loss:0.0035037907814715425\n",
      "train loss:0.009391762286058236\n",
      "train loss:0.007443151729620485\n",
      "train loss:0.012079781714538692\n",
      "train loss:0.005184244848230896\n",
      "train loss:0.004042875388664089\n",
      "train loss:0.006960578380054301\n",
      "train loss:0.002191141328752324\n",
      "train loss:0.00534501108148986\n",
      "train loss:0.0007329157133443099\n",
      "train loss:0.010119865675776503\n",
      "train loss:0.0033647312941304723\n",
      "train loss:0.009253271178248236\n",
      "train loss:0.0012066241079835428\n",
      "train loss:0.0060725626895319195\n",
      "train loss:0.022531464302089157\n",
      "train loss:0.005939372713791761\n",
      "train loss:0.009135653236972387\n",
      "train loss:0.003524469652593287\n",
      "train loss:0.019027616399810886\n",
      "train loss:0.004088286978618043\n",
      "=== epoch:10, train acc:0.997, test acc:0.977 ===\n",
      "train loss:0.009843710648264656\n",
      "train loss:0.002740672735735727\n",
      "train loss:0.0036516619776263064\n",
      "train loss:0.009893938588007833\n",
      "train loss:0.008644890748227137\n",
      "train loss:0.00463069901713645\n",
      "train loss:0.005054093587272165\n",
      "train loss:0.008359137981990991\n",
      "train loss:0.007933971095613016\n",
      "train loss:0.006669263043415795\n",
      "train loss:0.000258051221692099\n",
      "train loss:0.002560462637377257\n",
      "train loss:0.001905949074016447\n",
      "train loss:0.008785012095527246\n",
      "train loss:0.0038471321833267735\n",
      "train loss:0.006073011300206176\n",
      "train loss:0.00700944876507397\n",
      "train loss:0.0026192411913373635\n",
      "train loss:0.0017857161294093915\n",
      "train loss:0.0020291754429909332\n",
      "train loss:0.01249787743082365\n",
      "train loss:0.007523296916463101\n",
      "train loss:0.003388487720255139\n",
      "train loss:0.003015783731238156\n",
      "train loss:0.0013759791498504168\n",
      "train loss:0.003770892078555071\n",
      "train loss:0.0031778681549620626\n",
      "train loss:0.008007068087143581\n",
      "train loss:0.003245927397662043\n",
      "train loss:0.0031283989716411682\n",
      "train loss:0.001775561872448649\n",
      "train loss:0.0021950524071407043\n",
      "train loss:0.0053518483984132895\n",
      "train loss:0.00040233500469547147\n",
      "train loss:0.0019793485789552996\n",
      "train loss:0.0036316797587077317\n",
      "train loss:0.01057615500004006\n",
      "train loss:0.0012588296990410786\n",
      "train loss:0.0011748625424950797\n",
      "train loss:0.003949727820815849\n",
      "train loss:0.0014796465763044684\n",
      "train loss:0.0042829572681543394\n",
      "train loss:0.004324634952520688\n",
      "train loss:0.001743387246884619\n",
      "train loss:0.0038964331939666803\n",
      "train loss:0.002326048446408047\n",
      "train loss:0.010545541297959163\n",
      "train loss:0.0035580730437217356\n",
      "train loss:0.0014221712495590363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.005136098620929412\n",
      "train loss:0.0012109244281744255\n",
      "train loss:0.002845790647563246\n",
      "train loss:0.0026473840134086163\n",
      "train loss:0.0036836364981065574\n",
      "train loss:0.0021914814736713464\n",
      "train loss:0.007198252651636706\n",
      "train loss:0.004598251640821909\n",
      "train loss:0.0017384176848750086\n",
      "train loss:0.005700756636658688\n",
      "train loss:0.010885462911530275\n",
      "train loss:0.0038003534219396844\n",
      "train loss:0.00039470649792940386\n",
      "train loss:0.0007998927076913929\n",
      "train loss:0.0004606887932140527\n",
      "train loss:0.0015249256759311456\n",
      "train loss:0.005025901166646292\n",
      "train loss:0.0014804165236187935\n",
      "train loss:0.004595773109585035\n",
      "train loss:0.0015779698120828931\n",
      "train loss:0.0066092124996622115\n",
      "train loss:0.0008915092882222824\n",
      "train loss:0.005852102998635659\n",
      "train loss:0.00804492548134767\n",
      "train loss:0.0031112412837908383\n",
      "train loss:0.003726462656913327\n",
      "train loss:0.0038249678363868646\n",
      "train loss:0.0011438036457106762\n",
      "train loss:0.0042970203670099575\n",
      "train loss:0.0028721424664123973\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.98\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "\n",
    "network = CNN(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 20, 'filter_size': 9, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Momentum', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "MMT_train_acc=trainer.train_acc_list\n",
    "MMT_test_acc=trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD와 Momentum의 train_acc비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, SGD_train_acc, marker='o', label='SGD', markevery=2)\n",
    "plt.plot(x, MMT_train_acc, marker='s', label='MMT', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"train_accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD와 Momentum의 test_acc비교`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, SGD_test_acc, marker='o', label='SGD', markevery=2)\n",
    "plt.plot(x, MMT_test_acc, marker='s', label='MMT', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"test_accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD보다 Momentum을 사용했을 때 정확도가 더 빠르게 상승함을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. SGD와 Momentum의 10번 반복 및 정확도 그래프\n",
    "- 계산 시간이 오래 걸리기에 epoch=5로 설정한다\n",
    "- 시각화가 목적이므로 그래프를 위에 그리고 계산을 밑으로 내린다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반복하기 전 random 하게 train_test_data 를 10개 생성한다(random_state를 다르게 설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_0,x_test_0,t_train_0,t_test_0=train_test_split(X_data,t_data,test_size=0.2,shuffle=True,random_state=0)\n",
    "x_train_1,x_test_1,t_train_1,t_test_1=train_test_split(X_data,t_data,test_size=0.2,shuffle=True,random_state=1)\n",
    "x_train_2,x_test_2,t_train_2,t_test_2=train_test_split(X_data,t_data,test_size=0.2,shuffle=True,random_state=2)\n",
    "x_train_3,x_test_3,t_train_3,t_test_3=train_test_split(X_data,t_data,test_size=0.2,shuffle=True,random_state=3)\n",
    "x_train_4,x_test_4,t_train_4,t_test_4=train_test_split(X_data,t_data,test_size=0.2,shuffle=True,random_state=4)\n",
    "x_train_5,x_test_5,t_train_5,t_test_5=train_test_split(X_data,t_data,test_size=0.2,shuffle=True,random_state=5)\n",
    "x_train_6,x_test_6,t_train_6,t_test_6=train_test_split(X_data,t_data,test_size=0.2,shuffle=True,random_state=6)\n",
    "x_train_7,x_test_7,t_train_7,t_test_7=train_test_split(X_data,t_data,test_size=0.2,shuffle=True,random_state=7)\n",
    "x_train_8,x_test_8,t_train_8,t_test_8=train_test_split(X_data,t_data,test_size=0.2,shuffle=True,random_state=8)\n",
    "x_train_9,x_test_9,t_train_9,t_test_9=train_test_split(X_data,t_data,test_size=0.2,shuffle=True,random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.barh(range(10),MMT_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.barh(range(10),SGD_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5\n",
    "\n",
    "network = CNN(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 20, 'filter_size': 9, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "# 정확도 받아줄 list생성\n",
    "MMT_acc=[]\n",
    "SGD_acc=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3123621794828697\n",
      "=== epoch:1, train acc:0.081, test acc:0.096 ===\n",
      "train loss:2.2626616335936505\n",
      "train loss:2.2511257970383154\n",
      "train loss:2.2566507369915443\n",
      "train loss:2.229165795629021\n",
      "train loss:2.1636582473441175\n",
      "train loss:2.2431460767232836\n",
      "train loss:2.2092921062901327\n",
      "train loss:2.1835378017804055\n",
      "train loss:2.167888166987102\n",
      "train loss:2.181208763206406\n",
      "train loss:2.1361410710081845\n",
      "train loss:2.143502539913165\n",
      "train loss:2.1035920047445438\n",
      "train loss:2.1228886673767438\n",
      "train loss:2.1154610947203087\n",
      "train loss:2.023587621460314\n",
      "train loss:1.9836121755728562\n",
      "train loss:2.0145377183399895\n",
      "train loss:1.9851781637225878\n",
      "train loss:1.951300946447427\n",
      "train loss:1.9572042707910222\n",
      "train loss:1.8626737184569864\n",
      "train loss:1.8828726548947228\n",
      "train loss:1.7948407298175397\n",
      "train loss:1.7602488028740615\n",
      "train loss:1.8024847336476455\n",
      "train loss:1.7586424722332143\n",
      "train loss:1.6954636134500491\n",
      "train loss:1.6066656648849262\n",
      "train loss:1.5790393060585357\n",
      "train loss:1.4835385426913037\n",
      "train loss:1.5279599194643703\n",
      "train loss:1.4079773818028354\n",
      "train loss:1.4313845440635005\n",
      "train loss:1.3206675047712102\n",
      "train loss:1.1926727828252028\n",
      "train loss:1.2950575813489182\n",
      "train loss:1.187383064926659\n",
      "train loss:1.1771868821072633\n",
      "train loss:1.1226919983151307\n",
      "train loss:1.1197049899496263\n",
      "train loss:1.0874309920938494\n",
      "train loss:1.08133056749128\n",
      "train loss:1.0208101600973931\n",
      "train loss:0.927280720116499\n",
      "train loss:0.9603240342431492\n",
      "train loss:0.858787640403969\n",
      "train loss:0.957318914879026\n",
      "train loss:0.9657778271498613\n",
      "train loss:0.9278712418917666\n",
      "train loss:0.8280659234936023\n",
      "train loss:0.73109546190345\n",
      "train loss:0.7993305888934518\n",
      "train loss:0.7764856057371293\n",
      "train loss:0.7748274456330916\n",
      "train loss:0.6780639868574312\n",
      "train loss:0.7626980248904144\n",
      "train loss:0.6115805585574933\n",
      "train loss:0.6730521821465163\n",
      "train loss:0.6238457247122007\n",
      "train loss:0.679440874587755\n",
      "train loss:0.6494332250814446\n",
      "train loss:0.6086780060701794\n",
      "train loss:0.5480287406871579\n",
      "train loss:0.49052548835110465\n",
      "train loss:0.6208843100159844\n",
      "train loss:0.670438323361721\n",
      "train loss:0.5756969985525678\n",
      "train loss:0.4964481344547951\n",
      "train loss:0.6498913244581124\n",
      "train loss:0.5200985665528145\n",
      "train loss:0.45896882211573875\n",
      "train loss:0.6528885238986956\n",
      "train loss:0.5874391863678557\n",
      "train loss:0.4290281050060949\n",
      "train loss:0.40756767478714984\n",
      "train loss:0.6219477753007804\n",
      "train loss:0.5076338763834056\n",
      "train loss:0.4074389557867144\n",
      "train loss:0.45740042941608855\n",
      "=== epoch:2, train acc:0.842, test acc:0.855 ===\n",
      "train loss:0.4930275468902215\n",
      "train loss:0.5601204948451977\n",
      "train loss:0.45720999357165176\n",
      "train loss:0.4182729308150544\n",
      "train loss:0.46290223129809116\n",
      "train loss:0.36288091091014196\n",
      "train loss:0.28820260453982344\n",
      "train loss:0.38253262289078516\n",
      "train loss:0.3994705964109167\n",
      "train loss:0.44833156808760255\n",
      "train loss:0.6523097323888756\n",
      "train loss:0.4174332518940787\n",
      "train loss:0.4732050617978692\n",
      "train loss:0.4456402158786618\n",
      "train loss:0.3581921532207815\n",
      "train loss:0.45978801259851676\n",
      "train loss:0.3797124318348435\n",
      "train loss:0.43436570066800323\n",
      "train loss:0.4214651248882639\n",
      "train loss:0.42485937832947684\n",
      "train loss:0.278762498300852\n",
      "train loss:0.4288860839191228\n",
      "train loss:0.38025182783471156\n",
      "train loss:0.4628349001954614\n",
      "train loss:0.3307392688627166\n",
      "train loss:0.34356609189728393\n",
      "train loss:0.5165133771201487\n",
      "train loss:0.40701761981760703\n",
      "train loss:0.4948949126415449\n",
      "train loss:0.40602852601409717\n",
      "train loss:0.4764217965628211\n",
      "train loss:0.5083043610095378\n",
      "train loss:0.2991006340188167\n",
      "train loss:0.4760932250069178\n",
      "train loss:0.46699337457543927\n",
      "train loss:0.43704962498751526\n",
      "train loss:0.320381954457371\n",
      "train loss:0.3329637989368759\n",
      "train loss:0.3060511561218033\n",
      "train loss:0.33693938580483695\n",
      "train loss:0.31463148291065307\n",
      "train loss:0.3159679331836598\n",
      "train loss:0.5188732071931351\n",
      "train loss:0.30016476325181995\n",
      "train loss:0.42044164444040455\n",
      "train loss:0.29197360419708773\n",
      "train loss:0.31758137733555825\n",
      "train loss:0.33430751384849033\n",
      "train loss:0.4850642939004565\n",
      "train loss:0.4189454385978302\n",
      "train loss:0.5209705254756478\n",
      "train loss:0.43764714729625814\n",
      "train loss:0.4207987315475348\n",
      "train loss:0.4039672306044201\n",
      "train loss:0.37589409014334163\n",
      "train loss:0.4228418465001478\n",
      "train loss:0.35099927286972665\n",
      "train loss:0.2899498031926906\n",
      "train loss:0.3369175964023777\n",
      "train loss:0.3282845011910562\n",
      "train loss:0.26800434743290397\n",
      "train loss:0.29165923295812646\n",
      "train loss:0.3220430447253166\n",
      "train loss:0.3537351097387156\n",
      "train loss:0.36292481579103986\n",
      "train loss:0.21035003507069472\n",
      "train loss:0.2868108585279692\n",
      "train loss:0.35934768913664766\n",
      "train loss:0.2880230560797508\n",
      "train loss:0.2854832362265858\n",
      "train loss:0.40236097093190165\n",
      "train loss:0.2571854291131561\n",
      "train loss:0.3563370257656013\n",
      "train loss:0.25061863650310645\n",
      "train loss:0.3136350231293857\n",
      "train loss:0.2110749550902503\n",
      "train loss:0.3419362277747324\n",
      "train loss:0.21302710389872798\n",
      "train loss:0.2099886912068244\n",
      "train loss:0.31369137424843774\n",
      "=== epoch:3, train acc:0.885, test acc:0.895 ===\n",
      "train loss:0.3288371070587013\n",
      "train loss:0.3734243422972293\n",
      "train loss:0.29017758965967916\n",
      "train loss:0.366864167300752\n",
      "train loss:0.22645610187549764\n",
      "train loss:0.32843589304200904\n",
      "train loss:0.29789443200973353\n",
      "train loss:0.293208455305938\n",
      "train loss:0.2771337202282506\n",
      "train loss:0.31142084601510306\n",
      "train loss:0.2960574197751194\n",
      "train loss:0.2084229136107404\n",
      "train loss:0.26525919672050535\n",
      "train loss:0.3182455302455389\n",
      "train loss:0.36338499750282494\n",
      "train loss:0.3317976966402255\n",
      "train loss:0.4144643700330412\n",
      "train loss:0.17635697834583933\n",
      "train loss:0.2507021510348644\n",
      "train loss:0.3173214095971005\n",
      "train loss:0.2294801372211688\n",
      "train loss:0.24605867847007584\n",
      "train loss:0.276679107527912\n",
      "train loss:0.26923778832579065\n",
      "train loss:0.29957978526963663\n",
      "train loss:0.2730813343658264\n",
      "train loss:0.22768358141672082\n",
      "train loss:0.21980723070182573\n",
      "train loss:0.2114767509405554\n",
      "train loss:0.2527433971083441\n",
      "train loss:0.19900897446536817\n",
      "train loss:0.3021882194052611\n",
      "train loss:0.21621646489988108\n",
      "train loss:0.28924321096000377\n",
      "train loss:0.157928617407926\n",
      "train loss:0.316264574784976\n",
      "train loss:0.28562425828946086\n",
      "train loss:0.1325424037162217\n",
      "train loss:0.30709932105215887\n",
      "train loss:0.31500341426919787\n",
      "train loss:0.2917842300246058\n",
      "train loss:0.30508844996446244\n",
      "train loss:0.17442792970057105\n",
      "train loss:0.30969691512882297\n",
      "train loss:0.23985157532121237\n",
      "train loss:0.34903521539183285\n",
      "train loss:0.2696775895799256\n",
      "train loss:0.19965905743832532\n",
      "train loss:0.2852176345348715\n",
      "train loss:0.2253325450975706\n",
      "train loss:0.24159298927328518\n",
      "train loss:0.3087670268689363\n",
      "train loss:0.2960677384408578\n",
      "train loss:0.22747710010286715\n",
      "train loss:0.2535691022690788\n",
      "train loss:0.2976573581039627\n",
      "train loss:0.2393676074127411\n",
      "train loss:0.196394560630494\n",
      "train loss:0.329771682440609\n",
      "train loss:0.3375138595255793\n",
      "train loss:0.22355469203653247\n",
      "train loss:0.20012645247018918\n",
      "train loss:0.1938276625200146\n",
      "train loss:0.19773839564601958\n",
      "train loss:0.15948273222443907\n",
      "train loss:0.20647795196220925\n",
      "train loss:0.3245946592186189\n",
      "train loss:0.23560765818576562\n",
      "train loss:0.20239712532565363\n",
      "train loss:0.25422062201911255\n",
      "train loss:0.217065210763865\n",
      "train loss:0.18962947369705727\n",
      "train loss:0.21776129333249622\n",
      "train loss:0.3910784644847769\n",
      "train loss:0.2477418819223324\n",
      "train loss:0.22631445488009969\n",
      "train loss:0.304032411635513\n",
      "train loss:0.3307774768596597\n",
      "train loss:0.21635895788695222\n",
      "train loss:0.19173457891521717\n",
      "=== epoch:4, train acc:0.875, test acc:0.892 ===\n",
      "train loss:0.3067091306252879\n",
      "train loss:0.18167051807140805\n",
      "train loss:0.15408235091570904\n",
      "train loss:0.23240836219676794\n",
      "train loss:0.2086031865830804\n",
      "train loss:0.1630089742948603\n",
      "train loss:0.2014761463911438\n",
      "train loss:0.244201079549455\n",
      "train loss:0.20249814500321991\n",
      "train loss:0.38248361102134004\n",
      "train loss:0.2544998018613463\n",
      "train loss:0.14496779722328798\n",
      "train loss:0.2277325235062682\n",
      "train loss:0.24494821006374365\n",
      "train loss:0.302788838744781\n",
      "train loss:0.2710713159209839\n",
      "train loss:0.12823997922558866\n",
      "train loss:0.14873396493944446\n",
      "train loss:0.21649532320648152\n",
      "train loss:0.15740296398488282\n",
      "train loss:0.2624624412111222\n",
      "train loss:0.2636098340750542\n",
      "train loss:0.22260566686851305\n",
      "train loss:0.13619961125224594\n",
      "train loss:0.20071034124417347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.2923144498471644\n",
      "train loss:0.18733852167183823\n",
      "train loss:0.18215142055353875\n",
      "train loss:0.35957013972358476\n",
      "train loss:0.16566272420483177\n",
      "train loss:0.23002051283621172\n",
      "train loss:0.22382880284507933\n",
      "train loss:0.19643703518953742\n",
      "train loss:0.22795237533570537\n",
      "train loss:0.1735182699249517\n",
      "train loss:0.26866408047596735\n",
      "train loss:0.10810147408744379\n",
      "train loss:0.1710942259692412\n",
      "train loss:0.18962389257013024\n",
      "train loss:0.16182230388443994\n",
      "train loss:0.18852883493080305\n",
      "train loss:0.2458119512470201\n",
      "train loss:0.27034153596281985\n",
      "train loss:0.18413066258051247\n",
      "train loss:0.18164090528707127\n",
      "train loss:0.2105660984349665\n",
      "train loss:0.18196795760518672\n",
      "train loss:0.2709932390779511\n",
      "train loss:0.14611122542683294\n",
      "train loss:0.32963743936120626\n",
      "train loss:0.23475492683548665\n",
      "train loss:0.31210281836056974\n",
      "train loss:0.18491373579910944\n",
      "train loss:0.2342340900763287\n",
      "train loss:0.13310351198715192\n",
      "train loss:0.21043859153957553\n",
      "train loss:0.27849470477870747\n",
      "train loss:0.2399777869245269\n",
      "train loss:0.1946808844133425\n",
      "train loss:0.16686896798412068\n",
      "train loss:0.218164865853598\n",
      "train loss:0.20198276382707214\n",
      "train loss:0.290754393205005\n",
      "train loss:0.2737556666918676\n",
      "train loss:0.23174852067629562\n",
      "train loss:0.1415954329689198\n",
      "train loss:0.14349956434863764\n",
      "train loss:0.11251537410727723\n",
      "train loss:0.1322370200067628\n",
      "train loss:0.18665910511934883\n",
      "train loss:0.25466485199973726\n",
      "train loss:0.20486813256821987\n",
      "train loss:0.3125048234648183\n",
      "train loss:0.15681638680331175\n",
      "train loss:0.335355331506905\n",
      "train loss:0.20087685162823077\n",
      "train loss:0.12085750714459753\n",
      "train loss:0.291637658291832\n",
      "train loss:0.2229917732953407\n",
      "train loss:0.23279034470504947\n",
      "=== epoch:5, train acc:0.915, test acc:0.923 ===\n",
      "train loss:0.15010404719081227\n",
      "train loss:0.16209314374928446\n",
      "train loss:0.2615912586352699\n",
      "train loss:0.2252474363075445\n",
      "train loss:0.25977213943689537\n",
      "train loss:0.13815994071276227\n",
      "train loss:0.28361563036387805\n",
      "train loss:0.1598223912840079\n",
      "train loss:0.20567133236562726\n",
      "train loss:0.19393038547015926\n",
      "train loss:0.13704583176447974\n",
      "train loss:0.09070052814929695\n",
      "train loss:0.2929279536322708\n",
      "train loss:0.1861208621909246\n",
      "train loss:0.19905999616123013\n",
      "train loss:0.2297055796385014\n",
      "train loss:0.253827900748297\n",
      "train loss:0.1752543197387216\n",
      "train loss:0.202862892019472\n",
      "train loss:0.15964440665999266\n",
      "train loss:0.13713957682893077\n",
      "train loss:0.14301693872893925\n",
      "train loss:0.19137884467369687\n",
      "train loss:0.15389764865191277\n",
      "train loss:0.24161738381259984\n",
      "train loss:0.16516195240812848\n",
      "train loss:0.2059503166720954\n",
      "train loss:0.17583748840835198\n",
      "train loss:0.08916567144204618\n",
      "train loss:0.2725405928837872\n",
      "train loss:0.2077058344148881\n",
      "train loss:0.14686603095820935\n",
      "train loss:0.26757342495289793\n",
      "train loss:0.22195689857881662\n",
      "train loss:0.24150725119649508\n",
      "train loss:0.23228144605399156\n",
      "train loss:0.17010230871894932\n",
      "train loss:0.19812879247719284\n",
      "train loss:0.19505468632232137\n",
      "train loss:0.2862991566712705\n",
      "train loss:0.1715976996756906\n",
      "train loss:0.2135316874676865\n",
      "train loss:0.1590211332345994\n",
      "train loss:0.1859148628900961\n",
      "train loss:0.1403197102236936\n",
      "train loss:0.2296761025133448\n",
      "train loss:0.28068842334605487\n",
      "train loss:0.17921030836477342\n",
      "train loss:0.11746081805914656\n",
      "train loss:0.1725851182672666\n",
      "train loss:0.18616164211332006\n",
      "train loss:0.12005031225287345\n",
      "train loss:0.16250147832015346\n",
      "train loss:0.18984899968457689\n",
      "train loss:0.14794087619968954\n",
      "train loss:0.19212227637004034\n",
      "train loss:0.1350387368144981\n",
      "train loss:0.16878397379141086\n",
      "train loss:0.08011326412882022\n",
      "train loss:0.16451296922831948\n",
      "train loss:0.1531517310601212\n",
      "train loss:0.09877909333254017\n",
      "train loss:0.2109360745053979\n",
      "train loss:0.13238666431261562\n",
      "train loss:0.18350756679069502\n",
      "train loss:0.23377147170153564\n",
      "train loss:0.1859610082979535\n",
      "train loss:0.1680246889859982\n",
      "train loss:0.07632603235460868\n",
      "train loss:0.14024029068075974\n",
      "train loss:0.3687112786011387\n",
      "train loss:0.18673244069344377\n",
      "train loss:0.13302620813362245\n",
      "train loss:0.11924323710433464\n",
      "train loss:0.07958757900651899\n",
      "train loss:0.21645459683734264\n",
      "train loss:0.12766873003945423\n",
      "train loss:0.2005842363547412\n",
      "train loss:0.09082800980578742\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9345\n",
      "train loss:0.15390299239986932\n",
      "=== epoch:1, train acc:0.93, test acc:0.921 ===\n",
      "train loss:0.20761618404966703\n",
      "train loss:0.2810385303544389\n",
      "train loss:0.24627805439382\n",
      "train loss:0.19900245028311703\n",
      "train loss:0.28369678293900047\n",
      "train loss:0.16108553223727148\n",
      "train loss:0.32368319738058315\n",
      "train loss:0.33633130241193526\n",
      "train loss:0.3228354999885172\n",
      "train loss:0.2556971967108104\n",
      "train loss:0.43133362648353474\n",
      "train loss:0.24689862029610066\n",
      "train loss:0.29118222406826183\n",
      "train loss:0.21955400282061874\n",
      "train loss:0.35701263387622434\n",
      "train loss:0.3174281931044746\n",
      "train loss:0.20874110465277929\n",
      "train loss:0.3179423492824443\n",
      "train loss:0.37678919705483194\n",
      "train loss:0.22477787266660657\n",
      "train loss:0.22275084422356875\n",
      "train loss:0.23678463348060422\n",
      "train loss:0.34485655423258904\n",
      "train loss:0.32523101118251707\n",
      "train loss:0.4030669532344372\n",
      "train loss:0.2924223189314555\n",
      "train loss:0.27015930169673374\n",
      "train loss:0.2659135655256089\n",
      "train loss:0.3574193241054961\n",
      "train loss:0.19962405375364123\n",
      "train loss:0.27631211478000295\n",
      "train loss:0.1570562081169669\n",
      "train loss:0.19431640392531094\n",
      "train loss:0.17810724470867306\n",
      "train loss:0.21799465468145068\n",
      "train loss:0.2290732339489561\n",
      "train loss:0.21256082326911252\n",
      "train loss:0.20553746947570786\n",
      "train loss:0.15100261484451796\n",
      "train loss:0.08642314478541585\n",
      "train loss:0.1217307205023659\n",
      "train loss:0.10222858503851795\n",
      "train loss:0.15971980206643813\n",
      "train loss:0.3296781374380759\n",
      "train loss:0.27937645128524613\n",
      "train loss:0.1984361482556477\n",
      "train loss:0.09392608260199843\n",
      "train loss:0.17499720661579254\n",
      "train loss:0.20949926446361933\n",
      "train loss:0.10864796064251292\n",
      "train loss:0.30041065552261714\n",
      "train loss:0.13155188623816141\n",
      "train loss:0.11415319527136275\n",
      "train loss:0.07345832361337706\n",
      "train loss:0.12796590648097822\n",
      "train loss:0.11684727809905068\n",
      "train loss:0.10684493639391901\n",
      "train loss:0.18737802145144491\n",
      "train loss:0.10115734479325807\n",
      "train loss:0.1686513712216311\n",
      "train loss:0.10637563178342255\n",
      "train loss:0.17763464562243814\n",
      "train loss:0.18124563195254523\n",
      "train loss:0.06547330298859952\n",
      "train loss:0.08653929975005703\n",
      "train loss:0.12666616716006085\n",
      "train loss:0.2398208583203648\n",
      "train loss:0.1280875568601466\n",
      "train loss:0.15093297362059854\n",
      "train loss:0.19826858444210343\n",
      "train loss:0.07383668140064625\n",
      "train loss:0.12247687490732359\n",
      "train loss:0.11184101357618446\n",
      "train loss:0.17907123787521556\n",
      "train loss:0.07929379533427347\n",
      "train loss:0.243033634253933\n",
      "train loss:0.20151303045099578\n",
      "train loss:0.3251010770100416\n",
      "train loss:0.09173538957840396\n",
      "train loss:0.15801954826506331\n",
      "=== epoch:2, train acc:0.946, test acc:0.945 ===\n",
      "train loss:0.08845287712646685\n",
      "train loss:0.0814592275262308\n",
      "train loss:0.09219441707777826\n",
      "train loss:0.19106478807589852\n",
      "train loss:0.15473181485656975\n",
      "train loss:0.0902228605730657\n",
      "train loss:0.19373485356395964\n",
      "train loss:0.11876605169112167\n",
      "train loss:0.09486555652398725\n",
      "train loss:0.15918679532691526\n",
      "train loss:0.08410569479663652\n",
      "train loss:0.08492665423428063\n",
      "train loss:0.08679351979468851\n",
      "train loss:0.12870119011859368\n",
      "train loss:0.14404711231446016\n",
      "train loss:0.07603274421513101\n",
      "train loss:0.16489092950822734\n",
      "train loss:0.06878839003662383\n",
      "train loss:0.1562379182709861\n",
      "train loss:0.08298664092844932\n",
      "train loss:0.020503047196036878\n",
      "train loss:0.08304529587958079\n",
      "train loss:0.08571734599235116\n",
      "train loss:0.06654404093976463\n",
      "train loss:0.04787586493544858\n",
      "train loss:0.16966999602740224\n",
      "train loss:0.13509630997058075\n",
      "train loss:0.09486031712861837\n",
      "train loss:0.13084226956007583\n",
      "train loss:0.08394349059718145\n",
      "train loss:0.08256377802108736\n",
      "train loss:0.10608416806124195\n",
      "train loss:0.06620728828525477\n",
      "train loss:0.08196588185124971\n",
      "train loss:0.12174691858706216\n",
      "train loss:0.0733604989919649\n",
      "train loss:0.14564421644101047\n",
      "train loss:0.08734526131164069\n",
      "train loss:0.10238567316877942\n",
      "train loss:0.0537197337768773\n",
      "train loss:0.09883445689376888\n",
      "train loss:0.08087299265529171\n",
      "train loss:0.06517761299004535\n",
      "train loss:0.039601361424977215\n",
      "train loss:0.10166320386103306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.05515393837837572\n",
      "train loss:0.05841915214795192\n",
      "train loss:0.04845887834253415\n",
      "train loss:0.07286975634188597\n",
      "train loss:0.0877234042362657\n",
      "train loss:0.11270830426686716\n",
      "train loss:0.08825873066540799\n",
      "train loss:0.051697182227123724\n",
      "train loss:0.15789910235648658\n",
      "train loss:0.055507523227509264\n",
      "train loss:0.1087563652794169\n",
      "train loss:0.08863385406192896\n",
      "train loss:0.06924244178287715\n",
      "train loss:0.07528583867113581\n",
      "train loss:0.03331810473280005\n",
      "train loss:0.03973630115118569\n",
      "train loss:0.02041076489359404\n",
      "train loss:0.056185614824086454\n",
      "train loss:0.10127115892003592\n",
      "train loss:0.12937108670901126\n",
      "train loss:0.05473902549281512\n",
      "train loss:0.081840172607551\n",
      "train loss:0.07290520097482991\n",
      "train loss:0.13493230101274786\n",
      "train loss:0.04293817056374381\n",
      "train loss:0.1240892207624786\n",
      "train loss:0.04226177194706892\n",
      "train loss:0.11044204075622405\n",
      "train loss:0.06116722528754541\n",
      "train loss:0.03736118033798532\n",
      "train loss:0.08943062371338009\n",
      "train loss:0.07230338135565155\n",
      "train loss:0.030499018136617814\n",
      "train loss:0.05303520228263445\n",
      "train loss:0.06615582573281037\n",
      "=== epoch:3, train acc:0.966, test acc:0.958 ===\n",
      "train loss:0.04596661735811716\n",
      "train loss:0.03869559514696368\n",
      "train loss:0.06422987090108195\n",
      "train loss:0.05713352032242292\n",
      "train loss:0.12068790779815398\n",
      "train loss:0.05809293227863902\n",
      "train loss:0.05278462452867144\n",
      "train loss:0.049399070988590976\n",
      "train loss:0.05089569517757577\n",
      "train loss:0.06300111461681497\n",
      "train loss:0.023507021100765906\n",
      "train loss:0.08254697986743201\n",
      "train loss:0.01690071137410827\n",
      "train loss:0.04175137612265452\n",
      "train loss:0.13205337773236206\n",
      "train loss:0.0938375522540639\n",
      "train loss:0.0470656075879982\n",
      "train loss:0.023656392144094956\n",
      "train loss:0.1785360697435308\n",
      "train loss:0.031292943627659324\n",
      "train loss:0.019140178791157376\n",
      "train loss:0.0860071949834011\n",
      "train loss:0.04369571014023161\n",
      "train loss:0.039873680029257424\n",
      "train loss:0.033890411887230645\n",
      "train loss:0.035348113044312374\n",
      "train loss:0.03875558986983778\n",
      "train loss:0.11171934750312826\n",
      "train loss:0.07716253319099259\n",
      "train loss:0.033378442641495525\n",
      "train loss:0.03306793403675559\n",
      "train loss:0.038182263558673664\n",
      "train loss:0.1207141509045626\n",
      "train loss:0.01900631168555279\n",
      "train loss:0.048679651378373406\n",
      "train loss:0.027958729861969296\n",
      "train loss:0.03527728029552642\n",
      "train loss:0.03246806814802758\n",
      "train loss:0.07020031494202572\n",
      "train loss:0.03725290804794697\n",
      "train loss:0.03502568063676779\n",
      "train loss:0.013632000256843257\n",
      "train loss:0.07024434134609063\n",
      "train loss:0.038388183072105536\n",
      "train loss:0.045521059538861604\n",
      "train loss:0.04026453698404686\n",
      "train loss:0.018791214330467\n",
      "train loss:0.0362588217588845\n",
      "train loss:0.024644370249791173\n",
      "train loss:0.07903871438679062\n",
      "train loss:0.046232207674496664\n",
      "train loss:0.12047806234829908\n",
      "train loss:0.0729360705377679\n",
      "train loss:0.046137302715054666\n",
      "train loss:0.04702166542216807\n",
      "train loss:0.06601832146525984\n",
      "train loss:0.020946529283810557\n",
      "train loss:0.010334996612267498\n",
      "train loss:0.03707731664571289\n",
      "train loss:0.06787133876685256\n",
      "train loss:0.021173164140576996\n",
      "train loss:0.026865975118901386\n",
      "train loss:0.03948611712881669\n",
      "train loss:0.04389459142121424\n",
      "train loss:0.04334265845845573\n",
      "train loss:0.06014396380197089\n",
      "train loss:0.11740233658560223\n",
      "train loss:0.05411389399420466\n",
      "train loss:0.10439254264914022\n",
      "train loss:0.05119730369687425\n",
      "train loss:0.051015752263226216\n",
      "train loss:0.022074071347547686\n",
      "train loss:0.03169181232675578\n",
      "train loss:0.010756117640885778\n",
      "train loss:0.01754762581872319\n",
      "train loss:0.023048094056778358\n",
      "train loss:0.1101022493747986\n",
      "train loss:0.01742848604946218\n",
      "train loss:0.039149606107581066\n",
      "train loss:0.0390165915572888\n",
      "=== epoch:4, train acc:0.981, test acc:0.965 ===\n",
      "train loss:0.018679273424468946\n",
      "train loss:0.01576681078917603\n",
      "train loss:0.026019572938919256\n",
      "train loss:0.025309472350544544\n",
      "train loss:0.03508719283807433\n",
      "train loss:0.0649714231128233\n",
      "train loss:0.059817701471793445\n",
      "train loss:0.04450379433925897\n",
      "train loss:0.04865677883780442\n",
      "train loss:0.06460442944343986\n",
      "train loss:0.014948788689209616\n",
      "train loss:0.044780848025918855\n",
      "train loss:0.029850772309525198\n",
      "train loss:0.02288046165739703\n",
      "train loss:0.0642138745035581\n",
      "train loss:0.024794878961939774\n",
      "train loss:0.02691214126680865\n",
      "train loss:0.015154923901797603\n",
      "train loss:0.049424287034965576\n",
      "train loss:0.03592615372319399\n",
      "train loss:0.025490587841326046\n",
      "train loss:0.07717232529142587\n",
      "train loss:0.06497031263093456\n",
      "train loss:0.0104730331014475\n",
      "train loss:0.02814053713368803\n",
      "train loss:0.044030532744691736\n",
      "train loss:0.0307573015319336\n",
      "train loss:0.08646925075960522\n",
      "train loss:0.03967315239429496\n",
      "train loss:0.04816506142867433\n",
      "train loss:0.04177587547011031\n",
      "train loss:0.03167170749545856\n",
      "train loss:0.0528540190630149\n",
      "train loss:0.07127415054014832\n",
      "train loss:0.053327776536075115\n",
      "train loss:0.025463127973211513\n",
      "train loss:0.01020656183502667\n",
      "train loss:0.0657523712045088\n",
      "train loss:0.06039834495769813\n",
      "train loss:0.038170019455834224\n",
      "train loss:0.08623821721362405\n",
      "train loss:0.03700617077838942\n",
      "train loss:0.034515364463688955\n",
      "train loss:0.021387294907171363\n",
      "train loss:0.04472500972032511\n",
      "train loss:0.033052104474311735\n",
      "train loss:0.06513961329751984\n",
      "train loss:0.047702365731425826\n",
      "train loss:0.0382101147426288\n",
      "train loss:0.011617681273176741\n",
      "train loss:0.024751855456771908\n",
      "train loss:0.06380072397623295\n",
      "train loss:0.03624312651039725\n",
      "train loss:0.04408982217822767\n",
      "train loss:0.04103647418209214\n",
      "train loss:0.09049009692589863\n",
      "train loss:0.01824312761438586\n",
      "train loss:0.03752339228587541\n",
      "train loss:0.03844241214688207\n",
      "train loss:0.08489619507312912\n",
      "train loss:0.03494336458702332\n",
      "train loss:0.025923655805049992\n",
      "train loss:0.03011708057697481\n",
      "train loss:0.06111659762259805\n",
      "train loss:0.06297530002559965\n",
      "train loss:0.018974700750973076\n",
      "train loss:0.010407441575577183\n",
      "train loss:0.04277856515666754\n",
      "train loss:0.04138578740923237\n",
      "train loss:0.024585581448973347\n",
      "train loss:0.022335795477000944\n",
      "train loss:0.014114688404895069\n",
      "train loss:0.008222620250444714\n",
      "train loss:0.03339623605738515\n",
      "train loss:0.009885686195546904\n",
      "train loss:0.009742738860803856\n",
      "train loss:0.06252978126757197\n",
      "train loss:0.026438267090194204\n",
      "train loss:0.060765471741787154\n",
      "train loss:0.008721908110855478\n",
      "=== epoch:5, train acc:0.99, test acc:0.974 ===\n",
      "train loss:0.016019146117587\n",
      "train loss:0.01983706928216566\n",
      "train loss:0.026463328188032858\n",
      "train loss:0.038122027483859226\n",
      "train loss:0.03571758631010422\n",
      "train loss:0.037829041929500457\n",
      "train loss:0.014139056668647197\n",
      "train loss:0.03232663784178488\n",
      "train loss:0.035868552242215725\n",
      "train loss:0.011540517388237116\n",
      "train loss:0.0273884459378406\n",
      "train loss:0.03943114129225538\n",
      "train loss:0.03809020862915517\n",
      "train loss:0.011691623457614417\n",
      "train loss:0.03690032400394206\n",
      "train loss:0.0366528190002176\n",
      "train loss:0.02949100286103537\n",
      "train loss:0.028315731704510877\n",
      "train loss:0.023553070102217296\n",
      "train loss:0.009121158362718911\n",
      "train loss:0.040595387091143194\n",
      "train loss:0.03875972840774497\n",
      "train loss:0.026122609381792353\n",
      "train loss:0.028099231930937815\n",
      "train loss:0.020116612740059377\n",
      "train loss:0.038822748952503476\n",
      "train loss:0.01093725084529343\n",
      "train loss:0.02111738444209005\n",
      "train loss:0.04247135752455665\n",
      "train loss:0.005116743089067155\n",
      "train loss:0.010426095121660057\n",
      "train loss:0.037891751934691416\n",
      "train loss:0.025676820613406877\n",
      "train loss:0.014809314128646627\n",
      "train loss:0.02649958201786831\n",
      "train loss:0.06020936461952065\n",
      "train loss:0.054907929119525176\n",
      "train loss:0.01968322117658837\n",
      "train loss:0.03972620330301506\n",
      "train loss:0.02399257938363189\n",
      "train loss:0.021844838893113887\n",
      "train loss:0.023172946287576837\n",
      "train loss:0.023657177878035278\n",
      "train loss:0.013691097260464611\n",
      "train loss:0.01421326185443815\n",
      "train loss:0.061834094187039994\n",
      "train loss:0.08156861842783414\n",
      "train loss:0.056283108828467335\n",
      "train loss:0.020398728125939063\n",
      "train loss:0.05792047835088811\n",
      "train loss:0.062130274272615166\n",
      "train loss:0.02390496950978\n",
      "train loss:0.013959025213758287\n",
      "train loss:0.02367591219684565\n",
      "train loss:0.03424950112126254\n",
      "train loss:0.03723411374338061\n",
      "train loss:0.014631585935948075\n",
      "train loss:0.06014455228673122\n",
      "train loss:0.008484085877780389\n",
      "train loss:0.03528361993063799\n",
      "train loss:0.019958594591508602\n",
      "train loss:0.01215534187773583\n",
      "train loss:0.009669453312476711\n",
      "train loss:0.06738985554866579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02671825188704567\n",
      "train loss:0.018881136858677986\n",
      "train loss:0.005131040994195637\n",
      "train loss:0.019869762737761608\n",
      "train loss:0.007881083737345794\n",
      "train loss:0.03695320723721357\n",
      "train loss:0.04937050704862179\n",
      "train loss:0.027433085968217596\n",
      "train loss:0.02273877000172563\n",
      "train loss:0.018376674098687536\n",
      "train loss:0.015288488830326307\n",
      "train loss:0.021486277045211298\n",
      "train loss:0.03132373363339026\n",
      "train loss:0.008872782847622395\n",
      "train loss:0.020523804644408162\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9755\n"
     ]
    }
   ],
   "source": [
    "x_train=x_train_0\n",
    "x_test = x_test_0\n",
    "t_train=t_train_0\n",
    "t_test = t_test_0\n",
    "\n",
    "trainer_SGD = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='SGD', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_MMT = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Momentum', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_SGD.train()\n",
    "trainer_MMT.train()\n",
    "\n",
    "MMT_acc.append(trainer_MMT.test_acc_list[-1])\n",
    "SGD_acc.append(trainer_SGD.test_acc_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.019356740395505893\n",
      "=== epoch:1, train acc:0.988, test acc:0.991 ===\n",
      "train loss:0.012799358398021845\n",
      "train loss:0.02752705616233291\n",
      "train loss:0.03089538166989864\n",
      "train loss:0.08779262170276345\n",
      "train loss:0.01587871254043946\n",
      "train loss:0.01686594977298102\n",
      "train loss:0.006076779296157517\n",
      "train loss:0.022059937180391655\n",
      "train loss:0.009926258199984155\n",
      "train loss:0.05541661878026672\n",
      "train loss:0.014864517722029324\n",
      "train loss:0.00619936583842807\n",
      "train loss:0.00460483050222178\n",
      "train loss:0.026806928320209162\n",
      "train loss:0.005697096047862638\n",
      "train loss:0.0021277531865246248\n",
      "train loss:0.0026401878411610067\n",
      "train loss:0.010050812219239895\n",
      "train loss:0.010783733830749622\n",
      "train loss:0.024913201626710423\n",
      "train loss:0.032967919663749574\n",
      "train loss:0.011925706171278276\n",
      "train loss:0.006042208334841922\n",
      "train loss:0.016329326659992635\n",
      "train loss:0.007550822311372612\n",
      "train loss:0.004811327700615689\n",
      "train loss:0.022394456638523172\n",
      "train loss:0.008112841732719245\n",
      "train loss:0.007703876751160087\n",
      "train loss:0.018053268442247944\n",
      "train loss:0.03258962881941122\n",
      "train loss:0.013927191881722742\n",
      "train loss:0.009204075467920547\n",
      "train loss:0.05242514873595828\n",
      "train loss:0.015361503916413695\n",
      "train loss:0.007078460739790401\n",
      "train loss:0.014219549667872285\n",
      "train loss:0.007520124297325842\n",
      "train loss:0.0101141334386071\n",
      "train loss:0.006044353238531415\n",
      "train loss:0.06131045044022851\n",
      "train loss:0.007161921765045586\n",
      "train loss:0.009023743922124402\n",
      "train loss:0.00506521356397728\n",
      "train loss:0.016691223673660854\n",
      "train loss:0.015371053464935835\n",
      "train loss:0.01984803197992908\n",
      "train loss:0.009500397999028489\n",
      "train loss:0.011055420020035227\n",
      "train loss:0.01672845750189969\n",
      "train loss:0.009129895353890064\n",
      "train loss:0.030654315120788218\n",
      "train loss:0.014121073680912349\n",
      "train loss:0.06962173031014722\n",
      "train loss:0.024453252088756674\n",
      "train loss:0.03353208580183506\n",
      "train loss:0.009924513286396734\n",
      "train loss:0.013682231870985183\n",
      "train loss:0.042680408969212456\n",
      "train loss:0.008171515873809746\n",
      "train loss:0.022223100478689185\n",
      "train loss:0.018978017888564177\n",
      "train loss:0.004178085384439319\n",
      "train loss:0.0214647117803392\n",
      "train loss:0.01650707892564535\n",
      "train loss:0.004911249977373489\n",
      "train loss:0.030475509448702094\n",
      "train loss:0.03774993034517704\n",
      "train loss:0.007928035733528956\n",
      "train loss:0.007982538264514233\n",
      "train loss:0.07304280921756176\n",
      "train loss:0.011056468082568476\n",
      "train loss:0.006750117470038449\n",
      "train loss:0.007596617815487308\n",
      "train loss:0.010813774095736922\n",
      "train loss:0.009228074960575589\n",
      "train loss:0.016442707729750428\n",
      "train loss:0.009274002528656802\n",
      "train loss:0.01654662813819347\n",
      "train loss:0.004073584533129229\n",
      "=== epoch:2, train acc:0.997, test acc:0.992 ===\n",
      "train loss:0.014402243969544395\n",
      "train loss:0.013141101025343532\n",
      "train loss:0.002900312693366989\n",
      "train loss:0.011305133352062409\n",
      "train loss:0.006727762254938082\n",
      "train loss:0.021481590198212094\n",
      "train loss:0.015647047830146196\n",
      "train loss:0.006865397577347747\n",
      "train loss:0.021621519709848894\n",
      "train loss:0.008533536198011135\n",
      "train loss:0.014676241883592211\n",
      "train loss:0.0066101132209927145\n",
      "train loss:0.01075919722106674\n",
      "train loss:0.0038508969986897307\n",
      "train loss:0.01573548032717899\n",
      "train loss:0.011129124020956345\n",
      "train loss:0.012310678690925149\n",
      "train loss:0.007020322423483019\n",
      "train loss:0.0021457427070306163\n",
      "train loss:0.006562901282977773\n",
      "train loss:0.011064383379432332\n",
      "train loss:0.019855871859482847\n",
      "train loss:0.017517530060693308\n",
      "train loss:0.005262666059035816\n",
      "train loss:0.0010357912819388235\n",
      "train loss:0.019779720738416086\n",
      "train loss:0.002284058066142314\n",
      "train loss:0.04165834452785415\n",
      "train loss:0.013028999932610152\n",
      "train loss:0.006259490427285419\n",
      "train loss:0.05769076287179054\n",
      "train loss:0.00462934175170707\n",
      "train loss:0.01242247723073705\n",
      "train loss:0.011492950850250563\n",
      "train loss:0.015499648028938304\n",
      "train loss:0.01156930075561955\n",
      "train loss:0.006248810166443004\n",
      "train loss:0.018710714108750616\n",
      "train loss:0.002469606783537696\n",
      "train loss:0.006545226405935293\n",
      "train loss:0.044282388046348625\n",
      "train loss:0.010395163884148455\n",
      "train loss:0.023533762830517117\n",
      "train loss:0.022082058353531483\n",
      "train loss:0.0020779497722133314\n",
      "train loss:0.04113840065712197\n",
      "train loss:0.004184811434860772\n",
      "train loss:0.014040749800287586\n",
      "train loss:0.0038969017106175894\n",
      "train loss:0.010096246863438303\n",
      "train loss:0.010578361922157116\n",
      "train loss:0.05533070604049048\n",
      "train loss:0.006428149468168998\n",
      "train loss:0.019807821312409296\n",
      "train loss:0.005209036988800283\n",
      "train loss:0.013343390842230136\n",
      "train loss:0.013599924686121445\n",
      "train loss:0.003743418271199313\n",
      "train loss:0.012704807029574237\n",
      "train loss:0.009594090517793679\n",
      "train loss:0.021967181352593016\n",
      "train loss:0.013625652751625798\n",
      "train loss:0.056865163380861235\n",
      "train loss:0.028884778666685474\n",
      "train loss:0.013603978309680818\n",
      "train loss:0.015857068268093154\n",
      "train loss:0.0018141013213033303\n",
      "train loss:0.011860056636968637\n",
      "train loss:0.007358226738249119\n",
      "train loss:0.01969324567586618\n",
      "train loss:0.034012664450415946\n",
      "train loss:0.0325067675747319\n",
      "train loss:0.009157217824274949\n",
      "train loss:0.01515140141092379\n",
      "train loss:0.01862495683942438\n",
      "train loss:0.01265590586542906\n",
      "train loss:0.008523314078301803\n",
      "train loss:0.009008837830559415\n",
      "train loss:0.008520797875809486\n",
      "train loss:0.002950196368518555\n",
      "=== epoch:3, train acc:0.997, test acc:0.991 ===\n",
      "train loss:0.023253011289161886\n",
      "train loss:0.028588147997767516\n",
      "train loss:0.03407267505774653\n",
      "train loss:0.019547665901371185\n",
      "train loss:0.040734771827440605\n",
      "train loss:0.003908263229565173\n",
      "train loss:0.004630054011853387\n",
      "train loss:0.010602625473924187\n",
      "train loss:0.011451108546105964\n",
      "train loss:0.022699563705444726\n",
      "train loss:0.009083902244231416\n",
      "train loss:0.0060312440631529705\n",
      "train loss:0.011244054269693524\n",
      "train loss:0.009605576704580544\n",
      "train loss:0.009229939322745549\n",
      "train loss:0.00711721560756527\n",
      "train loss:0.057535306153728455\n",
      "train loss:0.01355412930035789\n",
      "train loss:0.010538167729508914\n",
      "train loss:0.012535694986620716\n",
      "train loss:0.036615509597681936\n",
      "train loss:0.009770348413947506\n",
      "train loss:0.012155468893082683\n",
      "train loss:0.006017655037560869\n",
      "train loss:0.005708563425170354\n",
      "train loss:0.008806707674410184\n",
      "train loss:0.025955067545983444\n",
      "train loss:0.03249588106269455\n",
      "train loss:0.0077248336858253795\n",
      "train loss:0.014018685830366289\n",
      "train loss:0.0017128338302013816\n",
      "train loss:0.00891742847460999\n",
      "train loss:0.03476683411158196\n",
      "train loss:0.06099997944447742\n",
      "train loss:0.008659911144856293\n",
      "train loss:0.0099461437737901\n",
      "train loss:0.016387905053142818\n",
      "train loss:0.015798791849777054\n",
      "train loss:0.014854415185459817\n",
      "train loss:0.012385563364915168\n",
      "train loss:0.004624279703909968\n",
      "train loss:0.002588075146490361\n",
      "train loss:0.012259124975828331\n",
      "train loss:0.033985980834777496\n",
      "train loss:0.01611872829692529\n",
      "train loss:0.006291866264634211\n",
      "train loss:0.008124710163469615\n",
      "train loss:0.011854311636724658\n",
      "train loss:0.013542182636069787\n",
      "train loss:0.012429184658710443\n",
      "train loss:0.00665705893021778\n",
      "train loss:0.003515622926404659\n",
      "train loss:0.010074092993141564\n",
      "train loss:0.008095466395007389\n",
      "train loss:0.018639382990740812\n",
      "train loss:0.009728146104921596\n",
      "train loss:0.005973310674038148\n",
      "train loss:0.00822247486137667\n",
      "train loss:0.0075299446129934375\n",
      "train loss:0.009313599591287757\n",
      "train loss:0.03204347756927849\n",
      "train loss:0.012329072693037015\n",
      "train loss:0.0038587675108994472\n",
      "train loss:0.015507980493713517\n",
      "train loss:0.007499239126615489\n",
      "train loss:0.014098743311957105\n",
      "train loss:0.002521974533423902\n",
      "train loss:0.012865159936120134\n",
      "train loss:0.006609177991484622\n",
      "train loss:0.03074186006838269\n",
      "train loss:0.02181983243466958\n",
      "train loss:0.0041915636631424434\n",
      "train loss:0.011834957783296357\n",
      "train loss:0.009631021084538893\n",
      "train loss:0.014562290696021435\n",
      "train loss:0.0038443255494630844\n",
      "train loss:0.01390549073819903\n",
      "train loss:0.004770550872833287\n",
      "train loss:0.010220733775005278\n",
      "train loss:0.018552970910821545\n",
      "=== epoch:4, train acc:0.997, test acc:0.991 ===\n",
      "train loss:0.006669808485452657\n",
      "train loss:0.01859412256740403\n",
      "train loss:0.007247807507574839\n",
      "train loss:0.014269966843868758\n",
      "train loss:0.003198455390141847\n",
      "train loss:0.0086160774585235\n",
      "train loss:0.003263471845416788\n",
      "train loss:0.006746679289299904\n",
      "train loss:0.007233690329331354\n",
      "train loss:0.011604327594628674\n",
      "train loss:0.014241844913556439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.01127042259617999\n",
      "train loss:0.011647075086212181\n",
      "train loss:0.00834590011463012\n",
      "train loss:0.010330990939130612\n",
      "train loss:0.010763318579251797\n",
      "train loss:0.0376163224147769\n",
      "train loss:0.013958153705464544\n",
      "train loss:0.012183839707128705\n",
      "train loss:0.023896791865840154\n",
      "train loss:0.0072802815188676715\n",
      "train loss:0.0019780857683535324\n",
      "train loss:0.0018129875448820235\n",
      "train loss:0.01446399371560153\n",
      "train loss:0.018357300977885873\n",
      "train loss:0.011317775097537614\n",
      "train loss:0.006065418580512798\n",
      "train loss:0.013227648257735057\n",
      "train loss:0.021916757317895722\n",
      "train loss:0.009139137659453596\n",
      "train loss:0.00777767989581716\n",
      "train loss:0.02227217753545725\n",
      "train loss:0.048824593490038655\n",
      "train loss:0.01221090160597067\n",
      "train loss:0.012018396059203502\n",
      "train loss:0.033629131012605076\n",
      "train loss:0.007271552908498157\n",
      "train loss:0.004702902733369686\n",
      "train loss:0.00615680648976121\n",
      "train loss:0.005740161472357398\n",
      "train loss:0.010496185637225772\n",
      "train loss:0.01180743169670108\n",
      "train loss:0.022031383600165415\n",
      "train loss:0.040233667271128724\n",
      "train loss:0.007068699447711797\n",
      "train loss:0.009762547364722146\n",
      "train loss:0.01178874073852542\n",
      "train loss:0.030053371219249093\n",
      "train loss:0.013811408346129556\n",
      "train loss:0.00860532848928662\n",
      "train loss:0.005530549733624459\n",
      "train loss:0.01695608615257271\n",
      "train loss:0.006274956098513779\n",
      "train loss:0.00842840757590875\n",
      "train loss:0.009005691257262843\n",
      "train loss:0.012334292327202711\n",
      "train loss:0.009289950430549287\n",
      "train loss:0.005306755468839851\n",
      "train loss:0.004479166104995669\n",
      "train loss:0.01839364690621464\n",
      "train loss:0.02245888927314185\n",
      "train loss:0.015293143760758205\n",
      "train loss:0.0019576416116708945\n",
      "train loss:0.01005261635263113\n",
      "train loss:0.003302209888231693\n",
      "train loss:0.01606350700273598\n",
      "train loss:0.007263914112531834\n",
      "train loss:0.05649164663734466\n",
      "train loss:0.01891565323593476\n",
      "train loss:0.0075074207292260145\n",
      "train loss:0.014191517893398974\n",
      "train loss:0.015768296872861364\n",
      "train loss:0.007723085117717672\n",
      "train loss:0.00846960583266288\n",
      "train loss:0.012958375893919601\n",
      "train loss:0.016870571926801206\n",
      "train loss:0.013038196402223378\n",
      "train loss:0.006756994938051399\n",
      "train loss:0.030945860447251405\n",
      "train loss:0.017303377697625712\n",
      "=== epoch:5, train acc:0.997, test acc:0.99 ===\n",
      "train loss:0.029942206988610155\n",
      "train loss:0.03462073172618499\n",
      "train loss:0.01693105825700622\n",
      "train loss:0.015463350295620949\n",
      "train loss:0.007368859486162469\n",
      "train loss:0.015758648585461013\n",
      "train loss:0.005270312433677856\n",
      "train loss:0.007755096933043153\n",
      "train loss:0.01070037515320295\n",
      "train loss:0.00863312438573384\n",
      "train loss:0.011918222613463923\n",
      "train loss:0.04792734314954125\n",
      "train loss:0.004254103943921346\n",
      "train loss:0.02354098171430095\n",
      "train loss:0.005484408864507462\n",
      "train loss:0.0025989286157724585\n",
      "train loss:0.006224722730584974\n",
      "train loss:0.007743669249325491\n",
      "train loss:0.020455163153778568\n",
      "train loss:0.005588729498178386\n",
      "train loss:0.008973522499248692\n",
      "train loss:0.005443672998979314\n",
      "train loss:0.005362877895143593\n",
      "train loss:0.005016673044768106\n",
      "train loss:0.01681458145689958\n",
      "train loss:0.012991444545183573\n",
      "train loss:0.01912923346919587\n",
      "train loss:0.02657539003785501\n",
      "train loss:0.009906002123421633\n",
      "train loss:0.0033778979698896\n",
      "train loss:0.013659449655516676\n",
      "train loss:0.0029945060847506575\n",
      "train loss:0.003486076763079377\n",
      "train loss:0.01664315517626394\n",
      "train loss:0.014860963745227494\n",
      "train loss:0.007691213678382871\n",
      "train loss:0.003918266209810763\n",
      "train loss:0.018087097291377627\n",
      "train loss:0.005381229748331937\n",
      "train loss:0.011149477473352112\n",
      "train loss:0.01252074748675168\n",
      "train loss:0.003723389879387044\n",
      "train loss:0.014365495278785296\n",
      "train loss:0.014208022015254935\n",
      "train loss:0.010049454535924082\n",
      "train loss:0.020221962092693703\n",
      "train loss:0.003286673333298273\n",
      "train loss:0.006327923422828946\n",
      "train loss:0.004428808050190849\n",
      "train loss:0.009561840515995386\n",
      "train loss:0.008969917097817925\n",
      "train loss:0.019118377279504436\n",
      "train loss:0.007360689770754044\n",
      "train loss:0.014274062161678527\n",
      "train loss:0.007635154596136648\n",
      "train loss:0.006204125606798073\n",
      "train loss:0.006457603175788322\n",
      "train loss:0.009721788704613644\n",
      "train loss:0.012915703907849588\n",
      "train loss:0.0392456010245658\n",
      "train loss:0.014796287066417304\n",
      "train loss:0.007467316442928875\n",
      "train loss:0.018641129595933403\n",
      "train loss:0.026596965963730484\n",
      "train loss:0.0169821891024466\n",
      "train loss:0.0112171187451632\n",
      "train loss:0.026213885062602728\n",
      "train loss:0.003943678079921868\n",
      "train loss:0.0063260147190068285\n",
      "train loss:0.003431377220238037\n",
      "train loss:0.010507155455346537\n",
      "train loss:0.01291273764554443\n",
      "train loss:0.014499203670359125\n",
      "train loss:0.008980711233458574\n",
      "train loss:0.015982594270172863\n",
      "train loss:0.014250983599543274\n",
      "train loss:0.01095508139482351\n",
      "train loss:0.004418520870486276\n",
      "train loss:0.005358740110477016\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9915\n",
      "train loss:0.004493130588154658\n",
      "=== epoch:1, train acc:0.997, test acc:0.991 ===\n",
      "train loss:0.01133018700061745\n",
      "train loss:0.012841643476521275\n",
      "train loss:0.004349718151697795\n",
      "train loss:0.010155830924902066\n",
      "train loss:0.019004149093676224\n",
      "train loss:0.02616775577927282\n",
      "train loss:0.008191948030232814\n",
      "train loss:0.003007345358714203\n",
      "train loss:0.00981322367034459\n",
      "train loss:0.009634042320584424\n",
      "train loss:0.007421156988531919\n",
      "train loss:0.003792740798541071\n",
      "train loss:0.010843576032334183\n",
      "train loss:0.012282467758828743\n",
      "train loss:0.029573039645340797\n",
      "train loss:0.014677514768709508\n",
      "train loss:0.013080712276580315\n",
      "train loss:0.006226180951463374\n",
      "train loss:0.00393368916135834\n",
      "train loss:0.022906746632493046\n",
      "train loss:0.002883134036839457\n",
      "train loss:0.008351523503549143\n",
      "train loss:0.008628809329546634\n",
      "train loss:0.007274406877900422\n",
      "train loss:0.025191113093538994\n",
      "train loss:0.009661691733285401\n",
      "train loss:0.016003833182805915\n",
      "train loss:0.0025035383599159135\n",
      "train loss:0.021174702690800542\n",
      "train loss:0.01745675637735718\n",
      "train loss:0.019413932601495962\n",
      "train loss:0.01083288495192907\n",
      "train loss:0.013957304657880929\n",
      "train loss:0.009982956435761664\n",
      "train loss:0.009548343000111698\n",
      "train loss:0.010192313302963654\n",
      "train loss:0.02186127235738087\n",
      "train loss:0.009429234072959619\n",
      "train loss:0.012907898735629298\n",
      "train loss:0.012092230844674132\n",
      "train loss:0.01778306956045165\n",
      "train loss:0.013231424730708468\n",
      "train loss:0.010704371716429757\n",
      "train loss:0.019800862982459824\n",
      "train loss:0.01089389529251722\n",
      "train loss:0.029841976734942044\n",
      "train loss:0.008066431540207012\n",
      "train loss:0.01632219300416636\n",
      "train loss:0.021600450579653457\n",
      "train loss:0.008271732519851365\n",
      "train loss:0.004239865044689344\n",
      "train loss:0.01790379225279787\n",
      "train loss:0.00776572263269867\n",
      "train loss:0.004811928559716355\n",
      "train loss:0.01089067083049548\n",
      "train loss:0.005689396297966249\n",
      "train loss:0.015459221882339273\n",
      "train loss:0.03772967953868615\n",
      "train loss:0.031240604536621495\n",
      "train loss:0.017496129710630887\n",
      "train loss:0.010228611088558912\n",
      "train loss:0.009107447781847314\n",
      "train loss:0.007550737865806531\n",
      "train loss:0.003053445571482437\n",
      "train loss:0.017655391534872947\n",
      "train loss:0.005507111854223398\n",
      "train loss:0.0331040968111136\n",
      "train loss:0.01579902951794884\n",
      "train loss:0.019123222689783975\n",
      "train loss:0.007633280778352983\n",
      "train loss:0.008453729669307387\n",
      "train loss:0.011133356907527618\n",
      "train loss:0.012857379842423305\n",
      "train loss:0.00976632391876916\n",
      "train loss:0.0034106264696946932\n",
      "train loss:0.009840063825571834\n",
      "train loss:0.0647719390426579\n",
      "train loss:0.010176749228126468\n",
      "train loss:0.040564746042957366\n",
      "train loss:0.006256021856277498\n",
      "=== epoch:2, train acc:0.993, test acc:0.986 ===\n",
      "train loss:0.005981866008015245\n",
      "train loss:0.017180437160393885\n",
      "train loss:0.010743294782624258\n",
      "train loss:0.008455826866312369\n",
      "train loss:0.006444389991527757\n",
      "train loss:0.02321579984626994\n",
      "train loss:0.006785813609107032\n",
      "train loss:0.013183889504781346\n",
      "train loss:0.00626541194363225\n",
      "train loss:0.01464552768250774\n",
      "train loss:0.009163039950878391\n",
      "train loss:0.011029625982885016\n",
      "train loss:0.018757019739563473\n",
      "train loss:0.034916544918248055\n",
      "train loss:0.009061366502928382\n",
      "train loss:0.008760224820871503\n",
      "train loss:0.025200390138906945\n",
      "train loss:0.008242572560445919\n",
      "train loss:0.015722048193108202\n",
      "train loss:0.005127832135954934\n",
      "train loss:0.02110063897443737\n",
      "train loss:0.006760786561205045\n",
      "train loss:0.0052669136779025785\n",
      "train loss:0.029347525717393196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.01247457345985515\n",
      "train loss:0.0128037554498048\n",
      "train loss:0.01863751474093305\n",
      "train loss:0.01983891540074188\n",
      "train loss:0.010269170515373151\n",
      "train loss:0.01099082827986547\n",
      "train loss:0.010212186753566914\n",
      "train loss:0.011232406564924058\n",
      "train loss:0.0035991981207621647\n",
      "train loss:0.0022252997899599026\n",
      "train loss:0.016832209283120438\n",
      "train loss:0.006945660342017457\n",
      "train loss:0.01183057496433662\n",
      "train loss:0.008253388519261549\n",
      "train loss:0.013283364933923791\n",
      "train loss:0.006496186697522796\n",
      "train loss:0.015321667054362227\n",
      "train loss:0.028920605382733507\n",
      "train loss:0.005362843025159824\n",
      "train loss:0.014437097500656401\n",
      "train loss:0.008776735423054205\n",
      "train loss:0.01744117840847841\n",
      "train loss:0.0035698862745510503\n",
      "train loss:0.00651529951940782\n",
      "train loss:0.002209034952952389\n",
      "train loss:0.011545635858158676\n",
      "train loss:0.009769442531333277\n",
      "train loss:0.002396075372968019\n",
      "train loss:0.0109114942713391\n",
      "train loss:0.028701618653838433\n",
      "train loss:0.009659786034274859\n",
      "train loss:0.030283195972586677\n",
      "train loss:0.007725853268773483\n",
      "train loss:0.002659183691735018\n",
      "train loss:0.013362132236457191\n",
      "train loss:0.009584402737825577\n",
      "train loss:0.019503635411075474\n",
      "train loss:0.006873673131204643\n",
      "train loss:0.008452199262146685\n",
      "train loss:0.0014265260602104666\n",
      "train loss:0.008298376875653385\n",
      "train loss:0.01075093821535745\n",
      "train loss:0.011468901110187479\n",
      "train loss:0.017107637983799977\n",
      "train loss:0.0056234967655638644\n",
      "train loss:0.003722743559560032\n",
      "train loss:0.00555571601879673\n",
      "train loss:0.009802077603066051\n",
      "train loss:0.004259783430718994\n",
      "train loss:0.011564711018075323\n",
      "train loss:0.015140177776398938\n",
      "train loss:0.00384116126019684\n",
      "train loss:0.01777793458513104\n",
      "train loss:0.005261929076742007\n",
      "train loss:0.005735931295037449\n",
      "train loss:0.017620990910436923\n",
      "=== epoch:3, train acc:0.996, test acc:0.986 ===\n",
      "train loss:0.01584817665761896\n",
      "train loss:0.007066704094694817\n",
      "train loss:0.016905970653251483\n",
      "train loss:0.003731430834348323\n",
      "train loss:0.014858097401031629\n",
      "train loss:0.024984804927977913\n",
      "train loss:0.007129592902955868\n",
      "train loss:0.013060318330566733\n",
      "train loss:0.0015764410424196618\n",
      "train loss:0.029342844844508775\n",
      "train loss:0.012724338327189865\n",
      "train loss:0.00656539153490005\n",
      "train loss:0.007168095569633518\n",
      "train loss:0.01556042459565639\n",
      "train loss:0.006969392594831545\n",
      "train loss:0.005825008399148886\n",
      "train loss:0.0178667966751765\n",
      "train loss:0.012179065620042267\n",
      "train loss:0.00802662429780552\n",
      "train loss:0.006584131216297659\n",
      "train loss:0.030030917233262483\n",
      "train loss:0.026951144053334236\n",
      "train loss:0.007120737855582501\n",
      "train loss:0.03545707535051992\n",
      "train loss:0.0032374688732817168\n",
      "train loss:0.008483613958154577\n",
      "train loss:0.03513353055191641\n",
      "train loss:0.006441852389096874\n",
      "train loss:0.005918018500852551\n",
      "train loss:0.01714161411424317\n",
      "train loss:0.007362224747863913\n",
      "train loss:0.005891978308188916\n",
      "train loss:0.022086082601655996\n",
      "train loss:0.01750363591913713\n",
      "train loss:0.011060041417231894\n",
      "train loss:0.033632774891561454\n",
      "train loss:0.009759877195277978\n",
      "train loss:0.011075135958234266\n",
      "train loss:0.011564039542712973\n",
      "train loss:0.007008547964447223\n",
      "train loss:0.027802895197461597\n",
      "train loss:0.028057003162399296\n",
      "train loss:0.03358944210390531\n",
      "train loss:0.005738420291559874\n",
      "train loss:0.009121103926306934\n",
      "train loss:0.006228661885061862\n",
      "train loss:0.005066435853086959\n",
      "train loss:0.00933957246134352\n",
      "train loss:0.004083926849248737\n",
      "train loss:0.022462076633369823\n",
      "train loss:0.024010194647050436\n",
      "train loss:0.008232710191255965\n",
      "train loss:0.008942282165864776\n",
      "train loss:0.012107723422342911\n",
      "train loss:0.002573499576853132\n",
      "train loss:0.009193682747211608\n",
      "train loss:0.0031194796156610795\n",
      "train loss:0.020796668072042387\n",
      "train loss:0.005550380420713683\n",
      "train loss:0.010214248866891444\n",
      "train loss:0.025322762829230436\n",
      "train loss:0.009366751804746741\n",
      "train loss:0.004619517921547226\n",
      "train loss:0.003590970070213624\n",
      "train loss:0.004236453375896237\n",
      "train loss:0.010861446077861523\n",
      "train loss:0.005630005383476058\n",
      "train loss:0.016726337635499702\n",
      "train loss:0.013223466824445678\n",
      "train loss:0.0037808351274205066\n",
      "train loss:0.003973114304874315\n",
      "train loss:0.009031810845112443\n",
      "train loss:0.0019694442728518057\n",
      "train loss:0.013807829485210811\n",
      "train loss:0.0031117713907094172\n",
      "train loss:0.02183408866061036\n",
      "train loss:0.016852615110605856\n",
      "train loss:0.0035637673919378748\n",
      "train loss:0.005777867903674489\n",
      "train loss:0.018220543597442986\n",
      "=== epoch:4, train acc:0.997, test acc:0.984 ===\n",
      "train loss:0.00889460854521801\n",
      "train loss:0.010456668599647929\n",
      "train loss:0.017018062337737908\n",
      "train loss:0.0019044073051824003\n",
      "train loss:0.010639170773150656\n",
      "train loss:0.007634907411384745\n",
      "train loss:0.00961838885022689\n",
      "train loss:0.011034612453281679\n",
      "train loss:0.006279069606620081\n",
      "train loss:0.014801616501692656\n",
      "train loss:0.006468609036591387\n",
      "train loss:0.004467917799258375\n",
      "train loss:0.007748366759231505\n",
      "train loss:0.0033996604535552278\n",
      "train loss:0.010978071168734877\n",
      "train loss:0.00522193282877763\n",
      "train loss:0.007239238423076788\n",
      "train loss:0.011221853811665001\n",
      "train loss:0.008069088729088597\n",
      "train loss:0.003801424916708129\n",
      "train loss:0.007150480875734001\n",
      "train loss:0.011330229140905573\n",
      "train loss:0.006324038062693961\n",
      "train loss:0.0019084073627130618\n",
      "train loss:0.0036384797236874355\n",
      "train loss:0.006740114857849205\n",
      "train loss:0.004362054701016634\n",
      "train loss:0.006231708448965402\n",
      "train loss:0.0032220001279847843\n",
      "train loss:0.01252866773836285\n",
      "train loss:0.004220708272274737\n",
      "train loss:0.00155258578020104\n",
      "train loss:0.005880898404143028\n",
      "train loss:0.0034037290589363223\n",
      "train loss:0.007889381916434481\n",
      "train loss:0.02437999916403491\n",
      "train loss:0.009054867172821654\n",
      "train loss:0.0008863079423149215\n",
      "train loss:0.012854394836553788\n",
      "train loss:0.010901335192914348\n",
      "train loss:0.0047405935558059295\n",
      "train loss:0.0033322131746474537\n",
      "train loss:0.01895120905443604\n",
      "train loss:0.00848104091450654\n",
      "train loss:0.007952900776322025\n",
      "train loss:0.002558622928187508\n",
      "train loss:0.0031338700899959694\n",
      "train loss:0.024976113938899425\n",
      "train loss:0.0020556488671295294\n",
      "train loss:0.0009516717732601748\n",
      "train loss:0.0022276044596126836\n",
      "train loss:0.0013861998841276629\n",
      "train loss:0.007445780923963202\n",
      "train loss:0.0072431677612063506\n",
      "train loss:0.0016386648700050194\n",
      "train loss:0.022154081170465835\n",
      "train loss:0.002605887521864767\n",
      "train loss:0.0015909873524005268\n",
      "train loss:0.0010663623941554845\n",
      "train loss:0.011741235070341709\n",
      "train loss:0.005606648124714551\n",
      "train loss:0.005349705482312906\n",
      "train loss:0.005641859583379113\n",
      "train loss:0.0033393110423581996\n",
      "train loss:0.006493191518588904\n",
      "train loss:0.0014100906182364815\n",
      "train loss:0.003155768086619501\n",
      "train loss:0.0042753117684739515\n",
      "train loss:0.00604071169198394\n",
      "train loss:0.00154616963725727\n",
      "train loss:0.002308532362891262\n",
      "train loss:0.005278559547791856\n",
      "train loss:0.004796177982657304\n",
      "train loss:0.0036950585744791797\n",
      "train loss:0.002048171572136842\n",
      "train loss:0.003829741010431405\n",
      "train loss:0.002209883097421079\n",
      "train loss:0.002316820695051738\n",
      "train loss:0.003959808039130047\n",
      "train loss:0.00766685882170547\n",
      "=== epoch:5, train acc:0.998, test acc:0.986 ===\n",
      "train loss:0.0022148907096693647\n",
      "train loss:0.002745651314457761\n",
      "train loss:0.0030012117484198284\n",
      "train loss:0.004923319077037176\n",
      "train loss:0.004540710944900207\n",
      "train loss:0.004036585398548161\n",
      "train loss:0.001575988383415751\n",
      "train loss:0.0016056798743349335\n",
      "train loss:0.018981261445173644\n",
      "train loss:0.0048252482660084105\n",
      "train loss:0.0014145570948077668\n",
      "train loss:0.00710835519463591\n",
      "train loss:0.0015644667023397992\n",
      "train loss:0.0017076798375833058\n",
      "train loss:0.0019030146245388605\n",
      "train loss:0.003329017809662823\n",
      "train loss:0.0064336605138822275\n",
      "train loss:0.007268482631687631\n",
      "train loss:0.005949182573057512\n",
      "train loss:0.0065562684897507075\n",
      "train loss:0.011360264215051742\n",
      "train loss:0.0053676025408177196\n",
      "train loss:0.006328624098209092\n",
      "train loss:0.0011378495352432816\n",
      "train loss:0.0009241298711445411\n",
      "train loss:0.007511452377750577\n",
      "train loss:0.00396897723181617\n",
      "train loss:0.008158522211673529\n",
      "train loss:0.006326379548898833\n",
      "train loss:0.0027899028196759156\n",
      "train loss:0.0022830905616410676\n",
      "train loss:0.002650535865463309\n",
      "train loss:0.006900665314262504\n",
      "train loss:0.0018979916753706363\n",
      "train loss:0.0046559346854043475\n",
      "train loss:0.006166752805387896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0014378138004152825\n",
      "train loss:0.0015720650861626654\n",
      "train loss:0.008963313263539344\n",
      "train loss:0.0015778141817251532\n",
      "train loss:0.01002820854632779\n",
      "train loss:0.005737180994029405\n",
      "train loss:0.005685617702714769\n",
      "train loss:0.003985952388449806\n",
      "train loss:0.003219376362359391\n",
      "train loss:0.009348632905067093\n",
      "train loss:0.001073544584546969\n",
      "train loss:0.008055998052799057\n",
      "train loss:0.02966966589028232\n",
      "train loss:0.00781202821040106\n",
      "train loss:0.004104551747987203\n",
      "train loss:0.0037194434159519806\n",
      "train loss:0.0035794237675820396\n",
      "train loss:0.0059762548996112564\n",
      "train loss:0.007306482813511716\n",
      "train loss:0.006543120686325139\n",
      "train loss:0.006257806646389823\n",
      "train loss:0.004027595163933981\n",
      "train loss:0.0007125104709765399\n",
      "train loss:0.000884370588542105\n",
      "train loss:0.0011470450510254523\n",
      "train loss:0.007917051491105564\n",
      "train loss:0.003380110516993608\n",
      "train loss:0.011359039476662076\n",
      "train loss:0.0029530463016597965\n",
      "train loss:0.0017956655243230996\n",
      "train loss:0.005964778379099557\n",
      "train loss:0.004971050093775505\n",
      "train loss:0.0026802782797679505\n",
      "train loss:0.001402379072902345\n",
      "train loss:0.0021316463307391644\n",
      "train loss:0.005206410153127001\n",
      "train loss:0.0021068542033385058\n",
      "train loss:0.0010015966336212154\n",
      "train loss:0.002706408198274331\n",
      "train loss:0.0010288713421630605\n",
      "train loss:0.0011636182538862527\n",
      "train loss:0.0016931164230055437\n",
      "train loss:0.0021443092348072956\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.988\n"
     ]
    }
   ],
   "source": [
    "x_train=x_train_1\n",
    "x_test = x_test_1\n",
    "t_train=t_train_1\n",
    "t_test = t_test_1\n",
    "\n",
    "trainer_SGD = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='SGD', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_MMT = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Momentum', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_SGD.train()\n",
    "trainer_MMT.train()\n",
    "\n",
    "MMT_acc.append(trainer_MMT.test_acc_list[-1])\n",
    "SGD_acc.append(trainer_SGD.test_acc_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.005826673972004559\n",
      "=== epoch:1, train acc:0.998, test acc:0.996 ===\n",
      "train loss:0.00995993141771814\n",
      "train loss:0.0054301741816742835\n",
      "train loss:0.0036040382096343554\n",
      "train loss:0.004592185995988287\n",
      "train loss:0.002491066058770273\n",
      "train loss:0.0026306141631222617\n",
      "train loss:0.006185773086370485\n",
      "train loss:0.021837501350643943\n",
      "train loss:0.0022062504891996633\n",
      "train loss:0.003861297574010483\n",
      "train loss:0.0008931220018894659\n",
      "train loss:0.0060655601579289965\n",
      "train loss:0.0025316722622458015\n",
      "train loss:0.0026728134652653423\n",
      "train loss:0.0011743388069750298\n",
      "train loss:0.009847087316520174\n",
      "train loss:0.008747698292672797\n",
      "train loss:0.004062110047644195\n",
      "train loss:0.004434099561823576\n",
      "train loss:0.000640709323435195\n",
      "train loss:0.0035217564074054588\n",
      "train loss:0.0043427308206641195\n",
      "train loss:0.004523641083584531\n",
      "train loss:0.011683850638468926\n",
      "train loss:0.019325418500408195\n",
      "train loss:0.008443860442529533\n",
      "train loss:0.0024751296716762758\n",
      "train loss:0.005981405941989894\n",
      "train loss:0.0014025777878100274\n",
      "train loss:0.00948320760810056\n",
      "train loss:0.014485271499817466\n",
      "train loss:0.045006833268665114\n",
      "train loss:0.003206248716734653\n",
      "train loss:0.0060339936744158405\n",
      "train loss:0.036635466224649554\n",
      "train loss:0.0004686921839605456\n",
      "train loss:0.003563649999026377\n",
      "train loss:0.005521046764794839\n",
      "train loss:0.0011165196589065223\n",
      "train loss:0.0017301028026222454\n",
      "train loss:0.0014668301731355512\n",
      "train loss:0.00863928322677569\n",
      "train loss:0.004058080445692124\n",
      "train loss:0.0009644475273178073\n",
      "train loss:0.000669082690069555\n",
      "train loss:0.003307969912521247\n",
      "train loss:0.00495561802722808\n",
      "train loss:0.002911688015233019\n",
      "train loss:0.0038005572265361496\n",
      "train loss:0.0022707229874252876\n",
      "train loss:0.0016851715276091053\n",
      "train loss:0.011628150500983142\n",
      "train loss:0.005427318327246503\n",
      "train loss:0.002901299377883932\n",
      "train loss:0.0007227589513418228\n",
      "train loss:0.0020973713818788313\n",
      "train loss:0.002564364528207139\n",
      "train loss:0.0037121792127475343\n",
      "train loss:0.0036682996079468633\n",
      "train loss:0.0009311592718684353\n",
      "train loss:0.0061202627388849485\n",
      "train loss:0.001882769110898374\n",
      "train loss:0.00035208832296729204\n",
      "train loss:0.0008879956199403168\n",
      "train loss:0.005662770375060284\n",
      "train loss:0.0016547483632588448\n",
      "train loss:0.004887183822874354\n",
      "train loss:0.010261950008691243\n",
      "train loss:0.0033788072329348023\n",
      "train loss:0.002621661615382492\n",
      "train loss:0.0025504262584927716\n",
      "train loss:0.0014417523593084716\n",
      "train loss:0.0029446933309944527\n",
      "train loss:0.0025293575094511174\n",
      "train loss:0.004910199597400763\n",
      "train loss:0.004291785632761106\n",
      "train loss:0.0025516950336798945\n",
      "train loss:0.0018861438727494342\n",
      "train loss:0.0036989088963497807\n",
      "train loss:0.003182749908084937\n",
      "=== epoch:2, train acc:0.998, test acc:0.998 ===\n",
      "train loss:0.0026013875309619807\n",
      "train loss:0.004134584617510846\n",
      "train loss:0.00388803778685873\n",
      "train loss:0.0028422888107600503\n",
      "train loss:0.0007680072156862818\n",
      "train loss:0.004375102315060446\n",
      "train loss:0.00234385646028255\n",
      "train loss:0.0014280491424369496\n",
      "train loss:0.0017431596451432372\n",
      "train loss:0.0017623683925807746\n",
      "train loss:0.0033575040376655815\n",
      "train loss:0.005400502268439852\n",
      "train loss:0.0022865502246225337\n",
      "train loss:0.0011424256804640684\n",
      "train loss:0.003469222482044212\n",
      "train loss:0.0011667118414627378\n",
      "train loss:0.026552270985178386\n",
      "train loss:0.004252962301470743\n",
      "train loss:0.002142532340220967\n",
      "train loss:0.00226660400800172\n",
      "train loss:0.004518771014575751\n",
      "train loss:0.00448457818127461\n",
      "train loss:0.0017254926627163327\n",
      "train loss:0.0020281849492163706\n",
      "train loss:0.0008528688582218847\n",
      "train loss:0.002234638236428079\n",
      "train loss:0.004057247689447553\n",
      "train loss:0.007622238615473013\n",
      "train loss:0.002871373404328716\n",
      "train loss:0.0019019188431329576\n",
      "train loss:0.002939988592668138\n",
      "train loss:0.0007101247457194788\n",
      "train loss:0.0022829051695257337\n",
      "train loss:0.0007030451897815039\n",
      "train loss:0.016169969600379565\n",
      "train loss:0.0017065524044174755\n",
      "train loss:0.0005147290424789054\n",
      "train loss:0.0015854418116170266\n",
      "train loss:0.003306497174880319\n",
      "train loss:0.000760456833704962\n",
      "train loss:0.004879899880439863\n",
      "train loss:0.0009170915643530357\n",
      "train loss:0.0008071896313375814\n",
      "train loss:0.0018966105080661742\n",
      "train loss:0.005395758580126276\n",
      "train loss:0.0010987200122399435\n",
      "train loss:0.004094271143221105\n",
      "train loss:0.004908473465938681\n",
      "train loss:0.0008524106589386524\n",
      "train loss:0.0019874407683750377\n",
      "train loss:0.0017635215635679285\n",
      "train loss:0.0037284679967963755\n",
      "train loss:0.003500476823978983\n",
      "train loss:0.0027446882936338363\n",
      "train loss:0.0012698961161965667\n",
      "train loss:0.0015777526769036093\n",
      "train loss:0.004056368052756993\n",
      "train loss:0.014658787687343204\n",
      "train loss:0.0029305448201741506\n",
      "train loss:0.005056548870366324\n",
      "train loss:0.002510577766787915\n",
      "train loss:0.0035462763175905464\n",
      "train loss:0.0011406973262107652\n",
      "train loss:0.0014208308228059712\n",
      "train loss:0.001608068400354127\n",
      "train loss:0.0008478776430373835\n",
      "train loss:0.0009018615163021812\n",
      "train loss:0.0023480681525430724\n",
      "train loss:0.0033906371563572966\n",
      "train loss:0.0030345868858955983\n",
      "train loss:0.002020512427772416\n",
      "train loss:0.004553507940062683\n",
      "train loss:0.0031683509626762025\n",
      "train loss:0.00937051209323984\n",
      "train loss:0.0014103380169947551\n",
      "train loss:0.005130655858178252\n",
      "train loss:0.003993814140769481\n",
      "train loss:0.0019971355899659157\n",
      "train loss:0.0017673449499125712\n",
      "train loss:0.001313987779345803\n",
      "=== epoch:3, train acc:0.998, test acc:0.996 ===\n",
      "train loss:0.0033139512199931\n",
      "train loss:0.006112237940798405\n",
      "train loss:0.0005358953208214274\n",
      "train loss:0.00215664674190007\n",
      "train loss:0.0057037874803524844\n",
      "train loss:0.0010842901666327455\n",
      "train loss:0.0026620077712648993\n",
      "train loss:0.0047475778041905625\n",
      "train loss:0.009040623773378739\n",
      "train loss:0.006331063826409102\n",
      "train loss:0.001699263134341482\n",
      "train loss:0.0013522131050205514\n",
      "train loss:0.004134051076178738\n",
      "train loss:0.004385575159709975\n",
      "train loss:0.0011727680529644937\n",
      "train loss:0.03712503796182264\n",
      "train loss:0.011404923711790188\n",
      "train loss:0.0021257408741829965\n",
      "train loss:0.0048416168682413364\n",
      "train loss:0.005218621730319387\n",
      "train loss:0.026292083031506527\n",
      "train loss:0.0025141826013801295\n",
      "train loss:0.0038660707413838576\n",
      "train loss:0.003636224518795334\n",
      "train loss:0.001487346012950065\n",
      "train loss:0.0056598946824553\n",
      "train loss:0.0013875257385871774\n",
      "train loss:0.0018267241061160967\n",
      "train loss:0.00724204238947217\n",
      "train loss:0.004913888605785611\n",
      "train loss:0.001684489106207342\n",
      "train loss:0.0009157852615899795\n",
      "train loss:0.005313327156624695\n",
      "train loss:0.004244215728630959\n",
      "train loss:0.0047202834961021425\n",
      "train loss:0.006015338307864791\n",
      "train loss:0.001873138521345399\n",
      "train loss:0.0029870150974485137\n",
      "train loss:0.0014568664430660747\n",
      "train loss:0.0026344347801903383\n",
      "train loss:0.0025036275572512916\n",
      "train loss:0.002103136068585468\n",
      "train loss:0.005569460027798715\n",
      "train loss:0.0012574521945858677\n",
      "train loss:0.0032948235986245255\n",
      "train loss:0.0057152500788488535\n",
      "train loss:0.0022461280675004315\n",
      "train loss:0.000960094196861399\n",
      "train loss:0.0009535625611684094\n",
      "train loss:0.0018558043232166127\n",
      "train loss:0.0032684486918387956\n",
      "train loss:0.0012345752046836148\n",
      "train loss:0.001821773465529487\n",
      "train loss:0.00631418913062445\n",
      "train loss:0.02707568264778304\n",
      "train loss:0.001772868568226355\n",
      "train loss:0.0015505877398679347\n",
      "train loss:0.0014002070793615662\n",
      "train loss:0.0043599245655497575\n",
      "train loss:0.005137361201383608\n",
      "train loss:0.002787374857270852\n",
      "train loss:0.003590613822493961\n",
      "train loss:0.0028021379042338663\n",
      "train loss:0.003156252931326917\n",
      "train loss:0.009162852101562343\n",
      "train loss:0.0013864558921571665\n",
      "train loss:0.005858731058527138\n",
      "train loss:0.002628481292209844\n",
      "train loss:0.004951001938098635\n",
      "train loss:0.0022457480500087126\n",
      "train loss:0.0010729805614533834\n",
      "train loss:0.0014381293802330684\n",
      "train loss:0.002081664824447448\n",
      "train loss:0.002925448560934895\n",
      "train loss:0.009529106995679484\n",
      "train loss:0.004062059551242402\n",
      "train loss:0.00193087899792222\n",
      "train loss:0.0025065854815684574\n",
      "train loss:0.0009630171293566128\n",
      "train loss:0.0028912330034070875\n",
      "=== epoch:4, train acc:1.0, test acc:0.997 ===\n",
      "train loss:0.004324220143328112\n",
      "train loss:0.0014547457410344683\n",
      "train loss:0.006053218678949375\n",
      "train loss:0.011019004512488807\n",
      "train loss:0.0007232382509112522\n",
      "train loss:0.004794365554543481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0007883316737789023\n",
      "train loss:0.0007061121037718614\n",
      "train loss:0.002142470422782599\n",
      "train loss:0.00026442757016688167\n",
      "train loss:0.0017247311075274272\n",
      "train loss:0.02363174514960801\n",
      "train loss:0.006862977098271185\n",
      "train loss:0.000520371005878151\n",
      "train loss:0.0024499485185510744\n",
      "train loss:0.0002709316520529678\n",
      "train loss:0.023610406772581034\n",
      "train loss:0.0009978780139808109\n",
      "train loss:0.001687065118551975\n",
      "train loss:0.000860536485569066\n",
      "train loss:0.0008425320868179557\n",
      "train loss:0.003077338072638011\n",
      "train loss:0.0011297915178242138\n",
      "train loss:0.0015298878749360892\n",
      "train loss:0.0014477030911264552\n",
      "train loss:0.002507399135985417\n",
      "train loss:0.003354146631410631\n",
      "train loss:0.002915818513297762\n",
      "train loss:0.004708323925725291\n",
      "train loss:0.0017895973256900311\n",
      "train loss:0.004486745598217944\n",
      "train loss:0.0020288855395295723\n",
      "train loss:0.0016721689223116144\n",
      "train loss:0.001178261467618276\n",
      "train loss:0.0010679890002294684\n",
      "train loss:0.006404325477894586\n",
      "train loss:0.004241678570991991\n",
      "train loss:0.005734820501183838\n",
      "train loss:0.002645196307808886\n",
      "train loss:0.0017263673115230115\n",
      "train loss:0.0008267891741555581\n",
      "train loss:0.001099690100570378\n",
      "train loss:0.0017127720477885093\n",
      "train loss:0.003663190838367323\n",
      "train loss:0.0021977013110106524\n",
      "train loss:0.002629886249330089\n",
      "train loss:0.004844295687669117\n",
      "train loss:0.0006532757130394332\n",
      "train loss:0.0016697889503398036\n",
      "train loss:0.0008566476876638174\n",
      "train loss:0.005324663997789456\n",
      "train loss:0.002219614093281944\n",
      "train loss:0.0019415476330552884\n",
      "train loss:0.0015929453018615925\n",
      "train loss:0.0006377739925583238\n",
      "train loss:0.004887208174801761\n",
      "train loss:0.0013315257477969017\n",
      "train loss:0.003017736090398426\n",
      "train loss:0.0011069911417260182\n",
      "train loss:0.0028363515272962726\n",
      "train loss:0.00111059288555556\n",
      "train loss:0.004738747981438856\n",
      "train loss:0.003753667738259822\n",
      "train loss:0.001709785220102416\n",
      "train loss:0.0008837855445124041\n",
      "train loss:0.0036896274950727736\n",
      "train loss:0.0037516940410978975\n",
      "train loss:0.01014668746483046\n",
      "train loss:0.005520368619073107\n",
      "train loss:0.003621602774381574\n",
      "train loss:0.008174606824200095\n",
      "train loss:0.002442760994169403\n",
      "train loss:0.002459430341148797\n",
      "train loss:0.019041195862625114\n",
      "train loss:0.002474185791054233\n",
      "train loss:0.0019407873775144004\n",
      "train loss:0.0014457967187189037\n",
      "train loss:0.004309739554582604\n",
      "train loss:0.0021737209828829396\n",
      "train loss:0.002477193886759186\n",
      "=== epoch:5, train acc:1.0, test acc:0.997 ===\n",
      "train loss:0.0007033183410827025\n",
      "train loss:0.016303895114003965\n",
      "train loss:0.0006082681229315581\n",
      "train loss:0.013592459018019436\n",
      "train loss:0.0033069947730070743\n",
      "train loss:0.0024248587189398918\n",
      "train loss:0.002088270080931251\n",
      "train loss:0.001071488540927198\n",
      "train loss:0.004875911776611001\n",
      "train loss:0.0032009039003037566\n",
      "train loss:0.0012707508408542842\n",
      "train loss:0.001244790496014746\n",
      "train loss:0.0033843073055152117\n",
      "train loss:0.0005662921905571336\n",
      "train loss:0.002484372448834423\n",
      "train loss:0.0017220394503118185\n",
      "train loss:0.0031165208777675997\n",
      "train loss:0.0039005495484232744\n",
      "train loss:0.0011507071187575899\n",
      "train loss:0.0025569551425324304\n",
      "train loss:0.0010494915193287014\n",
      "train loss:0.0006581017356901094\n",
      "train loss:0.001416136957917425\n",
      "train loss:0.0022415762991406414\n",
      "train loss:0.0013446501080330757\n",
      "train loss:0.0012849130978048956\n",
      "train loss:0.003863247762329941\n",
      "train loss:0.0018523267838861002\n",
      "train loss:0.003001119028939912\n",
      "train loss:0.000645713438873574\n",
      "train loss:0.007535479329007864\n",
      "train loss:0.000995526225845778\n",
      "train loss:0.001493092979079059\n",
      "train loss:0.0144798459741681\n",
      "train loss:0.0030000229731744783\n",
      "train loss:0.0025579386790748285\n",
      "train loss:0.0011716523974190213\n",
      "train loss:0.0006053283069981887\n",
      "train loss:0.0006136974536564248\n",
      "train loss:0.005283139787341624\n",
      "train loss:0.002038614198985125\n",
      "train loss:0.0024187424861391244\n",
      "train loss:0.0018559702105010898\n",
      "train loss:0.004932464031812354\n",
      "train loss:0.0030366445259527534\n",
      "train loss:0.003440999135430449\n",
      "train loss:0.0030183918709330666\n",
      "train loss:0.0014791371639720971\n",
      "train loss:0.003734094128607469\n",
      "train loss:0.02615895262363106\n",
      "train loss:0.0017527578250179942\n",
      "train loss:0.003971168098351555\n",
      "train loss:0.003450974557081943\n",
      "train loss:0.0015933419382461909\n",
      "train loss:0.003044588777130197\n",
      "train loss:0.0021433763615246867\n",
      "train loss:0.0031823880625787703\n",
      "train loss:0.0009141204204665573\n",
      "train loss:0.001739758296750101\n",
      "train loss:0.002014857843247352\n",
      "train loss:0.0019731531394694903\n",
      "train loss:0.004542630545379136\n",
      "train loss:0.005792637466963599\n",
      "train loss:0.0003125380409512792\n",
      "train loss:0.00198770593021869\n",
      "train loss:0.0003780515963232143\n",
      "train loss:0.0023970994957783543\n",
      "train loss:0.003035744251934005\n",
      "train loss:0.0007176856226884799\n",
      "train loss:0.0021524825605546156\n",
      "train loss:0.0017044607033630437\n",
      "train loss:0.008472709828957292\n",
      "train loss:0.004173134100907754\n",
      "train loss:0.0073334890185958014\n",
      "train loss:0.0010482526021947736\n",
      "train loss:0.00137996329936185\n",
      "train loss:0.00286261709821283\n",
      "train loss:0.0018835844971571908\n",
      "train loss:0.001625104387131266\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9975\n",
      "train loss:0.0018771229246702552\n",
      "=== epoch:1, train acc:1.0, test acc:0.997 ===\n",
      "train loss:0.004684455863594373\n",
      "train loss:0.0019369529827686506\n",
      "train loss:0.013582641407049649\n",
      "train loss:0.0035552087046537366\n",
      "train loss:0.003293123929923112\n",
      "train loss:0.006184293238740736\n",
      "train loss:0.0025961862645891903\n",
      "train loss:0.001326173913992893\n",
      "train loss:0.0075551237656968214\n",
      "train loss:0.0037433151717034302\n",
      "train loss:0.007240192894888401\n",
      "train loss:0.0020547439717465855\n",
      "train loss:0.0024568998546348375\n",
      "train loss:0.002178460792605934\n",
      "train loss:0.003591406936866655\n",
      "train loss:0.002963835475873357\n",
      "train loss:0.002262319145134018\n",
      "train loss:0.00195384413510706\n",
      "train loss:0.0032327506619089425\n",
      "train loss:0.0030552566239573985\n",
      "train loss:0.001813143411527838\n",
      "train loss:0.003918182377247423\n",
      "train loss:0.003907355719337947\n",
      "train loss:0.0050912051805634476\n",
      "train loss:0.0022727639480867186\n",
      "train loss:0.0037384529361372985\n",
      "train loss:0.005544555312220745\n",
      "train loss:0.004303929699034498\n",
      "train loss:0.0020348186362501154\n",
      "train loss:0.0005221371332749622\n",
      "train loss:0.005102108777511996\n",
      "train loss:0.002407299102251146\n",
      "train loss:0.0012894625769562956\n",
      "train loss:0.006795145189140825\n",
      "train loss:0.0008266760368659587\n",
      "train loss:0.0028553246658311977\n",
      "train loss:0.0005625364105414405\n",
      "train loss:0.0041278105846137255\n",
      "train loss:0.0024646360636048515\n",
      "train loss:0.002087615487363986\n",
      "train loss:0.021127703883841956\n",
      "train loss:0.00477861084981785\n",
      "train loss:0.001069265876188168\n",
      "train loss:0.002189884549072312\n",
      "train loss:0.0014389640537108602\n",
      "train loss:0.0015180876463480179\n",
      "train loss:0.012241100387003219\n",
      "train loss:0.002652027483063747\n",
      "train loss:0.0033162567578448724\n",
      "train loss:0.0014286747769672653\n",
      "train loss:0.0035573339312226925\n",
      "train loss:0.0019346312320847373\n",
      "train loss:0.0017131557096357695\n",
      "train loss:0.0027587139465544237\n",
      "train loss:0.0030703889627601176\n",
      "train loss:0.0013101658313738405\n",
      "train loss:0.001395642703368358\n",
      "train loss:0.001552728296139249\n",
      "train loss:0.004591871687707658\n",
      "train loss:0.0011867702300747678\n",
      "train loss:0.0019460581240427991\n",
      "train loss:0.00520457499595633\n",
      "train loss:0.002225645068940898\n",
      "train loss:0.0011498160244176974\n",
      "train loss:0.001752021715977578\n",
      "train loss:0.0022253059172226007\n",
      "train loss:0.003629445902220503\n",
      "train loss:0.00021167283691079155\n",
      "train loss:0.003747471358756499\n",
      "train loss:0.0035315406293195974\n",
      "train loss:0.00027714641721256797\n",
      "train loss:0.0014787881169619817\n",
      "train loss:0.0027847499354554895\n",
      "train loss:0.002687779402841755\n",
      "train loss:0.001491779002785875\n",
      "train loss:0.0022904686986692786\n",
      "train loss:0.003301060154895624\n",
      "train loss:0.0018978609128957188\n",
      "train loss:0.0030524945393413756\n",
      "train loss:0.004031505418140561\n",
      "=== epoch:2, train acc:1.0, test acc:0.996 ===\n",
      "train loss:0.0012525984690158274\n",
      "train loss:0.0033190046840609337\n",
      "train loss:0.001405467881882069\n",
      "train loss:0.001657814578622118\n",
      "train loss:0.0006318011102069307\n",
      "train loss:0.001347742919020131\n",
      "train loss:0.004051940396470618\n",
      "train loss:0.005625227040874712\n",
      "train loss:0.0024654916482713244\n",
      "train loss:0.0022847564714970283\n",
      "train loss:0.006786335878273842\n",
      "train loss:0.0009915740420305666\n",
      "train loss:0.009625930553650073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0026811603223029523\n",
      "train loss:0.0021250207591812552\n",
      "train loss:0.007675525628433857\n",
      "train loss:0.00124175032579256\n",
      "train loss:0.004865325995789833\n",
      "train loss:0.0013613442762058583\n",
      "train loss:0.0005571828661281805\n",
      "train loss:0.0029288923160667698\n",
      "train loss:0.003141295346578484\n",
      "train loss:0.001441151992659345\n",
      "train loss:0.0033527242947338585\n",
      "train loss:0.00402084392655094\n",
      "train loss:0.0061889135100825195\n",
      "train loss:0.0027779314907518658\n",
      "train loss:0.007132254448041369\n",
      "train loss:0.0006976705391349644\n",
      "train loss:0.010458578764449606\n",
      "train loss:0.0038024693830511942\n",
      "train loss:0.007418506134440647\n",
      "train loss:0.00607788918009958\n",
      "train loss:0.007439125599325786\n",
      "train loss:0.011683253014813258\n",
      "train loss:0.003942925893214841\n",
      "train loss:0.0030241323640199883\n",
      "train loss:0.004336736962083794\n",
      "train loss:0.001215702580334832\n",
      "train loss:0.012958035733048648\n",
      "train loss:0.003040958298350666\n",
      "train loss:0.004842152225664013\n",
      "train loss:0.0045245376613522684\n",
      "train loss:0.003918534684779824\n",
      "train loss:0.005793992546273969\n",
      "train loss:0.005586272494739079\n",
      "train loss:0.007326843992432825\n",
      "train loss:0.004640065247223418\n",
      "train loss:0.0033589098314477234\n",
      "train loss:0.011841252961675923\n",
      "train loss:0.00846172593620418\n",
      "train loss:0.0006916299311129744\n",
      "train loss:0.005263609848724015\n",
      "train loss:0.0006348319803679066\n",
      "train loss:0.0031727597585775925\n",
      "train loss:0.0014048637352841987\n",
      "train loss:0.007691892674170175\n",
      "train loss:0.000698541744432638\n",
      "train loss:0.006141008433237635\n",
      "train loss:0.0016194518959704832\n",
      "train loss:0.0031009063904280515\n",
      "train loss:0.0033679674501430733\n",
      "train loss:0.0033347632524616277\n",
      "train loss:0.006546514375315842\n",
      "train loss:0.002351845250514357\n",
      "train loss:0.0015492936907862674\n",
      "train loss:0.006279641623499813\n",
      "train loss:0.004182391773423133\n",
      "train loss:0.009552788418513626\n",
      "train loss:0.002556307734228287\n",
      "train loss:0.004247198839743303\n",
      "train loss:0.002823368768008584\n",
      "train loss:0.0024028614763558663\n",
      "train loss:0.0005787534405543677\n",
      "train loss:0.0017944662412462315\n",
      "train loss:0.011311335791611756\n",
      "train loss:0.007792910996013638\n",
      "train loss:0.0012352887977892732\n",
      "train loss:0.004291006557891603\n",
      "train loss:0.003933344263877301\n",
      "=== epoch:3, train acc:1.0, test acc:0.997 ===\n",
      "train loss:0.0026356414563338737\n",
      "train loss:0.0032778308218548912\n",
      "train loss:0.0034132047081456207\n",
      "train loss:0.016183290664612833\n",
      "train loss:0.002843161116570657\n",
      "train loss:0.008067033878037038\n",
      "train loss:0.0007418772841317306\n",
      "train loss:0.0013389686079360622\n",
      "train loss:0.014133292639825634\n",
      "train loss:0.0028138195555530337\n",
      "train loss:0.00206668172466355\n",
      "train loss:0.0023227366292592057\n",
      "train loss:0.03655583143589132\n",
      "train loss:0.010159588094261339\n",
      "train loss:0.0017309075422491562\n",
      "train loss:0.004873009880842813\n",
      "train loss:0.0020347477309349236\n",
      "train loss:0.003021971605444223\n",
      "train loss:0.00304916488477201\n",
      "train loss:0.003056804906204425\n",
      "train loss:0.004274573817147025\n",
      "train loss:0.010677204480112456\n",
      "train loss:0.004288821668242596\n",
      "train loss:0.0017265017859926272\n",
      "train loss:0.0031082065403884363\n",
      "train loss:0.002563632595976247\n",
      "train loss:0.005550662456928836\n",
      "train loss:0.002859165438137109\n",
      "train loss:0.0014668605840811551\n",
      "train loss:0.0022411731641279365\n",
      "train loss:0.0020294287118702943\n",
      "train loss:0.006271141951717682\n",
      "train loss:0.01380300155168388\n",
      "train loss:0.002379952531349523\n",
      "train loss:0.001218435130131373\n",
      "train loss:0.0029794063039154457\n",
      "train loss:0.0003265562920799215\n",
      "train loss:0.01098626787342535\n",
      "train loss:0.02306975937379572\n",
      "train loss:0.0010464087433740825\n",
      "train loss:0.0021150758756701003\n",
      "train loss:0.005015251619156879\n",
      "train loss:0.002651919243568367\n",
      "train loss:0.0020957059989624312\n",
      "train loss:0.007331717316512534\n",
      "train loss:0.001558964629815568\n",
      "train loss:0.0012862355658380428\n",
      "train loss:0.0011089122382156047\n",
      "train loss:0.004515557299972053\n",
      "train loss:0.0025684247717349985\n",
      "train loss:0.0025408328783401756\n",
      "train loss:0.0027261521759397718\n",
      "train loss:0.0029017233639930615\n",
      "train loss:0.0012457675515976541\n",
      "train loss:0.004499018180820406\n",
      "train loss:0.0024282885952366974\n",
      "train loss:0.0022857063729091734\n",
      "train loss:0.0009315074412757672\n",
      "train loss:0.002476470207940862\n",
      "train loss:0.003068951217743615\n",
      "train loss:0.0016325348691294595\n",
      "train loss:0.0024523774288847475\n",
      "train loss:0.0004019035363578094\n",
      "train loss:0.0019272102641765155\n",
      "train loss:0.00039207245415734805\n",
      "train loss:0.0035801168444510425\n",
      "train loss:0.0012169859008113507\n",
      "train loss:0.002717234548392921\n",
      "train loss:0.005840037147985576\n",
      "train loss:0.0009871134233305505\n",
      "train loss:0.0037201293242934384\n",
      "train loss:0.0020054933567346654\n",
      "train loss:0.0013862227801376978\n",
      "train loss:0.001431170353903462\n",
      "train loss:0.002509764294810102\n",
      "train loss:0.0009268968779011774\n",
      "train loss:0.0028497252597815183\n",
      "train loss:0.001313639121633771\n",
      "train loss:0.0016011828964008718\n",
      "train loss:0.0010310168631428559\n",
      "=== epoch:4, train acc:1.0, test acc:0.994 ===\n",
      "train loss:0.0011066159112561604\n",
      "train loss:0.0011263851905428226\n",
      "train loss:0.004329291094784032\n",
      "train loss:0.0014834059974017217\n",
      "train loss:0.00020221763635711994\n",
      "train loss:0.0019596393072866458\n",
      "train loss:0.0013826865452622181\n",
      "train loss:0.001790233969412112\n",
      "train loss:0.0010841100705558755\n",
      "train loss:0.0007662234755519996\n",
      "train loss:0.0006958901596350974\n",
      "train loss:0.0030752940113731976\n",
      "train loss:0.0037550021543860984\n",
      "train loss:0.0015657353179400716\n",
      "train loss:0.0024065183057290305\n",
      "train loss:0.0014723573461951917\n",
      "train loss:0.002880031034228117\n",
      "train loss:0.00190574510281597\n",
      "train loss:0.001291056059651039\n",
      "train loss:0.001111018781485013\n",
      "train loss:0.0021740582178750552\n",
      "train loss:0.0005396394424457791\n",
      "train loss:0.00036295860374809923\n",
      "train loss:0.0017509821283036373\n",
      "train loss:0.0036660631538503108\n",
      "train loss:0.0016416926406044553\n",
      "train loss:0.001359154434205191\n",
      "train loss:0.0035433560725346724\n",
      "train loss:0.005913149676396184\n",
      "train loss:0.0003267091321427346\n",
      "train loss:0.0008863221106732578\n",
      "train loss:0.0034774590647299263\n",
      "train loss:0.0016066434750513473\n",
      "train loss:0.0015277731017979055\n",
      "train loss:0.004452784717940176\n",
      "train loss:0.0018157464010414947\n",
      "train loss:0.001400115470429695\n",
      "train loss:0.0023499667985194196\n",
      "train loss:0.0013535748094178487\n",
      "train loss:0.004548278416449683\n",
      "train loss:0.0008700025436400734\n",
      "train loss:0.0018706840064304153\n",
      "train loss:0.0011910564754936065\n",
      "train loss:0.004072612802392681\n",
      "train loss:0.0015262488195229862\n",
      "train loss:0.0019908596983689796\n",
      "train loss:0.003079048003565014\n",
      "train loss:0.0008615808839825375\n",
      "train loss:0.0024573941217477346\n",
      "train loss:0.0020493510758355336\n",
      "train loss:0.001407997322015723\n",
      "train loss:0.0017957518866726252\n",
      "train loss:0.0011712680872090926\n",
      "train loss:0.0014294190953820674\n",
      "train loss:0.0005310794681410026\n",
      "train loss:0.00166412655065278\n",
      "train loss:0.0012868179936853545\n",
      "train loss:0.0005012602493706499\n",
      "train loss:0.0004732977979601759\n",
      "train loss:0.0012971600026447355\n",
      "train loss:0.0017058876920299513\n",
      "train loss:0.0008458000968939242\n",
      "train loss:0.001791063488174793\n",
      "train loss:0.0006977198099864306\n",
      "train loss:0.0018308142340359116\n",
      "train loss:0.0015854591586267012\n",
      "train loss:0.000748037265271628\n",
      "train loss:0.0016665312609130518\n",
      "train loss:0.0012459392466087303\n",
      "train loss:0.0015067168762044022\n",
      "train loss:0.0009743238285799607\n",
      "train loss:0.0008518941227138029\n",
      "train loss:0.0013948802404479502\n",
      "train loss:0.001168221363256324\n",
      "train loss:0.001494819090399013\n",
      "train loss:0.0010250646102159726\n",
      "train loss:0.0007949651307468333\n",
      "train loss:0.00033421895497445173\n",
      "train loss:0.0008993850912254981\n",
      "train loss:0.001476676481777839\n",
      "=== epoch:5, train acc:1.0, test acc:0.996 ===\n",
      "train loss:0.0011621968340568265\n",
      "train loss:0.0016671376575529634\n",
      "train loss:0.0014530388777170662\n",
      "train loss:0.0003116078251021654\n",
      "train loss:0.0010176522046854636\n",
      "train loss:0.0005079930949128339\n",
      "train loss:0.002855714832327329\n",
      "train loss:0.0009573851764022018\n",
      "train loss:0.002370870836758754\n",
      "train loss:0.0008482731705842814\n",
      "train loss:0.0024172832022172643\n",
      "train loss:0.0007632761508174175\n",
      "train loss:0.001130053457879945\n",
      "train loss:0.000992514239538006\n",
      "train loss:0.0011627792847003257\n",
      "train loss:0.0005218362652821047\n",
      "train loss:0.0006721450369941909\n",
      "train loss:0.0014389978632678398\n",
      "train loss:0.0013798419762957475\n",
      "train loss:0.0004326059741124321\n",
      "train loss:0.0004332214918912615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0002725684275484593\n",
      "train loss:0.0025336282268978703\n",
      "train loss:0.0009971197815129306\n",
      "train loss:0.0006931797957169376\n",
      "train loss:0.0008097726573169433\n",
      "train loss:0.0009302761351078756\n",
      "train loss:0.0006724425494923809\n",
      "train loss:0.0007627179038241788\n",
      "train loss:0.0011772897596870793\n",
      "train loss:0.0036401070542921078\n",
      "train loss:0.00012071235275611141\n",
      "train loss:0.0005227305872280885\n",
      "train loss:0.0009110837530266092\n",
      "train loss:0.0020658775556137497\n",
      "train loss:0.0013637887677465225\n",
      "train loss:0.0006535083059190674\n",
      "train loss:0.0004333061614776634\n",
      "train loss:0.0005757786000365776\n",
      "train loss:0.0003068501000865894\n",
      "train loss:0.0011885118311420703\n",
      "train loss:0.0005770451293192566\n",
      "train loss:0.0005781974364878746\n",
      "train loss:0.0004432606573830273\n",
      "train loss:0.0011393364695879672\n",
      "train loss:0.0011578983792052222\n",
      "train loss:0.00206837577685978\n",
      "train loss:0.0002414060492828251\n",
      "train loss:0.00037015328526529383\n",
      "train loss:0.0006210031545294671\n",
      "train loss:0.0008019046160845561\n",
      "train loss:0.003285434686776104\n",
      "train loss:0.0008567753864136406\n",
      "train loss:0.0007575716818141886\n",
      "train loss:0.0009595716305466435\n",
      "train loss:0.0010818752818809868\n",
      "train loss:0.0005277485737749957\n",
      "train loss:0.0008053035286108587\n",
      "train loss:0.0015231426947976683\n",
      "train loss:9.19265964067967e-05\n",
      "train loss:0.0004304670009219231\n",
      "train loss:0.0013473791725413195\n",
      "train loss:0.0027701596432027197\n",
      "train loss:0.0012706631894223819\n",
      "train loss:0.00030598564735017243\n",
      "train loss:0.0004394362197748689\n",
      "train loss:0.0026340833573688686\n",
      "train loss:0.00034022971822519074\n",
      "train loss:0.0016703126710121382\n",
      "train loss:0.0023560380799875426\n",
      "train loss:0.0006676658887794676\n",
      "train loss:0.0012691660205958978\n",
      "train loss:0.0018518898586357357\n",
      "train loss:0.0028870883230210586\n",
      "train loss:0.00038786855941730055\n",
      "train loss:0.0006098195916265547\n",
      "train loss:0.0011009885761619424\n",
      "train loss:0.0017069635161303617\n",
      "train loss:0.00035741182707088576\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.997\n"
     ]
    }
   ],
   "source": [
    "x_train=x_train_2\n",
    "x_test = x_test_2\n",
    "t_train=t_train_2\n",
    "t_test = t_test_2\n",
    "\n",
    "trainer_SGD = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='SGD', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_MMT = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Momentum', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_SGD.train()\n",
    "trainer_MMT.train()\n",
    "\n",
    "MMT_acc.append(trainer_MMT.test_acc_list[-1])\n",
    "SGD_acc.append(trainer_SGD.test_acc_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0015405985439664771\n",
      "=== epoch:1, train acc:0.999, test acc:0.999 ===\n",
      "train loss:0.0010468446532341978\n",
      "train loss:0.002436974911960187\n",
      "train loss:0.0007205368296296801\n",
      "train loss:0.0019705774283969627\n",
      "train loss:0.0031206649642712627\n",
      "train loss:0.001675199734711831\n",
      "train loss:0.0014368409387167035\n",
      "train loss:0.0010967576195788744\n",
      "train loss:0.0014030830768365177\n",
      "train loss:0.0014269863380260604\n",
      "train loss:0.0013040457621750635\n",
      "train loss:0.0006184026084092605\n",
      "train loss:0.004330019474865012\n",
      "train loss:0.0009162818571401817\n",
      "train loss:0.0009505709158242134\n",
      "train loss:0.0009712536580395181\n",
      "train loss:0.0015597675886456078\n",
      "train loss:0.0007669394154085237\n",
      "train loss:0.00037313460172311676\n",
      "train loss:0.004290285465580465\n",
      "train loss:0.007468986949629797\n",
      "train loss:0.0020800687117184364\n",
      "train loss:0.00011941593061575833\n",
      "train loss:0.00134952236670643\n",
      "train loss:0.0028679833734993484\n",
      "train loss:0.0039547045046988485\n",
      "train loss:0.001187033896209667\n",
      "train loss:0.0004361987851733864\n",
      "train loss:0.0007987525430037344\n",
      "train loss:0.0009745342371163414\n",
      "train loss:0.0013253209748519035\n",
      "train loss:0.002071210227616807\n",
      "train loss:0.00040059780540134516\n",
      "train loss:0.0005378229475067387\n",
      "train loss:0.0024434786084083842\n",
      "train loss:0.0022642823361809098\n",
      "train loss:0.0003811574701692436\n",
      "train loss:0.0009062197618610082\n",
      "train loss:0.008387682851597534\n",
      "train loss:0.0014606417903498012\n",
      "train loss:0.0017289248258770245\n",
      "train loss:0.0008467655302345263\n",
      "train loss:0.0004106360944076279\n",
      "train loss:0.002915638654858473\n",
      "train loss:0.0010276077513260604\n",
      "train loss:0.001102857659704868\n",
      "train loss:0.00046147392820395923\n",
      "train loss:0.002382535494759145\n",
      "train loss:0.0002469961282503285\n",
      "train loss:0.00042534835002744115\n",
      "train loss:0.001130557320184089\n",
      "train loss:0.0002062315652885089\n",
      "train loss:0.0011579710182946944\n",
      "train loss:0.001573756226675882\n",
      "train loss:5.293201915667404e-05\n",
      "train loss:0.001732844701085647\n",
      "train loss:0.0006042163815895013\n",
      "train loss:0.0028299060193109783\n",
      "train loss:0.001101484943563572\n",
      "train loss:0.0009743934494867153\n",
      "train loss:0.0020809376858424793\n",
      "train loss:0.0026086824910741065\n",
      "train loss:0.00040909100073043236\n",
      "train loss:0.0024824782598335225\n",
      "train loss:0.000940504441755342\n",
      "train loss:0.006097025231228976\n",
      "train loss:0.00048404103542801994\n",
      "train loss:0.000500886015195367\n",
      "train loss:0.0006385201050199809\n",
      "train loss:0.000892746516538243\n",
      "train loss:0.0006235487176591161\n",
      "train loss:0.004396938205986657\n",
      "train loss:0.0017364807380049708\n",
      "train loss:0.0004948017396501271\n",
      "train loss:0.0006673973623809442\n",
      "train loss:0.0007001152807860642\n",
      "train loss:0.00046858386492491105\n",
      "train loss:0.0028352796416047515\n",
      "train loss:0.0006758612662599496\n",
      "train loss:0.001980502315645897\n",
      "=== epoch:2, train acc:0.999, test acc:0.999 ===\n",
      "train loss:0.0002340846969450682\n",
      "train loss:0.0014356613755681536\n",
      "train loss:0.0017027493520521842\n",
      "train loss:0.00102711779916502\n",
      "train loss:0.0018848033675701772\n",
      "train loss:0.002298093837918063\n",
      "train loss:0.002108524617244867\n",
      "train loss:0.019723943004958628\n",
      "train loss:0.001457807488626319\n",
      "train loss:0.0015726166973955202\n",
      "train loss:0.010126322280249005\n",
      "train loss:0.0010665952557783562\n",
      "train loss:0.0011216357448985278\n",
      "train loss:0.0004779287170264365\n",
      "train loss:0.0002527685511973327\n",
      "train loss:0.0011705560397802831\n",
      "train loss:0.000565498815269975\n",
      "train loss:0.0005989332020251752\n",
      "train loss:0.0015204732783056743\n",
      "train loss:0.00041442307486266845\n",
      "train loss:0.0015972303439963115\n",
      "train loss:0.0001806577491948651\n",
      "train loss:0.0011429741253014742\n",
      "train loss:0.000362437733963351\n",
      "train loss:0.0001889873381685759\n",
      "train loss:0.001560038370560929\n",
      "train loss:0.0002999984373068425\n",
      "train loss:0.0014084029758052287\n",
      "train loss:0.0011984573991614517\n",
      "train loss:0.0010290653944809677\n",
      "train loss:0.0006013511645478461\n",
      "train loss:0.0004793884295175418\n",
      "train loss:0.0007725141358308102\n",
      "train loss:0.003979921716990273\n",
      "train loss:0.00038080635230228205\n",
      "train loss:0.0010989894057080365\n",
      "train loss:0.0007381190750030215\n",
      "train loss:0.0004963516823177753\n",
      "train loss:0.0017542793466252754\n",
      "train loss:0.0017304378624271631\n",
      "train loss:0.0014856099580060889\n",
      "train loss:0.00018869188677087093\n",
      "train loss:0.002698053750561301\n",
      "train loss:0.0007635287078905079\n",
      "train loss:0.0014324582645913976\n",
      "train loss:0.0004377483673936132\n",
      "train loss:0.0002839501586856388\n",
      "train loss:0.0012202742101294981\n",
      "train loss:0.0009003077276176923\n",
      "train loss:0.0005900337557942951\n",
      "train loss:0.0008575498826259718\n",
      "train loss:0.0008991957248669503\n",
      "train loss:0.0013093323885149926\n",
      "train loss:0.00044548656892271274\n",
      "train loss:0.0011283171365622094\n",
      "train loss:0.00850194152581691\n",
      "train loss:0.0019698130501373447\n",
      "train loss:0.0011407483899251048\n",
      "train loss:0.0008004021052818765\n",
      "train loss:0.002146049964040186\n",
      "train loss:0.0014122089115511018\n",
      "train loss:0.001899364158829992\n",
      "train loss:0.00037408908690505496\n",
      "train loss:0.0008793976934173723\n",
      "train loss:0.005317026672742067\n",
      "train loss:0.0008993277977862353\n",
      "train loss:0.001519588013076868\n",
      "train loss:0.00042607907612762926\n",
      "train loss:0.0015694354453237335\n",
      "train loss:0.00029404274319440484\n",
      "train loss:0.002143189997332334\n",
      "train loss:0.0009028491215219187\n",
      "train loss:0.00021040394631749768\n",
      "train loss:0.0003564625124906846\n",
      "train loss:0.0008141505735706034\n",
      "train loss:0.0016708119747859652\n",
      "train loss:0.0005347676740855073\n",
      "train loss:0.0010728809644033963\n",
      "train loss:0.0013861631952196659\n",
      "train loss:0.00568090977673822\n",
      "=== epoch:3, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00031375213316611023\n",
      "train loss:0.0010738138538504555\n",
      "train loss:0.00017668533118469606\n",
      "train loss:0.00699153195972698\n",
      "train loss:0.0013802875786774757\n",
      "train loss:0.0008751824990045983\n",
      "train loss:0.00042380496542438704\n",
      "train loss:0.0015431986142302766\n",
      "train loss:0.0009580083100596079\n",
      "train loss:0.005787356728576304\n",
      "train loss:0.0005192657637175044\n",
      "train loss:0.000839780587284775\n",
      "train loss:0.0019174574649033337\n",
      "train loss:0.0006411906054351502\n",
      "train loss:0.0009512250310832252\n",
      "train loss:0.01043804511286682\n",
      "train loss:0.0007457770427188562\n",
      "train loss:0.0021626024453182736\n",
      "train loss:0.004533177711992582\n",
      "train loss:0.0015571657749347593\n",
      "train loss:0.00023215380444510523\n",
      "train loss:0.0008625484832159505\n",
      "train loss:0.0010720472858354241\n",
      "train loss:0.009141125348526518\n",
      "train loss:0.0011080929350583033\n",
      "train loss:0.0023791035795532145\n",
      "train loss:0.0004810215178651364\n",
      "train loss:0.00042658406228645896\n",
      "train loss:0.000802926506888767\n",
      "train loss:0.0026846003646047445\n",
      "train loss:0.0005411842886491362\n",
      "train loss:0.0010390892665305047\n",
      "train loss:0.001227660978870376\n",
      "train loss:0.001776491716746413\n",
      "train loss:0.0044639700821468625\n",
      "train loss:0.0006196096555414703\n",
      "train loss:0.001027446080087797\n",
      "train loss:0.0008568953769184627\n",
      "train loss:0.0003204575698767683\n",
      "train loss:0.002476464989541446\n",
      "train loss:0.0010915846357003915\n",
      "train loss:0.0014317500826443938\n",
      "train loss:0.0005570936411677988\n",
      "train loss:0.002433776680422611\n",
      "train loss:0.0011319168692741307\n",
      "train loss:0.001149770027122559\n",
      "train loss:0.00216392448470305\n",
      "train loss:0.0015996580856123634\n",
      "train loss:0.00012197807747849244\n",
      "train loss:0.0012490706044434444\n",
      "train loss:0.0014993126569467522\n",
      "train loss:0.0010874393881510954\n",
      "train loss:0.001131479044658186\n",
      "train loss:0.0007738573088559229\n",
      "train loss:0.0003474082580901675\n",
      "train loss:0.002287357121576871\n",
      "train loss:0.0030656381183163557\n",
      "train loss:0.0009265019359534661\n",
      "train loss:0.0005784661478796764\n",
      "train loss:0.0010090179779524465\n",
      "train loss:0.0020412723955872815\n",
      "train loss:0.0006865515641003695\n",
      "train loss:0.001072461208828254\n",
      "train loss:0.0020478783616139826\n",
      "train loss:0.0008463279556030654\n",
      "train loss:0.0002858645176658744\n",
      "train loss:0.00039894646755072737\n",
      "train loss:0.0010794910373279768\n",
      "train loss:0.0018561679097443187\n",
      "train loss:0.001402856997581182\n",
      "train loss:0.0010993022589414533\n",
      "train loss:0.0004907783978263853\n",
      "train loss:0.0009714814117615669\n",
      "train loss:0.0008236169305011262\n",
      "train loss:0.0004997764824455886\n",
      "train loss:0.0008997385371839656\n",
      "train loss:0.0003815601520094874\n",
      "train loss:0.001461200909020549\n",
      "train loss:0.001129924678608898\n",
      "train loss:0.00022295350841381053\n",
      "=== epoch:4, train acc:0.999, test acc:1.0 ===\n",
      "train loss:0.002188463835568584\n",
      "train loss:0.0023339849427386527\n",
      "train loss:0.00013639201400179768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0011533150955878664\n",
      "train loss:0.000211407675938262\n",
      "train loss:0.0005583309062854309\n",
      "train loss:0.0010577909718436965\n",
      "train loss:0.00047169399865582673\n",
      "train loss:0.0010590935040603556\n",
      "train loss:0.00045428223471949273\n",
      "train loss:0.0005600457717967701\n",
      "train loss:0.002481558196572155\n",
      "train loss:0.0004321290719571387\n",
      "train loss:0.0011709532805516823\n",
      "train loss:0.0014246175015186365\n",
      "train loss:0.000904869559757728\n",
      "train loss:0.0011659661904765174\n",
      "train loss:0.0005437987844238694\n",
      "train loss:0.00026893504083706055\n",
      "train loss:0.0008250638672344266\n",
      "train loss:0.0009507235194868683\n",
      "train loss:0.0005070563962126089\n",
      "train loss:0.0021432833088360025\n",
      "train loss:0.0008834396578634072\n",
      "train loss:0.0008415731638853574\n",
      "train loss:0.000966262270722307\n",
      "train loss:0.0030450199820236585\n",
      "train loss:0.0005770423024971897\n",
      "train loss:0.000512115461210411\n",
      "train loss:0.0019470199179996315\n",
      "train loss:0.0008323935891056103\n",
      "train loss:0.00024669740677017927\n",
      "train loss:0.0028132148935524383\n",
      "train loss:0.0015968280557805377\n",
      "train loss:0.0012031977752554545\n",
      "train loss:0.0016422067437054835\n",
      "train loss:0.0001343338392047796\n",
      "train loss:0.0004352386718662437\n",
      "train loss:0.0005031188035494995\n",
      "train loss:0.000467668156748762\n",
      "train loss:0.001072407303297883\n",
      "train loss:0.0006927641611041774\n",
      "train loss:0.0011217260434140729\n",
      "train loss:0.0015841773053624387\n",
      "train loss:0.0016297487761329338\n",
      "train loss:0.001443902506027831\n",
      "train loss:0.00029556161471650693\n",
      "train loss:0.001161803167102644\n",
      "train loss:0.0006428311331857546\n",
      "train loss:0.0017302380398818385\n",
      "train loss:0.0010797451448084864\n",
      "train loss:0.0008265601503238664\n",
      "train loss:0.0003621040352992836\n",
      "train loss:0.0005228830870305497\n",
      "train loss:0.0008702554946198241\n",
      "train loss:0.0004884640665487024\n",
      "train loss:0.0014240579164089514\n",
      "train loss:0.0004459415067580063\n",
      "train loss:0.0007424037618115885\n",
      "train loss:0.0007221100340158114\n",
      "train loss:0.0006958052768194063\n",
      "train loss:0.0004600292118279463\n",
      "train loss:0.0011420256875945632\n",
      "train loss:0.00365019223720621\n",
      "train loss:0.0002507718221423989\n",
      "train loss:0.0010577266634078709\n",
      "train loss:0.0017004268054198386\n",
      "train loss:0.0017709420030568826\n",
      "train loss:0.0011644178419284903\n",
      "train loss:0.0016253006463499528\n",
      "train loss:0.0004788520408381345\n",
      "train loss:0.0003103268781375406\n",
      "train loss:0.0008962688794727152\n",
      "train loss:0.0008367014570413376\n",
      "train loss:0.00022018656134824304\n",
      "train loss:0.0005254058717991512\n",
      "train loss:0.0006567535827117281\n",
      "train loss:0.00043524766115301644\n",
      "train loss:0.0008027537344941736\n",
      "train loss:0.00040634168827691946\n",
      "=== epoch:5, train acc:0.999, test acc:0.999 ===\n",
      "train loss:0.0006157250237406825\n",
      "train loss:0.002414912345427891\n",
      "train loss:0.0007049390519998887\n",
      "train loss:0.0004707383369829375\n",
      "train loss:0.005818275982471299\n",
      "train loss:0.0032510397709562557\n",
      "train loss:0.0008960905373224324\n",
      "train loss:0.0013647519471740282\n",
      "train loss:0.0007897281216738769\n",
      "train loss:0.0004967138916561792\n",
      "train loss:0.0013746799409059913\n",
      "train loss:0.0005946269782834627\n",
      "train loss:0.0013462666098803542\n",
      "train loss:0.0002217843840873804\n",
      "train loss:0.0018668954528565727\n",
      "train loss:0.00010864293004018767\n",
      "train loss:0.0007214766780084948\n",
      "train loss:0.0002841346539594258\n",
      "train loss:0.006079625469175366\n",
      "train loss:0.0006624479651768009\n",
      "train loss:0.0010242626733986374\n",
      "train loss:0.00041392535145006837\n",
      "train loss:0.0012593845919653805\n",
      "train loss:0.000618924038735855\n",
      "train loss:0.0007830934257237726\n",
      "train loss:0.0015514919918743339\n",
      "train loss:0.00018210145569742383\n",
      "train loss:0.0007889192210351623\n",
      "train loss:0.0003387503792629872\n",
      "train loss:0.0011838618688640374\n",
      "train loss:0.000714572050354848\n",
      "train loss:0.003657086826881442\n",
      "train loss:0.002008521400181763\n",
      "train loss:0.008666386112859817\n",
      "train loss:0.0008886393527315785\n",
      "train loss:0.00029906921324459544\n",
      "train loss:0.0006485366880064135\n",
      "train loss:8.627041710904759e-05\n",
      "train loss:0.0017715595950594286\n",
      "train loss:0.0006317084537504204\n",
      "train loss:0.0014181655359600581\n",
      "train loss:0.001735324329173898\n",
      "train loss:0.00018895463573969623\n",
      "train loss:9.773710994547172e-05\n",
      "train loss:0.0008584724151603594\n",
      "train loss:0.00028036764209895805\n",
      "train loss:0.00018346689023300924\n",
      "train loss:0.0010491884561066901\n",
      "train loss:0.0008344559624687871\n",
      "train loss:0.0007171245851401716\n",
      "train loss:0.0010387671186495532\n",
      "train loss:0.0010861058271489045\n",
      "train loss:0.0013421345669396133\n",
      "train loss:0.0005034031032752818\n",
      "train loss:0.0008833800376165571\n",
      "train loss:0.0016303938833399203\n",
      "train loss:0.0015857041701027976\n",
      "train loss:0.0003924925690794145\n",
      "train loss:0.0005697119685230499\n",
      "train loss:0.0006496196415316165\n",
      "train loss:0.002697170318794113\n",
      "train loss:0.0008084066204618928\n",
      "train loss:0.001676246287195097\n",
      "train loss:0.000493245324303771\n",
      "train loss:0.0018635779067745673\n",
      "train loss:0.0014191294210027002\n",
      "train loss:0.0006552576661966781\n",
      "train loss:0.0010928052798314174\n",
      "train loss:0.0015091641749997147\n",
      "train loss:0.000389474332381027\n",
      "train loss:0.0008568054878358056\n",
      "train loss:0.0003048234024458855\n",
      "train loss:0.0024759974675948667\n",
      "train loss:0.0009932141185447487\n",
      "train loss:0.0014425998180940492\n",
      "train loss:0.0016125436363516024\n",
      "train loss:0.0013405691417728185\n",
      "train loss:0.0011473686199789535\n",
      "train loss:0.0010401728193647688\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9995\n",
      "train loss:0.0013721242435059259\n",
      "=== epoch:1, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0003349557799404092\n",
      "train loss:0.0008752329012829032\n",
      "train loss:0.00032586918681189674\n",
      "train loss:0.0005555018704195748\n",
      "train loss:0.00041185712395516087\n",
      "train loss:0.0007341936553106687\n",
      "train loss:0.0011993489808705708\n",
      "train loss:0.0005637386885725989\n",
      "train loss:0.005865775718552567\n",
      "train loss:0.0011666519571207844\n",
      "train loss:0.0027410355455485067\n",
      "train loss:0.000817746528953198\n",
      "train loss:0.0012214672974713537\n",
      "train loss:0.00736444944343804\n",
      "train loss:0.0013429046333858414\n",
      "train loss:0.0010385356146092551\n",
      "train loss:0.002420689500921298\n",
      "train loss:0.002885370921348369\n",
      "train loss:0.002420226647025459\n",
      "train loss:0.0005207340660110314\n",
      "train loss:0.00045711492632263416\n",
      "train loss:0.0001033570908479637\n",
      "train loss:0.0005408445839109352\n",
      "train loss:0.005275627247332493\n",
      "train loss:0.003223350070006193\n",
      "train loss:0.0005064827001483426\n",
      "train loss:0.0004919571764429411\n",
      "train loss:0.0007469499814024609\n",
      "train loss:0.00019115771632774195\n",
      "train loss:0.0004917309399615038\n",
      "train loss:0.00028590701116677113\n",
      "train loss:0.0005719372309147206\n",
      "train loss:0.0005592986026966384\n",
      "train loss:0.0038047463505315898\n",
      "train loss:0.0022242526483900395\n",
      "train loss:0.003802323182757266\n",
      "train loss:0.004048946892187261\n",
      "train loss:0.0004496194725313895\n",
      "train loss:0.003148971016331134\n",
      "train loss:0.0007785942237506688\n",
      "train loss:0.00032402135594112864\n",
      "train loss:0.0001829768043537039\n",
      "train loss:0.0013266557818409047\n",
      "train loss:0.003685758121684826\n",
      "train loss:0.0031803111500593234\n",
      "train loss:0.0009876695165051271\n",
      "train loss:0.0017806314476277515\n",
      "train loss:0.004251774518712657\n",
      "train loss:0.0021296655115287722\n",
      "train loss:0.0003337815311092565\n",
      "train loss:0.0005927586425884565\n",
      "train loss:0.0010012033549153057\n",
      "train loss:0.000786977181910869\n",
      "train loss:0.005083681321528009\n",
      "train loss:0.0016313424340278922\n",
      "train loss:0.001220303564450959\n",
      "train loss:0.0003578573914644053\n",
      "train loss:0.0037414206600776064\n",
      "train loss:0.0015424046146827147\n",
      "train loss:0.0005193185834453252\n",
      "train loss:0.005083921556285793\n",
      "train loss:0.0025645059458975637\n",
      "train loss:0.0009082635055491009\n",
      "train loss:0.0025256755261080744\n",
      "train loss:0.000655630329980655\n",
      "train loss:0.00025723143653754756\n",
      "train loss:0.0005257507824255885\n",
      "train loss:0.0005161204944566864\n",
      "train loss:0.0007942615607604897\n",
      "train loss:0.00046518221087716536\n",
      "train loss:0.0016858580672854157\n",
      "train loss:0.00040972978920105353\n",
      "train loss:0.001282540903975174\n",
      "train loss:0.005339907717081423\n",
      "train loss:0.00046151392990184935\n",
      "train loss:0.00031221569030723484\n",
      "train loss:0.0011008054423445122\n",
      "train loss:0.003152374021793614\n",
      "train loss:0.002087366215994937\n",
      "train loss:0.0003522301329578624\n",
      "=== epoch:2, train acc:1.0, test acc:0.998 ===\n",
      "train loss:0.0007421792405377604\n",
      "train loss:0.0036442627713913485\n",
      "train loss:0.0009134289537496861\n",
      "train loss:0.0003058892430099202\n",
      "train loss:0.0021561958340611444\n",
      "train loss:0.00033151807455154337\n",
      "train loss:0.00022093485175634227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0027917285670841867\n",
      "train loss:0.00012369494429011134\n",
      "train loss:0.0010041947040115725\n",
      "train loss:0.0005353073612381426\n",
      "train loss:0.000640977096882556\n",
      "train loss:0.004092491480871007\n",
      "train loss:0.0007553307902279783\n",
      "train loss:0.0007190538435918317\n",
      "train loss:0.0013024873132905018\n",
      "train loss:0.0022763730459383403\n",
      "train loss:0.0005871172754198146\n",
      "train loss:0.0012809844833698355\n",
      "train loss:0.0021114710874041744\n",
      "train loss:0.002916280904548441\n",
      "train loss:0.00419587719161147\n",
      "train loss:0.0007900129854900037\n",
      "train loss:0.0010406096427830278\n",
      "train loss:0.0007968520933246552\n",
      "train loss:0.00213966928848121\n",
      "train loss:0.0003368563916691723\n",
      "train loss:0.0007982564047784534\n",
      "train loss:0.0008390420303932046\n",
      "train loss:0.0027511392573131305\n",
      "train loss:0.003922489553322302\n",
      "train loss:0.0018181253402779405\n",
      "train loss:0.0015878843454141074\n",
      "train loss:0.0031015633702868154\n",
      "train loss:0.0023614546873132764\n",
      "train loss:0.0018643196166286964\n",
      "train loss:0.003695941903996189\n",
      "train loss:0.0014827939620364475\n",
      "train loss:0.0036366508363852856\n",
      "train loss:0.00197518716030132\n",
      "train loss:0.00021002278071608968\n",
      "train loss:0.0007161947173693374\n",
      "train loss:0.002853732015401065\n",
      "train loss:0.0018900583137515512\n",
      "train loss:0.0015326889185069742\n",
      "train loss:0.0006082997519751639\n",
      "train loss:0.0032164763241156213\n",
      "train loss:0.0009351407603304239\n",
      "train loss:0.00647651522140239\n",
      "train loss:0.0004903246336428139\n",
      "train loss:0.0012789147586845485\n",
      "train loss:0.00045023336873223775\n",
      "train loss:0.002084154455456724\n",
      "train loss:0.0010693814191285037\n",
      "train loss:0.0010020228580639565\n",
      "train loss:0.0005221028090713576\n",
      "train loss:0.0008915652086500401\n",
      "train loss:0.00031464676445257874\n",
      "train loss:0.0017118672773366764\n",
      "train loss:0.0004524076572424422\n",
      "train loss:0.00037274246186836334\n",
      "train loss:0.0002929273470824008\n",
      "train loss:0.001035181288781216\n",
      "train loss:0.0009214029906770912\n",
      "train loss:0.0014561400480438036\n",
      "train loss:0.002167187208675246\n",
      "train loss:0.005335763964106197\n",
      "train loss:0.000330949258855308\n",
      "train loss:0.0007931275936834908\n",
      "train loss:0.0002765167696281802\n",
      "train loss:0.0012541736823939644\n",
      "train loss:0.000839709504635736\n",
      "train loss:0.0010996652963963349\n",
      "train loss:0.001940269969412739\n",
      "train loss:0.0009035403409260907\n",
      "train loss:0.0011302559657530124\n",
      "train loss:0.0011065536230727563\n",
      "train loss:0.0019822492205591384\n",
      "train loss:0.000512838745002873\n",
      "train loss:0.0006296730270404162\n",
      "=== epoch:3, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.0005167741342518687\n",
      "train loss:0.0006972621127935949\n",
      "train loss:0.0012191606907554853\n",
      "train loss:0.0004995902507976983\n",
      "train loss:0.002345883408512364\n",
      "train loss:0.0022053539053619874\n",
      "train loss:0.0005065500363076554\n",
      "train loss:0.0002823895769996801\n",
      "train loss:0.00016797430951695865\n",
      "train loss:0.000600514368641877\n",
      "train loss:0.000661417640144958\n",
      "train loss:0.0025892648666773306\n",
      "train loss:0.006850828555935944\n",
      "train loss:0.0004063205643860537\n",
      "train loss:0.002622625503158401\n",
      "train loss:0.00021520573677309877\n",
      "train loss:0.0029132602514945444\n",
      "train loss:0.0008083646842789681\n",
      "train loss:0.0005448369642080965\n",
      "train loss:0.00022971474618588912\n",
      "train loss:0.0005261392002111374\n",
      "train loss:0.0004052972129698416\n",
      "train loss:0.0008439870305487629\n",
      "train loss:0.0009897461540710252\n",
      "train loss:0.00010695021073396257\n",
      "train loss:0.0014024387418608405\n",
      "train loss:0.0007554836061695041\n",
      "train loss:0.00019499148428891923\n",
      "train loss:0.0004994285115660857\n",
      "train loss:0.00048014126357373885\n",
      "train loss:0.000514702248042179\n",
      "train loss:0.0036289861490325127\n",
      "train loss:0.004138418609217075\n",
      "train loss:0.0016038431009866743\n",
      "train loss:0.0007207024796603219\n",
      "train loss:0.00033762832404045574\n",
      "train loss:0.0010198132334528509\n",
      "train loss:0.001448635959121678\n",
      "train loss:0.0006768262270604859\n",
      "train loss:0.0033817509686484854\n",
      "train loss:0.00031254169073756605\n",
      "train loss:0.0007804493214794721\n",
      "train loss:0.0005987539435582318\n",
      "train loss:0.0006662159299916748\n",
      "train loss:0.0006233256030497205\n",
      "train loss:0.006166137681842781\n",
      "train loss:0.0005976449428055467\n",
      "train loss:0.0002669724799591525\n",
      "train loss:0.00041808060931488184\n",
      "train loss:0.002591215037186388\n",
      "train loss:0.0006867665201881678\n",
      "train loss:0.0004973747661474386\n",
      "train loss:0.0006482823782897134\n",
      "train loss:0.0007738047203502917\n",
      "train loss:0.002142942031444138\n",
      "train loss:0.0036785319766587006\n",
      "train loss:0.00026678313357029993\n",
      "train loss:0.000561637846952372\n",
      "train loss:0.0007724611226774913\n",
      "train loss:0.0006021513414316802\n",
      "train loss:0.00018677219114549522\n",
      "train loss:0.00040369837840754415\n",
      "train loss:0.0004954740751458933\n",
      "train loss:0.0004640454037275737\n",
      "train loss:0.001160584839076853\n",
      "train loss:0.0010770592941081343\n",
      "train loss:0.00014242467689197028\n",
      "train loss:0.0010267994305560035\n",
      "train loss:0.0008284277059579268\n",
      "train loss:0.0013605650781523524\n",
      "train loss:0.0004361966153454039\n",
      "train loss:0.00015851308429861878\n",
      "train loss:0.001051611303808738\n",
      "train loss:0.0007004185288722666\n",
      "train loss:0.0009066723993449116\n",
      "train loss:0.0011457481105473506\n",
      "train loss:0.0005933510707927132\n",
      "train loss:0.001888834528434771\n",
      "train loss:0.0005678889587697518\n",
      "train loss:0.0002636498014251098\n",
      "=== epoch:4, train acc:1.0, test acc:0.998 ===\n",
      "train loss:0.00047963108927229507\n",
      "train loss:0.000528152458517329\n",
      "train loss:0.0003532946358118826\n",
      "train loss:0.0003509793792923075\n",
      "train loss:0.0007947073818124783\n",
      "train loss:0.0004582543994164974\n",
      "train loss:0.000637015513390372\n",
      "train loss:0.0022425077529687775\n",
      "train loss:0.0002456742817156153\n",
      "train loss:0.0008748769727885758\n",
      "train loss:0.0003892133196025623\n",
      "train loss:0.000413961271206291\n",
      "train loss:0.0001817741455889701\n",
      "train loss:0.00038899385668558816\n",
      "train loss:0.0005178809328310306\n",
      "train loss:0.0006295496053447858\n",
      "train loss:0.0007761146710565303\n",
      "train loss:0.0005599611513236529\n",
      "train loss:0.000931744249568153\n",
      "train loss:0.0002966260668861746\n",
      "train loss:0.0003524562446444998\n",
      "train loss:0.000389473637988986\n",
      "train loss:0.0003888072230263055\n",
      "train loss:0.0007514541803940625\n",
      "train loss:0.0004687934661921133\n",
      "train loss:0.0003676443507453507\n",
      "train loss:0.0006678051233130522\n",
      "train loss:0.0006761229474396531\n",
      "train loss:0.00037123169890136156\n",
      "train loss:0.003099791998702335\n",
      "train loss:0.0005766228702560358\n",
      "train loss:0.000477287823537131\n",
      "train loss:0.0007485970569597864\n",
      "train loss:0.0008592571087555257\n",
      "train loss:0.000694538033672995\n",
      "train loss:0.00017344093860838448\n",
      "train loss:0.0046194518636650615\n",
      "train loss:0.0003955216143775108\n",
      "train loss:0.0007026238287430949\n",
      "train loss:0.0001676712918171237\n",
      "train loss:0.0008241946440038767\n",
      "train loss:0.00045189178774434144\n",
      "train loss:0.000410100699815355\n",
      "train loss:0.0011185463717888144\n",
      "train loss:0.00039834856983298454\n",
      "train loss:0.0004370143913741352\n",
      "train loss:0.000488879073401283\n",
      "train loss:0.0003299213773633948\n",
      "train loss:0.0013527828965249394\n",
      "train loss:0.0003271831837589638\n",
      "train loss:0.0005696126286306792\n",
      "train loss:0.0014217289839054997\n",
      "train loss:0.0003935389702539272\n",
      "train loss:0.0006541987722172643\n",
      "train loss:0.0011700223643054822\n",
      "train loss:0.0005206096960297084\n",
      "train loss:0.0012087777213089976\n",
      "train loss:0.0017909974575250078\n",
      "train loss:0.0002851237380401129\n",
      "train loss:0.000986547682937498\n",
      "train loss:0.0006057632308837356\n",
      "train loss:0.0010406632031016853\n",
      "train loss:0.00026569470092390114\n",
      "train loss:0.0008813495446076463\n",
      "train loss:0.00031626483173888574\n",
      "train loss:0.0006622634183754507\n",
      "train loss:0.0009178626499458872\n",
      "train loss:0.00035011898116616086\n",
      "train loss:9.460436401606625e-05\n",
      "train loss:0.0006200814566757537\n",
      "train loss:0.0004717963285244277\n",
      "train loss:0.00041756575117634406\n",
      "train loss:0.0008506142687195001\n",
      "train loss:0.0005544023815511514\n",
      "train loss:0.00015868322325880574\n",
      "train loss:0.0002278990174744883\n",
      "train loss:0.0004291182349376297\n",
      "train loss:0.00043828810603182904\n",
      "train loss:0.0002259396638936352\n",
      "train loss:0.00019696706920357049\n",
      "=== epoch:5, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.000868663943600439\n",
      "train loss:0.0002363471882023599\n",
      "train loss:0.00035885058222916036\n",
      "train loss:0.0007873129103956788\n",
      "train loss:0.0005054032234584034\n",
      "train loss:0.0004232610318265586\n",
      "train loss:0.0004686593123304229\n",
      "train loss:0.0006293262537118418\n",
      "train loss:0.0010890552893772258\n",
      "train loss:0.00029472865981167276\n",
      "train loss:0.0005277932319258937\n",
      "train loss:0.00028139883635822246\n",
      "train loss:0.00038303502943370455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0004554982385192357\n",
      "train loss:0.0001789422106047255\n",
      "train loss:0.0010497652223212142\n",
      "train loss:0.00017087404928596166\n",
      "train loss:0.0001529332333954272\n",
      "train loss:0.0005144274706810478\n",
      "train loss:0.0013983123793031007\n",
      "train loss:0.0006297296828767\n",
      "train loss:0.0014080953143022986\n",
      "train loss:0.0003500909759587997\n",
      "train loss:0.00018767555461606636\n",
      "train loss:0.0004511272258811252\n",
      "train loss:0.000455323866020734\n",
      "train loss:0.0006278880052533373\n",
      "train loss:0.0003520364788394658\n",
      "train loss:0.0006525180713171251\n",
      "train loss:0.0008586713532299352\n",
      "train loss:5.90523770340935e-05\n",
      "train loss:0.0006338614551524909\n",
      "train loss:0.000654112193422269\n",
      "train loss:0.0005123954556116798\n",
      "train loss:0.0005281702918250384\n",
      "train loss:0.0003922536339921271\n",
      "train loss:0.0008110429858245818\n",
      "train loss:0.00041800037190976034\n",
      "train loss:0.0007236030456446621\n",
      "train loss:0.0007909270228091167\n",
      "train loss:0.0001106222536963359\n",
      "train loss:0.0005021561403330875\n",
      "train loss:0.0007529256358045222\n",
      "train loss:0.00014475199992401727\n",
      "train loss:0.00026250650211077\n",
      "train loss:0.00030870075553969826\n",
      "train loss:0.0006929282980283594\n",
      "train loss:0.00047959573324152466\n",
      "train loss:0.0006704021900648904\n",
      "train loss:0.0003851700045429063\n",
      "train loss:0.000567684552604115\n",
      "train loss:0.0004440588526833376\n",
      "train loss:0.0011530396149982654\n",
      "train loss:0.0009117704891507092\n",
      "train loss:0.0005140329973117296\n",
      "train loss:0.00035090064587361893\n",
      "train loss:0.00020209427446319162\n",
      "train loss:0.0004776537180522266\n",
      "train loss:0.0009391485916002515\n",
      "train loss:0.00037353718537657565\n",
      "train loss:0.00023839361850083322\n",
      "train loss:0.000257373530321991\n",
      "train loss:0.0001939461667719928\n",
      "train loss:0.000198934067563899\n",
      "train loss:0.0005342087998024345\n",
      "train loss:0.00038413624782605225\n",
      "train loss:0.0006590685662435272\n",
      "train loss:0.0019695046063928395\n",
      "train loss:0.0005811578761946476\n",
      "train loss:0.0006982107768216518\n",
      "train loss:0.0007721641730359273\n",
      "train loss:0.0004866969468511638\n",
      "train loss:0.00016225678792650925\n",
      "train loss:9.0260380860507e-05\n",
      "train loss:0.0007387186069830069\n",
      "train loss:0.0004731462719028938\n",
      "train loss:0.00020838358632002338\n",
      "train loss:0.00050739194397364\n",
      "train loss:0.0005354016110866274\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9985\n"
     ]
    }
   ],
   "source": [
    "x_train=x_train_3\n",
    "x_test = x_test_3\n",
    "t_train=t_train_3\n",
    "t_test = t_test_3\n",
    "\n",
    "trainer_SGD = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='SGD', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_MMT = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Momentum', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_SGD.train()\n",
    "trainer_MMT.train()\n",
    "\n",
    "MMT_acc.append(trainer_MMT.test_acc_list[-1])\n",
    "SGD_acc.append(trainer_SGD.test_acc_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0002865001019853522\n",
      "=== epoch:1, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0004078240868332423\n",
      "train loss:0.0004490799659696619\n",
      "train loss:0.0013827081944630946\n",
      "train loss:0.0006158399470003996\n",
      "train loss:0.0006582073450116921\n",
      "train loss:0.0006888535840492888\n",
      "train loss:9.066595012664693e-05\n",
      "train loss:0.0006570149149199866\n",
      "train loss:0.0016050344914998278\n",
      "train loss:0.00039577314558228265\n",
      "train loss:8.975544794200013e-05\n",
      "train loss:0.00039195269020150927\n",
      "train loss:0.0012393578087386695\n",
      "train loss:0.0063649575308971\n",
      "train loss:0.0009119819046198227\n",
      "train loss:0.0010276373224154189\n",
      "train loss:0.00012864798577408697\n",
      "train loss:0.00042719255115484763\n",
      "train loss:0.0009072920247126391\n",
      "train loss:0.00046238903544253536\n",
      "train loss:0.0013198162896033677\n",
      "train loss:0.00039810615311112655\n",
      "train loss:0.0010040558529587568\n",
      "train loss:0.0002020045458012687\n",
      "train loss:0.0005152727120041157\n",
      "train loss:0.000369705567133538\n",
      "train loss:0.001068293560292632\n",
      "train loss:0.0005126446953915454\n",
      "train loss:0.0007174260732369951\n",
      "train loss:0.00010043306466565715\n",
      "train loss:0.000662930166023222\n",
      "train loss:0.00014354461877486517\n",
      "train loss:0.0009793862799939066\n",
      "train loss:0.0002008657567661822\n",
      "train loss:0.0008477498031315503\n",
      "train loss:0.0004615085575610911\n",
      "train loss:0.0005361447136485252\n",
      "train loss:0.0004567514821392534\n",
      "train loss:0.0011563280702734059\n",
      "train loss:0.0010637121334431286\n",
      "train loss:0.0008076298654481346\n",
      "train loss:0.0011763009506193277\n",
      "train loss:0.0004441966597267305\n",
      "train loss:0.0009093312704951534\n",
      "train loss:0.000567549762393712\n",
      "train loss:0.00034262451848098734\n",
      "train loss:0.0008079522924521158\n",
      "train loss:0.000790706115006574\n",
      "train loss:0.001456106168359311\n",
      "train loss:0.001149638857077891\n",
      "train loss:0.0006579790581574223\n",
      "train loss:0.005034244817720039\n",
      "train loss:0.00040389720108539894\n",
      "train loss:0.0005512263605042684\n",
      "train loss:0.00017981264871552556\n",
      "train loss:0.00020176517749406121\n",
      "train loss:0.0004903990226178211\n",
      "train loss:0.0019911082577507497\n",
      "train loss:0.00029453086391664177\n",
      "train loss:0.000540834957558225\n",
      "train loss:0.000497676920345645\n",
      "train loss:0.0012676884352230192\n",
      "train loss:0.001177627957957858\n",
      "train loss:0.00018582253937080638\n",
      "train loss:0.0003921974410164591\n",
      "train loss:0.00017050799627333022\n",
      "train loss:0.0011665111578066092\n",
      "train loss:0.0005823428917030342\n",
      "train loss:0.0004651720717232761\n",
      "train loss:0.00023025200699380906\n",
      "train loss:0.0013880969588994446\n",
      "train loss:0.0006701321292002893\n",
      "train loss:0.0003084286556000051\n",
      "train loss:0.0002455081160670709\n",
      "train loss:0.00018887954500617624\n",
      "train loss:0.00012967741837339243\n",
      "train loss:0.00025727208667449946\n",
      "train loss:0.0010200907474607014\n",
      "train loss:0.004880985622713915\n",
      "train loss:0.0012418163853210613\n",
      "=== epoch:2, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0008673001485303552\n",
      "train loss:0.0004175159727598627\n",
      "train loss:0.00015398391087047308\n",
      "train loss:0.004397621804632199\n",
      "train loss:0.0007689252520364628\n",
      "train loss:0.0007421595246465356\n",
      "train loss:7.328983728949145e-05\n",
      "train loss:0.00030829360919631043\n",
      "train loss:0.0010923151989783502\n",
      "train loss:0.0004951639211691621\n",
      "train loss:0.0004913449424298757\n",
      "train loss:0.00010489721168411175\n",
      "train loss:8.122809404392943e-05\n",
      "train loss:0.0005301388347389134\n",
      "train loss:0.0008252723167047837\n",
      "train loss:0.001920859434231406\n",
      "train loss:0.002171855421042408\n",
      "train loss:0.001744132186172215\n",
      "train loss:0.00015598234886420663\n",
      "train loss:0.0009541487254147266\n",
      "train loss:0.00034725686234295073\n",
      "train loss:0.00041915096202145\n",
      "train loss:0.0001455674587605279\n",
      "train loss:0.0005202079377068877\n",
      "train loss:0.0007224834401361135\n",
      "train loss:0.0007311502590391581\n",
      "train loss:0.0007049146461556007\n",
      "train loss:0.00028916184563544634\n",
      "train loss:0.0017742290557829455\n",
      "train loss:0.0009361811341308434\n",
      "train loss:0.0011537149442214594\n",
      "train loss:0.0005380816524160831\n",
      "train loss:0.00024133249279100378\n",
      "train loss:0.0006199786242280031\n",
      "train loss:0.00037661834875381256\n",
      "train loss:0.0003845814388000995\n",
      "train loss:0.00021193780431867898\n",
      "train loss:0.0006288407259641941\n",
      "train loss:0.00033942054933926277\n",
      "train loss:0.0005467938336900338\n",
      "train loss:0.0005925433575408327\n",
      "train loss:0.0005790967755527485\n",
      "train loss:0.0008143737179107608\n",
      "train loss:0.0006510608136015863\n",
      "train loss:0.001232907462560502\n",
      "train loss:0.000663463515001407\n",
      "train loss:0.0008033688806101586\n",
      "train loss:0.000250935496621442\n",
      "train loss:0.0006460239322092391\n",
      "train loss:0.000615144692483414\n",
      "train loss:0.00046240661896549247\n",
      "train loss:0.001384398959243858\n",
      "train loss:0.0008492110955505376\n",
      "train loss:0.0006724826745895065\n",
      "train loss:0.00040595781567898874\n",
      "train loss:0.0004501038430622994\n",
      "train loss:0.0004063687561066282\n",
      "train loss:0.0008123929138108451\n",
      "train loss:0.0008530570043275173\n",
      "train loss:0.0005968604650810727\n",
      "train loss:0.0006530242444731007\n",
      "train loss:0.0002791442245608714\n",
      "train loss:0.00022150042553539532\n",
      "train loss:0.000576902014053889\n",
      "train loss:0.0007727796021323977\n",
      "train loss:0.00011891887068909758\n",
      "train loss:0.0004960865983265933\n",
      "train loss:0.0009830393827222927\n",
      "train loss:0.00043250127895821646\n",
      "train loss:0.00119896538956469\n",
      "train loss:0.0004155033150481149\n",
      "train loss:0.0002582374342835809\n",
      "train loss:0.0015859170559053121\n",
      "train loss:0.0004563635463657774\n",
      "train loss:0.0011713627277379016\n",
      "train loss:0.0006933199802668591\n",
      "train loss:0.0008221229095648683\n",
      "train loss:0.0001756959578880736\n",
      "train loss:0.00023752519385979336\n",
      "train loss:0.00021369323690256114\n",
      "=== epoch:3, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0002086399729165311\n",
      "train loss:0.0006650956565401139\n",
      "train loss:0.00031154999532147107\n",
      "train loss:0.0019120424669028116\n",
      "train loss:0.0009010900635448478\n",
      "train loss:0.001220505361931631\n",
      "train loss:0.0003883410944156689\n",
      "train loss:0.00011641500061682582\n",
      "train loss:0.000143938058426976\n",
      "train loss:0.0003321700734555197\n",
      "train loss:0.00037839342114979373\n",
      "train loss:0.0009346876841797233\n",
      "train loss:0.001105395852231964\n",
      "train loss:0.00041214503601708156\n",
      "train loss:0.00019110250392666815\n",
      "train loss:0.0003906876143046345\n",
      "train loss:0.00015214657841445796\n",
      "train loss:0.00021583277015000502\n",
      "train loss:0.0006862129037340529\n",
      "train loss:0.0025414990379294326\n",
      "train loss:0.00020573170395408563\n",
      "train loss:0.0002691710595371768\n",
      "train loss:0.0006643781814353937\n",
      "train loss:0.0006574202247558322\n",
      "train loss:0.000790265725095255\n",
      "train loss:0.0007767690973272561\n",
      "train loss:0.0004037350648118301\n",
      "train loss:0.00028415883166045205\n",
      "train loss:0.0004861830969261934\n",
      "train loss:0.0006429711319451106\n",
      "train loss:0.00023404921950548747\n",
      "train loss:0.00024995784374921127\n",
      "train loss:0.00019233950049278486\n",
      "train loss:0.0006951539685537589\n",
      "train loss:0.00021839932336130025\n",
      "train loss:0.0005337866143433782\n",
      "train loss:0.0005135875814248856\n",
      "train loss:0.0008251918676497992\n",
      "train loss:0.0004123395714024689\n",
      "train loss:0.00011142809584956913\n",
      "train loss:0.00039557256334909455\n",
      "train loss:0.0004260490697224817\n",
      "train loss:0.0004466196008750912\n",
      "train loss:0.0009048201173705723\n",
      "train loss:0.00024494149242799414\n",
      "train loss:0.004521184693810046\n",
      "train loss:7.389071071894909e-05\n",
      "train loss:0.00042327836672888307\n",
      "train loss:0.0003651078756746647\n",
      "train loss:0.0007896029174021882\n",
      "train loss:0.0008379219964438826\n",
      "train loss:0.0024770627882380746\n",
      "train loss:0.00018942477387422227\n",
      "train loss:0.0007027759983391184\n",
      "train loss:0.0005313620463268144\n",
      "train loss:0.0014207052041462795\n",
      "train loss:0.0012071811174065535\n",
      "train loss:5.763273118164404e-05\n",
      "train loss:0.00018060285307476619\n",
      "train loss:0.00022433422971803742\n",
      "train loss:0.0004733756471706156\n",
      "train loss:0.003004698087866208\n",
      "train loss:0.0008013538742367798\n",
      "train loss:0.00042862499690521057\n",
      "train loss:0.001541460928038094\n",
      "train loss:0.00020563404061766977\n",
      "train loss:0.002116680278498719\n",
      "train loss:0.0005661904758623766\n",
      "train loss:0.0006816896109000232\n",
      "train loss:0.00048581982492602573\n",
      "train loss:0.0003376760571486925\n",
      "train loss:0.00010212590208938071\n",
      "train loss:0.0007951174766740565\n",
      "train loss:0.0006335362782536376\n",
      "train loss:0.0001455693108786966\n",
      "train loss:0.0003361400093957199\n",
      "train loss:0.0003845556392230544\n",
      "train loss:0.00028350999629574576\n",
      "train loss:0.00035548936363682254\n",
      "train loss:0.0006082969368891374\n",
      "=== epoch:4, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0022266094575446667\n",
      "train loss:0.0022015175152208357\n",
      "train loss:0.00030041788815893033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00021291282748602452\n",
      "train loss:0.001575090702866793\n",
      "train loss:0.0003511616102508366\n",
      "train loss:0.0007921707250449741\n",
      "train loss:0.0005251665332484638\n",
      "train loss:0.0003066674097107269\n",
      "train loss:0.0010195409220154063\n",
      "train loss:0.00046189520944921484\n",
      "train loss:0.00021573450284063732\n",
      "train loss:0.00016551117073630188\n",
      "train loss:0.0001448871580671721\n",
      "train loss:0.0005684826791596886\n",
      "train loss:0.00029563296516316076\n",
      "train loss:0.00015777980172184747\n",
      "train loss:0.0003964363101685167\n",
      "train loss:0.0007925862330574072\n",
      "train loss:0.0003968731559688738\n",
      "train loss:0.0005216699563403636\n",
      "train loss:0.0013214560125534125\n",
      "train loss:0.0002186893176638455\n",
      "train loss:0.0002840174996005279\n",
      "train loss:0.00014213295739202723\n",
      "train loss:0.00032676903916011686\n",
      "train loss:0.0014443147628930375\n",
      "train loss:0.0007033481665656492\n",
      "train loss:0.000460738232615399\n",
      "train loss:0.00043182608135653334\n",
      "train loss:0.00013676198770604797\n",
      "train loss:0.0008025979767520604\n",
      "train loss:0.000342574625685639\n",
      "train loss:0.00024218351685486393\n",
      "train loss:0.000289461540150312\n",
      "train loss:0.0005043782800949735\n",
      "train loss:0.00038228657780084137\n",
      "train loss:0.0007855833137600718\n",
      "train loss:0.0001714628565911546\n",
      "train loss:0.00047874194917947095\n",
      "train loss:0.0007683324489249884\n",
      "train loss:0.0001594355957567692\n",
      "train loss:0.0005310114583932751\n",
      "train loss:0.0004548683971855825\n",
      "train loss:0.0002807139967215071\n",
      "train loss:0.0003657091796895292\n",
      "train loss:0.0003496520651785651\n",
      "train loss:0.00022691931447754433\n",
      "train loss:0.0005953871009149576\n",
      "train loss:0.0003100401812964904\n",
      "train loss:0.0005669910195325523\n",
      "train loss:0.0022000173076207447\n",
      "train loss:0.00021880521277694247\n",
      "train loss:0.0005241214279427815\n",
      "train loss:0.00014788136881235705\n",
      "train loss:0.00023555252586933168\n",
      "train loss:0.00017561246537602159\n",
      "train loss:0.00026598294174978135\n",
      "train loss:0.0004196029335286237\n",
      "train loss:0.0003554933279119369\n",
      "train loss:0.00015852204973143547\n",
      "train loss:0.0009810954229314453\n",
      "train loss:0.0007838123562882932\n",
      "train loss:0.0007121018112348171\n",
      "train loss:0.0008426847645801436\n",
      "train loss:0.0019325819954266448\n",
      "train loss:0.0009028489256115038\n",
      "train loss:0.0003638641478174044\n",
      "train loss:0.00019649332328021242\n",
      "train loss:0.0003349694407831695\n",
      "train loss:0.0010623443637074706\n",
      "train loss:0.002309782065824926\n",
      "train loss:0.00031266295028594044\n",
      "train loss:0.0007344899872920983\n",
      "train loss:0.0003905524822517158\n",
      "train loss:0.0003672476582962568\n",
      "train loss:0.00031599090112944384\n",
      "train loss:0.0005132549004673234\n",
      "train loss:0.00038598451163341334\n",
      "train loss:0.0014069924240084578\n",
      "=== epoch:5, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.001968574491963937\n",
      "train loss:0.00040579202168429377\n",
      "train loss:0.00031950626713748563\n",
      "train loss:0.00020209177816482088\n",
      "train loss:0.0005335472853346377\n",
      "train loss:0.0004173020033784224\n",
      "train loss:0.0001970019556337858\n",
      "train loss:0.000207307296173912\n",
      "train loss:0.0010862100334554652\n",
      "train loss:0.00016361590268458071\n",
      "train loss:0.00016466891972061662\n",
      "train loss:0.0001856775946335594\n",
      "train loss:0.0005362283686204938\n",
      "train loss:0.0011917297198127854\n",
      "train loss:0.00033075821747137924\n",
      "train loss:0.0036011811807815995\n",
      "train loss:0.0007477999181456531\n",
      "train loss:0.00013166135128790644\n",
      "train loss:0.00018403371351161384\n",
      "train loss:0.0007264970207106994\n",
      "train loss:0.00022883249648575018\n",
      "train loss:0.00039544884639918453\n",
      "train loss:0.00017974448515198154\n",
      "train loss:0.0006627431252025444\n",
      "train loss:0.00030688800548769434\n",
      "train loss:0.0005696456761440211\n",
      "train loss:0.0033481844329380788\n",
      "train loss:0.00021878063379975515\n",
      "train loss:0.0007371715745210783\n",
      "train loss:0.0021772323244745053\n",
      "train loss:0.001724537191215064\n",
      "train loss:0.00021011143671295938\n",
      "train loss:0.00028470773275919315\n",
      "train loss:0.0008169277969555839\n",
      "train loss:0.0001661741235092637\n",
      "train loss:0.0004519384606593299\n",
      "train loss:0.0002839593042982426\n",
      "train loss:0.0015617737058609058\n",
      "train loss:0.0007727770523729307\n",
      "train loss:0.00043667335588240276\n",
      "train loss:0.0001738273885989969\n",
      "train loss:0.00023448179882528186\n",
      "train loss:0.0001365088443970421\n",
      "train loss:0.00029221727300005566\n",
      "train loss:0.0035342509434333137\n",
      "train loss:0.00010768160147198\n",
      "train loss:0.0001723872045650442\n",
      "train loss:0.0009016162176715512\n",
      "train loss:0.00019405770840203712\n",
      "train loss:0.0009219783202961691\n",
      "train loss:0.0010922512233620947\n",
      "train loss:0.0006607730293430611\n",
      "train loss:0.0003172617086320199\n",
      "train loss:0.0006674224339531038\n",
      "train loss:0.0004071737201533289\n",
      "train loss:0.0004842949670251281\n",
      "train loss:0.000620980503690842\n",
      "train loss:0.0004590898222474703\n",
      "train loss:0.0007935405122145197\n",
      "train loss:0.0004514761607046664\n",
      "train loss:0.00037758113869080926\n",
      "train loss:0.00024961873456148066\n",
      "train loss:0.0003352746406648377\n",
      "train loss:0.001850207141423396\n",
      "train loss:0.00040857311279091676\n",
      "train loss:0.0003845762539249143\n",
      "train loss:0.00013015801034868896\n",
      "train loss:0.000658272999225329\n",
      "train loss:0.0004715619053508999\n",
      "train loss:0.0002644524861901752\n",
      "train loss:0.00030554958441078193\n",
      "train loss:0.0002910305786199172\n",
      "train loss:0.00038508066201601334\n",
      "train loss:0.0015074545237446402\n",
      "train loss:0.0003778335115239198\n",
      "train loss:0.0008023333490740055\n",
      "train loss:0.0007931291905873561\n",
      "train loss:0.00037896723783780497\n",
      "train loss:0.0015005690013497003\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:1.0\n",
      "train loss:0.0002604277843711193\n",
      "=== epoch:1, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0004282582576064116\n",
      "train loss:0.00038461911449455045\n",
      "train loss:0.0009810711378099746\n",
      "train loss:0.0002876747314602066\n",
      "train loss:0.00026583003473878156\n",
      "train loss:0.00046578802431531763\n",
      "train loss:0.00048457993944431337\n",
      "train loss:0.0008488733332367473\n",
      "train loss:0.0002570360026401094\n",
      "train loss:0.0003545112176345773\n",
      "train loss:0.0009936421993275775\n",
      "train loss:0.0006645952138984\n",
      "train loss:0.0008168434254678565\n",
      "train loss:0.0004206344645575249\n",
      "train loss:0.0032458070914399683\n",
      "train loss:0.0005765647839350211\n",
      "train loss:0.00035301910920825837\n",
      "train loss:0.0003232583584778482\n",
      "train loss:0.002523796833083944\n",
      "train loss:0.00043725777735060097\n",
      "train loss:0.00039578934633844736\n",
      "train loss:0.00042463942800612754\n",
      "train loss:0.0019314684940790153\n",
      "train loss:0.0010628005514535708\n",
      "train loss:0.0004439822766795119\n",
      "train loss:0.0005782319772472215\n",
      "train loss:0.0018499345305680501\n",
      "train loss:0.0014504200847487954\n",
      "train loss:0.0008198163304878087\n",
      "train loss:0.0006916893069369697\n",
      "train loss:0.001618680114317616\n",
      "train loss:0.0004911105964147927\n",
      "train loss:0.0006634327403042108\n",
      "train loss:0.00026886219607611306\n",
      "train loss:0.00014758467991816292\n",
      "train loss:0.0009926224801356234\n",
      "train loss:0.0011937882553140523\n",
      "train loss:0.0007096044801369678\n",
      "train loss:0.000742342409529419\n",
      "train loss:0.00010712694995418525\n",
      "train loss:0.0008578347461225952\n",
      "train loss:0.0010009293453991535\n",
      "train loss:0.00040609277052312234\n",
      "train loss:0.0020354090084972543\n",
      "train loss:0.0010657807315274759\n",
      "train loss:0.0005292720685788287\n",
      "train loss:0.00019115845618965477\n",
      "train loss:0.001051507394816473\n",
      "train loss:0.0012073608413769353\n",
      "train loss:0.0007102181309787416\n",
      "train loss:0.002107794397324837\n",
      "train loss:0.0004762099876841252\n",
      "train loss:6.60045467523657e-05\n",
      "train loss:0.0006383893872337115\n",
      "train loss:0.0006330930971456293\n",
      "train loss:0.00020446645064474734\n",
      "train loss:0.0004432117411097856\n",
      "train loss:0.00035505724332593957\n",
      "train loss:0.0004696384742750685\n",
      "train loss:0.0011465627297312657\n",
      "train loss:0.00031374209983554884\n",
      "train loss:0.0006518551284513091\n",
      "train loss:0.0003038879915283211\n",
      "train loss:0.00048226674061519376\n",
      "train loss:0.001354516052526825\n",
      "train loss:0.0005441481752732011\n",
      "train loss:0.0015417376113517536\n",
      "train loss:0.0009008201954233563\n",
      "train loss:0.0013327561610181726\n",
      "train loss:0.00014517730434765908\n",
      "train loss:0.002137540636442368\n",
      "train loss:0.0004661339059048417\n",
      "train loss:0.00036624638794762035\n",
      "train loss:0.0005145345070326134\n",
      "train loss:0.0005395040117791079\n",
      "train loss:0.0004010794944448302\n",
      "train loss:0.00070600915857164\n",
      "train loss:0.0008460478420954341\n",
      "train loss:0.0008729261430374316\n",
      "train loss:0.0011069824573167347\n",
      "=== epoch:2, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.0008822877111420821\n",
      "train loss:0.00113927703685061\n",
      "train loss:0.0037765971329656945\n",
      "train loss:0.0020508583619012867\n",
      "train loss:0.0003703444709101527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0005159560949792199\n",
      "train loss:0.00038481895364386634\n",
      "train loss:0.00031559360308202824\n",
      "train loss:0.0004381938495175517\n",
      "train loss:0.0015820570072222892\n",
      "train loss:0.002209438266084598\n",
      "train loss:0.0010215828947764502\n",
      "train loss:0.00048717948563812\n",
      "train loss:0.0004235047558764953\n",
      "train loss:0.0005917980363300572\n",
      "train loss:0.001459587114779373\n",
      "train loss:0.001387423857409422\n",
      "train loss:0.0010926464754784952\n",
      "train loss:0.0018693589819721067\n",
      "train loss:0.0007822944661518309\n",
      "train loss:0.000678657754316912\n",
      "train loss:0.0013028798507286183\n",
      "train loss:0.00040801353222033815\n",
      "train loss:0.0003079699862375663\n",
      "train loss:0.0006480035914392557\n",
      "train loss:0.0004183055818330391\n",
      "train loss:0.0009843969993316617\n",
      "train loss:0.00026575648633353943\n",
      "train loss:0.000302735871081047\n",
      "train loss:0.000652205162003523\n",
      "train loss:0.0003267938637388773\n",
      "train loss:0.0026711670513136303\n",
      "train loss:0.0009262628857309085\n",
      "train loss:0.0003917687401054493\n",
      "train loss:0.00024429183570054023\n",
      "train loss:0.001452003363605624\n",
      "train loss:0.0003770219455708968\n",
      "train loss:0.00047276625358238773\n",
      "train loss:0.0001468112532295295\n",
      "train loss:0.0004507560565362794\n",
      "train loss:0.0007412239713408305\n",
      "train loss:0.0008665504396996498\n",
      "train loss:0.0013597579057801604\n",
      "train loss:0.0006100128517163637\n",
      "train loss:0.0012208187463288612\n",
      "train loss:0.001372339822975093\n",
      "train loss:0.00159459817604691\n",
      "train loss:0.0003467309229829476\n",
      "train loss:0.0003244327985953832\n",
      "train loss:0.00012929530186867792\n",
      "train loss:0.0012143414886576585\n",
      "train loss:0.00018456888082463532\n",
      "train loss:0.0003760089077983892\n",
      "train loss:0.00025204140223051755\n",
      "train loss:0.0002792984660270253\n",
      "train loss:6.775234834003408e-05\n",
      "train loss:2.81551975136521e-05\n",
      "train loss:0.0020530718238663133\n",
      "train loss:0.0009334407727832146\n",
      "train loss:0.0003916171724314195\n",
      "train loss:0.0007125916425438574\n",
      "train loss:0.000685716042863212\n",
      "train loss:0.0001979272875716463\n",
      "train loss:0.0005189130905118533\n",
      "train loss:0.00033045815270954407\n",
      "train loss:0.00018218811296948675\n",
      "train loss:0.00031304690820714435\n",
      "train loss:0.00030153382891476917\n",
      "train loss:0.0010559502133816918\n",
      "train loss:0.0002476854807383297\n",
      "train loss:0.00020348705576401328\n",
      "train loss:0.0003771349292518025\n",
      "train loss:0.0006245919067189849\n",
      "train loss:0.0011376925463754404\n",
      "train loss:0.0005284752978778598\n",
      "train loss:0.0012268410685018322\n",
      "train loss:0.00013617384555306383\n",
      "train loss:0.0010485725894962186\n",
      "train loss:0.0002947286399162623\n",
      "train loss:0.0012176881544468132\n",
      "=== epoch:3, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.0008675347439341343\n",
      "train loss:0.0012665926809293743\n",
      "train loss:0.0009182989023545003\n",
      "train loss:0.0007227522019014967\n",
      "train loss:0.0005495664612030774\n",
      "train loss:0.0009168807981116038\n",
      "train loss:0.00027933787920246365\n",
      "train loss:0.0004016916420516949\n",
      "train loss:0.0005088448148847678\n",
      "train loss:0.00024146295124776866\n",
      "train loss:0.000731393959202448\n",
      "train loss:0.0011736108313934127\n",
      "train loss:0.00047551998478590716\n",
      "train loss:0.0006013003344519741\n",
      "train loss:0.0008390373230177166\n",
      "train loss:0.00039832495367166267\n",
      "train loss:0.000306679828394024\n",
      "train loss:0.00048432536710355016\n",
      "train loss:0.0010234525837858504\n",
      "train loss:8.832148597483969e-05\n",
      "train loss:0.0005996499924605105\n",
      "train loss:0.0002534455327574523\n",
      "train loss:0.0001751381537243286\n",
      "train loss:0.0005902796678827999\n",
      "train loss:0.0006598309468748261\n",
      "train loss:2.8916789733986328e-05\n",
      "train loss:0.0006242672252870651\n",
      "train loss:0.0009305815670307997\n",
      "train loss:0.0008106187842329658\n",
      "train loss:0.0006218566769448368\n",
      "train loss:0.0006702414305333229\n",
      "train loss:0.000824194764875961\n",
      "train loss:0.0004750750580329491\n",
      "train loss:0.00046297146876390075\n",
      "train loss:0.0006861599844224258\n",
      "train loss:0.0003158213381103329\n",
      "train loss:0.0022000671372007464\n",
      "train loss:0.0004646733072987753\n",
      "train loss:0.00016147229071501214\n",
      "train loss:9.286075558686426e-05\n",
      "train loss:0.0009122333923614584\n",
      "train loss:0.0009985806330807456\n",
      "train loss:0.0004005552884442391\n",
      "train loss:0.0010098096313784638\n",
      "train loss:0.001728668749184398\n",
      "train loss:0.00048656570580428074\n",
      "train loss:0.0004234099203118216\n",
      "train loss:0.0004266467813329436\n",
      "train loss:0.000375508695277768\n",
      "train loss:0.00021579317591277213\n",
      "train loss:9.287211864506784e-05\n",
      "train loss:0.0007721056110118849\n",
      "train loss:0.0011464285175200436\n",
      "train loss:0.0011996464377252026\n",
      "train loss:0.0003364450734085745\n",
      "train loss:0.0006960375558188002\n",
      "train loss:0.001078175456489326\n",
      "train loss:0.00034610508663384325\n",
      "train loss:0.0005270550627594993\n",
      "train loss:0.0006390956596743621\n",
      "train loss:0.00014995902548416227\n",
      "train loss:0.00014119439523573176\n",
      "train loss:0.0002807397635791036\n",
      "train loss:0.0005844523060489809\n",
      "train loss:0.0002391841844183953\n",
      "train loss:0.0019655833521008805\n",
      "train loss:9.990117083800428e-05\n",
      "train loss:0.00024808870845760897\n",
      "train loss:0.0019789803145738387\n",
      "train loss:0.0006234823974546485\n",
      "train loss:0.00020241299306304015\n",
      "train loss:0.009630980462802214\n",
      "train loss:0.0007940908855306017\n",
      "train loss:0.0005545299209781532\n",
      "train loss:9.222868246306758e-05\n",
      "train loss:6.111830608438433e-05\n",
      "train loss:0.0007085098799146465\n",
      "train loss:0.0002513256602343553\n",
      "train loss:0.000666672825776197\n",
      "train loss:0.002118038406363577\n",
      "=== epoch:4, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.0006041043613901599\n",
      "train loss:0.00028241753226045997\n",
      "train loss:0.0005550795409370343\n",
      "train loss:0.0005168117775057876\n",
      "train loss:0.00332369456354392\n",
      "train loss:0.001238819953621323\n",
      "train loss:0.0018209701371030634\n",
      "train loss:0.00030919294717538984\n",
      "train loss:0.0002840463660896964\n",
      "train loss:0.00045061516740420754\n",
      "train loss:0.0003939573481430571\n",
      "train loss:0.0015263544277214142\n",
      "train loss:0.0004171283442722734\n",
      "train loss:0.0032979477227411264\n",
      "train loss:0.0010380037751568083\n",
      "train loss:0.0035660610974429306\n",
      "train loss:0.0004678109973822092\n",
      "train loss:0.00043083202851328003\n",
      "train loss:0.00046858212750020554\n",
      "train loss:0.0005763432714227242\n",
      "train loss:0.0092390673301168\n",
      "train loss:0.0003888649193672606\n",
      "train loss:0.0022906780648473853\n",
      "train loss:0.0027768522052565633\n",
      "train loss:0.0002389909435334163\n",
      "train loss:0.0003938895158120008\n",
      "train loss:0.00017836755512885477\n",
      "train loss:0.0006663275482764899\n",
      "train loss:0.000393926178777419\n",
      "train loss:0.00031852348063585964\n",
      "train loss:0.0029085699944687298\n",
      "train loss:0.0010690098407356394\n",
      "train loss:0.0012427288958237082\n",
      "train loss:0.0007523883532415271\n",
      "train loss:0.0006268311393304439\n",
      "train loss:0.0035803712559408243\n",
      "train loss:0.0003839986557149731\n",
      "train loss:9.569719153139445e-05\n",
      "train loss:0.001080264801335841\n",
      "train loss:0.0009144864472296877\n",
      "train loss:0.0004456881547292642\n",
      "train loss:0.000358748398673914\n",
      "train loss:0.0011565322655610122\n",
      "train loss:0.00014794865476683836\n",
      "train loss:0.0008095814229897342\n",
      "train loss:0.0004790470312348641\n",
      "train loss:0.002117905656861645\n",
      "train loss:0.0011076098844882898\n",
      "train loss:0.00022119723180954822\n",
      "train loss:0.001223864345429855\n",
      "train loss:0.0002764510795208321\n",
      "train loss:0.0008611145592932562\n",
      "train loss:0.0016643728845604111\n",
      "train loss:0.0007159518748592732\n",
      "train loss:0.0002717180764746402\n",
      "train loss:0.0014899927571726834\n",
      "train loss:0.0002561817663042865\n",
      "train loss:4.227420564388577e-05\n",
      "train loss:0.0016948863038217037\n",
      "train loss:0.00022403795165022296\n",
      "train loss:0.001456806168200072\n",
      "train loss:0.0007923439973832164\n",
      "train loss:0.000217344421313583\n",
      "train loss:0.00025981135328772756\n",
      "train loss:0.0009531605377300896\n",
      "train loss:0.00019999666398616076\n",
      "train loss:0.00017336225170721514\n",
      "train loss:0.0013430766114191114\n",
      "train loss:0.0007947378487306291\n",
      "train loss:0.0009178918987123793\n",
      "train loss:0.000277566325900914\n",
      "train loss:0.0003554047477882488\n",
      "train loss:9.607211436324073e-05\n",
      "train loss:0.000300255708288652\n",
      "train loss:0.0006981108751645952\n",
      "train loss:0.000263046728085962\n",
      "train loss:0.0016855556139777053\n",
      "train loss:0.00047284636430701974\n",
      "train loss:0.0007020628786503256\n",
      "train loss:0.00041223333875926144\n",
      "=== epoch:5, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.0011340253310450684\n",
      "train loss:0.0009358981835713789\n",
      "train loss:0.0005094388992040283\n",
      "train loss:0.00020865537474782194\n",
      "train loss:0.00031248548223769437\n",
      "train loss:0.0003826541141964217\n",
      "train loss:0.0013045441134463813\n",
      "train loss:0.0015909295350011723\n",
      "train loss:0.0005327723154862954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00038084002382486886\n",
      "train loss:0.0002977276329367647\n",
      "train loss:0.000736686019696109\n",
      "train loss:0.0004896088092603497\n",
      "train loss:0.0005035089730005591\n",
      "train loss:0.0002278197568890998\n",
      "train loss:0.0007087659684182129\n",
      "train loss:8.26365527689864e-05\n",
      "train loss:0.0001300477103140747\n",
      "train loss:0.0006661017208845643\n",
      "train loss:0.00020431536248094635\n",
      "train loss:0.0003534419479318843\n",
      "train loss:0.00028404300889745904\n",
      "train loss:0.00039847576630971964\n",
      "train loss:0.0005646569511411875\n",
      "train loss:0.0002897820356146474\n",
      "train loss:0.00041492653640374876\n",
      "train loss:0.00014233604456131014\n",
      "train loss:0.0005206792799961988\n",
      "train loss:0.00016139423919924085\n",
      "train loss:0.0006010148705936241\n",
      "train loss:0.00036854534054039917\n",
      "train loss:0.0006789286029607832\n",
      "train loss:0.00017835360846173596\n",
      "train loss:0.0009447177711832097\n",
      "train loss:0.0004840300082233863\n",
      "train loss:0.00014148234599060426\n",
      "train loss:0.00029449459622250547\n",
      "train loss:0.0001806757069652829\n",
      "train loss:0.00041294503907266\n",
      "train loss:0.0005445541083751722\n",
      "train loss:0.0006682823222990008\n",
      "train loss:0.00039563000856613197\n",
      "train loss:7.483260123757877e-05\n",
      "train loss:8.889163465666019e-05\n",
      "train loss:0.0002882268264635363\n",
      "train loss:0.001216094816223924\n",
      "train loss:0.00024274944606384548\n",
      "train loss:0.0004807766433010137\n",
      "train loss:0.0007727753044785603\n",
      "train loss:0.00044076846046564653\n",
      "train loss:0.0004191529282560127\n",
      "train loss:0.0002449244722205771\n",
      "train loss:0.0005158098155533402\n",
      "train loss:0.0002835042174312918\n",
      "train loss:0.00033341015310521234\n",
      "train loss:0.00016611704909438697\n",
      "train loss:0.00015317621254692\n",
      "train loss:0.00015756706248005455\n",
      "train loss:0.00012242793114584246\n",
      "train loss:0.0002153768975159533\n",
      "train loss:0.0010950519130720367\n",
      "train loss:0.0003994698181971905\n",
      "train loss:0.00026939830057245393\n",
      "train loss:6.195084493692318e-05\n",
      "train loss:0.00017463228784454657\n",
      "train loss:0.0005709813980850867\n",
      "train loss:0.00041357652522806126\n",
      "train loss:0.00046838904423092214\n",
      "train loss:0.0003295796602462046\n",
      "train loss:0.00019671178788366239\n",
      "train loss:0.0006703545702959469\n",
      "train loss:0.0003320347029218069\n",
      "train loss:0.00046067201587207223\n",
      "train loss:0.0007001633943922485\n",
      "train loss:0.00010257033475412657\n",
      "train loss:0.00032167910112733855\n",
      "train loss:0.00033465932503306753\n",
      "train loss:0.0007970079470356429\n",
      "train loss:0.0005224177455292568\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:1.0\n"
     ]
    }
   ],
   "source": [
    "x_train=x_train_4\n",
    "x_test = x_test_4\n",
    "t_train=t_train_4\n",
    "t_test = t_test_4\n",
    "\n",
    "trainer_SGD = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='SGD', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_MMT = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Momentum', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_SGD.train()\n",
    "trainer_MMT.train()\n",
    "\n",
    "MMT_acc.append(trainer_MMT.test_acc_list[-1])\n",
    "SGD_acc.append(trainer_SGD.test_acc_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.000702329706777092\n",
      "=== epoch:1, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.00031871642717636495\n",
      "train loss:0.0008916743167579253\n",
      "train loss:0.00045896880953451383\n",
      "train loss:0.0004473427670606809\n",
      "train loss:0.0010119147406833131\n",
      "train loss:0.00011632957320947518\n",
      "train loss:0.0006840844988158245\n",
      "train loss:0.0004223735308544933\n",
      "train loss:0.00106274885119233\n",
      "train loss:0.00011249094419162638\n",
      "train loss:0.00034964641786740675\n",
      "train loss:0.0004237264624667858\n",
      "train loss:0.0002734964464147883\n",
      "train loss:0.00035329212508928204\n",
      "train loss:0.00016266677873754052\n",
      "train loss:0.0008787508321191102\n",
      "train loss:8.981843301357066e-05\n",
      "train loss:0.0006505930081922746\n",
      "train loss:0.0002393280300820992\n",
      "train loss:0.0012734687883916443\n",
      "train loss:0.0004458577202227229\n",
      "train loss:0.00158161033501547\n",
      "train loss:0.0004007586721777729\n",
      "train loss:0.00045725301391437813\n",
      "train loss:0.00025628007704045193\n",
      "train loss:0.001821054235302977\n",
      "train loss:8.103639051249635e-05\n",
      "train loss:0.0004853911562880998\n",
      "train loss:0.0008017710477994286\n",
      "train loss:0.000248901376325668\n",
      "train loss:0.000573041913640782\n",
      "train loss:0.00019728965698517998\n",
      "train loss:0.0002846869075137899\n",
      "train loss:0.0003303373591973547\n",
      "train loss:0.00029633857680971246\n",
      "train loss:0.0003196194877770003\n",
      "train loss:0.0010286035306393694\n",
      "train loss:5.387653483233444e-05\n",
      "train loss:0.00029024149800772935\n",
      "train loss:0.00022231144771470664\n",
      "train loss:0.0003057848461031271\n",
      "train loss:0.00013582288202024486\n",
      "train loss:0.00031837526162446284\n",
      "train loss:0.00020029804080274788\n",
      "train loss:0.00039281172644101885\n",
      "train loss:0.00033970802859674393\n",
      "train loss:0.003175919520891803\n",
      "train loss:0.00020264799620363914\n",
      "train loss:0.000291308226353619\n",
      "train loss:0.00023397983006257907\n",
      "train loss:0.0008604495464685993\n",
      "train loss:0.00033440854794400924\n",
      "train loss:0.0012746082614359488\n",
      "train loss:0.0004620392550227325\n",
      "train loss:0.0008961899243498926\n",
      "train loss:0.0007470828386411977\n",
      "train loss:0.00015800354199028047\n",
      "train loss:0.0005749613521061233\n",
      "train loss:0.0005150849267997516\n",
      "train loss:0.0007527248078894656\n",
      "train loss:0.0003429118138508523\n",
      "train loss:0.000682416998730925\n",
      "train loss:0.0006671698275546123\n",
      "train loss:0.00017483809380659102\n",
      "train loss:0.0005562565925882822\n",
      "train loss:0.0003235557210861668\n",
      "train loss:0.00037944615689819704\n",
      "train loss:0.0002150810642719042\n",
      "train loss:0.0001289197549480709\n",
      "train loss:0.0004711192040680875\n",
      "train loss:0.0006553964974346777\n",
      "train loss:0.0003864181118404043\n",
      "train loss:0.00022565829289583402\n",
      "train loss:0.0003052981348782315\n",
      "train loss:0.000534566946614518\n",
      "train loss:0.002292720997085068\n",
      "train loss:0.00042555718621251857\n",
      "train loss:0.0002983929934013594\n",
      "train loss:0.000243421536644201\n",
      "train loss:6.768570306187097e-05\n",
      "=== epoch:2, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.00025085610293564643\n",
      "train loss:0.0002449927150995396\n",
      "train loss:0.0006963202472336183\n",
      "train loss:0.0006290718506297078\n",
      "train loss:0.00045657867704006357\n",
      "train loss:0.0005893652653942734\n",
      "train loss:0.00042208959174546735\n",
      "train loss:0.0009750315959436321\n",
      "train loss:0.0002624291293192147\n",
      "train loss:0.0007237890855431917\n",
      "train loss:0.00048638093845760705\n",
      "train loss:0.00031340453373228735\n",
      "train loss:0.0003518138965248074\n",
      "train loss:0.0002644158066509169\n",
      "train loss:0.0006241268777685458\n",
      "train loss:0.0003428638853251511\n",
      "train loss:0.00037729316943636187\n",
      "train loss:0.0003402603651668409\n",
      "train loss:0.0005394582189573263\n",
      "train loss:0.0002823497921848368\n",
      "train loss:2.895309053718469e-05\n",
      "train loss:0.0005061523412375135\n",
      "train loss:0.0005141454999053639\n",
      "train loss:0.0006288357576312901\n",
      "train loss:0.0005294047865574488\n",
      "train loss:5.5604902391530144e-05\n",
      "train loss:0.00014822880888281634\n",
      "train loss:0.0001502707115848211\n",
      "train loss:0.00021430506562446008\n",
      "train loss:0.00041406644646631096\n",
      "train loss:0.0004122869703588838\n",
      "train loss:0.0002704077810713636\n",
      "train loss:0.00037526985075901143\n",
      "train loss:0.0002579707965768241\n",
      "train loss:0.0001474905447421752\n",
      "train loss:0.0006677925797605568\n",
      "train loss:0.0001862065968441783\n",
      "train loss:0.0002840190438820469\n",
      "train loss:0.00044395834320853954\n",
      "train loss:0.00017018837664002325\n",
      "train loss:0.00017947087498259976\n",
      "train loss:2.207361446231263e-05\n",
      "train loss:0.0003959623543599032\n",
      "train loss:0.0005082168290255025\n",
      "train loss:0.00017988213295028308\n",
      "train loss:0.000265104623070414\n",
      "train loss:0.00023382226073824844\n",
      "train loss:0.00017246823243572822\n",
      "train loss:0.0001311270147438907\n",
      "train loss:0.00019257063950756708\n",
      "train loss:0.00034923636093523965\n",
      "train loss:9.979078984595317e-05\n",
      "train loss:0.0003865216705557491\n",
      "train loss:0.0005357919230767698\n",
      "train loss:0.0005613820991297374\n",
      "train loss:0.0008159117220061791\n",
      "train loss:0.0003808348601023803\n",
      "train loss:0.00028497995010687077\n",
      "train loss:0.0003561085931407539\n",
      "train loss:0.00043929765540614917\n",
      "train loss:0.0006335122532834709\n",
      "train loss:0.0004958236872306323\n",
      "train loss:0.0005815022715249553\n",
      "train loss:0.00045491256911266023\n",
      "train loss:0.0006950526975423949\n",
      "train loss:0.00034749357538463907\n",
      "train loss:0.0004954233087901253\n",
      "train loss:0.0005396713447354294\n",
      "train loss:0.001140336457104573\n",
      "train loss:0.0014151742943397502\n",
      "train loss:0.0002035983030930405\n",
      "train loss:0.00013166991107495222\n",
      "train loss:0.0004005137915805166\n",
      "train loss:0.00018824673877072237\n",
      "train loss:6.0498747206973516e-05\n",
      "train loss:7.920829401998013e-05\n",
      "train loss:0.0004996505868011688\n",
      "train loss:0.0005602593105042504\n",
      "train loss:0.00017787642326327842\n",
      "train loss:0.0009498904809528107\n",
      "=== epoch:3, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.0008783725130944036\n",
      "train loss:0.0008622486609586861\n",
      "train loss:0.002198400272178052\n",
      "train loss:0.0004265157215707467\n",
      "train loss:0.0006907172152815613\n",
      "train loss:0.00024667937468767024\n",
      "train loss:0.00021936475719807236\n",
      "train loss:0.00033317894062853405\n",
      "train loss:0.0004267877155582705\n",
      "train loss:0.00033272317423830065\n",
      "train loss:0.0003904731324158574\n",
      "train loss:0.0006782164206972805\n",
      "train loss:4.1569748443666494e-05\n",
      "train loss:0.000339251016084654\n",
      "train loss:0.00046576362539645685\n",
      "train loss:0.00013230390499429138\n",
      "train loss:0.00022820123269703637\n",
      "train loss:0.0011436728937813108\n",
      "train loss:0.00021303590212268567\n",
      "train loss:0.00013367894736939916\n",
      "train loss:0.0004973486107325514\n",
      "train loss:0.000689268911598138\n",
      "train loss:0.00015134680478375088\n",
      "train loss:0.0007076421491565836\n",
      "train loss:0.0003651342369534852\n",
      "train loss:8.890751383132216e-05\n",
      "train loss:0.0004897577247668646\n",
      "train loss:4.70694653731101e-05\n",
      "train loss:0.0001912325690354718\n",
      "train loss:9.128809379008866e-05\n",
      "train loss:0.00041088971206881147\n",
      "train loss:0.0006900216405411597\n",
      "train loss:0.0006528227911167889\n",
      "train loss:0.00039313540549245295\n",
      "train loss:0.0007729591245205849\n",
      "train loss:0.0004813346989109186\n",
      "train loss:0.0003523908630980799\n",
      "train loss:0.00022818428446426682\n",
      "train loss:0.00037547815050970864\n",
      "train loss:0.00022023565782428435\n",
      "train loss:0.00030681604564242106\n",
      "train loss:0.0003387255912315192\n",
      "train loss:0.00021863860477157478\n",
      "train loss:0.0004936242374494411\n",
      "train loss:0.00040462395961870406\n",
      "train loss:0.00015093614911416354\n",
      "train loss:8.092312039537357e-05\n",
      "train loss:0.0006429410590179943\n",
      "train loss:0.0009351000642790986\n",
      "train loss:0.0002989273548512995\n",
      "train loss:0.0001378938683285323\n",
      "train loss:0.00019052140259546768\n",
      "train loss:0.0004965885768496973\n",
      "train loss:0.00042085722361130955\n",
      "train loss:0.0003217100612111526\n",
      "train loss:0.0002687626283024934\n",
      "train loss:0.0016683293287441151\n",
      "train loss:0.0003033272230063158\n",
      "train loss:0.00018514152683170244\n",
      "train loss:0.00017585217809953647\n",
      "train loss:0.00019345349870311668\n",
      "train loss:0.0014222406510168472\n",
      "train loss:0.0004245528725198934\n",
      "train loss:0.00048552196823737086\n",
      "train loss:0.0005954630140098566\n",
      "train loss:0.0004791894897839594\n",
      "train loss:0.0010572624148194774\n",
      "train loss:0.0003422325925125101\n",
      "train loss:0.00016651345410942565\n",
      "train loss:0.00024004314966133793\n",
      "train loss:0.00023354302944166987\n",
      "train loss:0.00024922211941002457\n",
      "train loss:0.0005989175428026306\n",
      "train loss:0.0002605823001924634\n",
      "train loss:0.0006920647937946116\n",
      "train loss:0.0003292402139240503\n",
      "train loss:0.0007619861853613917\n",
      "train loss:0.0004908292050121866\n",
      "train loss:0.00021729741589865783\n",
      "train loss:0.0001678451725383956\n",
      "=== epoch:4, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.0004309153015451532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00019976297025808053\n",
      "train loss:0.001221427921082663\n",
      "train loss:0.0005903441348700467\n",
      "train loss:0.0002917966903504916\n",
      "train loss:0.00013471678629105457\n",
      "train loss:0.00024135872331484487\n",
      "train loss:0.0008296778545687146\n",
      "train loss:0.00032936051118914045\n",
      "train loss:0.0005638905169320451\n",
      "train loss:0.00021227022768839597\n",
      "train loss:0.0006133519855810954\n",
      "train loss:0.0006686727565057046\n",
      "train loss:0.0005483562574397741\n",
      "train loss:0.0011148857496118866\n",
      "train loss:7.716246457237321e-05\n",
      "train loss:0.00041340661633620186\n",
      "train loss:0.00016989925265461996\n",
      "train loss:0.0008229582315388699\n",
      "train loss:0.00032805010135023856\n",
      "train loss:0.0005695240665988049\n",
      "train loss:0.00014814536773738465\n",
      "train loss:0.0002446879384205055\n",
      "train loss:0.00039790815587302004\n",
      "train loss:0.0007290294461401979\n",
      "train loss:0.00013281948789250307\n",
      "train loss:0.0005872186501495758\n",
      "train loss:9.398146950738533e-05\n",
      "train loss:0.001089819139136968\n",
      "train loss:0.0003237008654371455\n",
      "train loss:0.00017043195755511712\n",
      "train loss:0.00045310388225684376\n",
      "train loss:0.0001474316737781902\n",
      "train loss:0.000966469809553636\n",
      "train loss:0.00012899612360160685\n",
      "train loss:0.0001939657504544029\n",
      "train loss:0.0001510487332577632\n",
      "train loss:0.0004501072750987735\n",
      "train loss:0.0008683579823249151\n",
      "train loss:0.00023158371615585163\n",
      "train loss:0.0006024329738852391\n",
      "train loss:0.0003265638233380192\n",
      "train loss:0.00045134908829642413\n",
      "train loss:0.0002797867302091852\n",
      "train loss:0.00045272334669146263\n",
      "train loss:0.0005599862383784105\n",
      "train loss:0.0005826786894704714\n",
      "train loss:0.0021943223979696215\n",
      "train loss:0.00017406707855520422\n",
      "train loss:0.0007013525555764454\n",
      "train loss:0.00024891989877985236\n",
      "train loss:0.0003811649618579203\n",
      "train loss:0.0006109396339319845\n",
      "train loss:0.00022688660216275386\n",
      "train loss:0.0007272765579905203\n",
      "train loss:0.0003274701988487111\n",
      "train loss:0.0004185143254888891\n",
      "train loss:0.0006918171858247132\n",
      "train loss:0.00030734974419672757\n",
      "train loss:0.000310133956807455\n",
      "train loss:0.00046296799747023233\n",
      "train loss:0.0002505180343487303\n",
      "train loss:0.00042261622963285636\n",
      "train loss:0.0006082696958674068\n",
      "train loss:0.001853723795680197\n",
      "train loss:0.000980372626203703\n",
      "train loss:0.00046229625177413274\n",
      "train loss:0.00030485893176380855\n",
      "train loss:0.0006023158405064102\n",
      "train loss:0.0004601699879740238\n",
      "train loss:0.00113934565494942\n",
      "train loss:0.00043124934945069156\n",
      "train loss:0.00021791747166312325\n",
      "train loss:0.00047405246686803325\n",
      "train loss:0.00033003381987207434\n",
      "train loss:0.00011660267752565054\n",
      "train loss:0.00010890084637434018\n",
      "train loss:0.0010856463077501387\n",
      "train loss:0.00026745113177016947\n",
      "train loss:0.0006758071310851156\n",
      "=== epoch:5, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.0006638698471497988\n",
      "train loss:0.00020419587095765374\n",
      "train loss:0.0006584868715197144\n",
      "train loss:0.0009103052430556599\n",
      "train loss:0.0004161023576905622\n",
      "train loss:0.00012249686561899035\n",
      "train loss:0.00027934830846803807\n",
      "train loss:0.0009285144212557593\n",
      "train loss:0.00011961781325175129\n",
      "train loss:0.0005056897095388594\n",
      "train loss:0.00014772879411448564\n",
      "train loss:6.049377657809339e-05\n",
      "train loss:0.0006803519223704401\n",
      "train loss:0.0004281101128744008\n",
      "train loss:6.698193840879559e-05\n",
      "train loss:0.0005831798272602809\n",
      "train loss:0.00021025756640919612\n",
      "train loss:0.0003229206186310072\n",
      "train loss:0.00032036193291705905\n",
      "train loss:0.00036334447615531053\n",
      "train loss:0.00014182131854498556\n",
      "train loss:0.0002783865512096131\n",
      "train loss:0.0004989686174425531\n",
      "train loss:5.583779222997745e-05\n",
      "train loss:0.0002608625621874602\n",
      "train loss:0.0009783647259781576\n",
      "train loss:0.0004197530015546424\n",
      "train loss:0.0010865600660574988\n",
      "train loss:0.0004097056279464959\n",
      "train loss:0.00012985795829057763\n",
      "train loss:0.00027151545505509403\n",
      "train loss:0.00027573243930244034\n",
      "train loss:0.0004968787897290986\n",
      "train loss:0.000420998918319679\n",
      "train loss:0.0002961383631657607\n",
      "train loss:0.00011533446262712788\n",
      "train loss:0.0003351175009402323\n",
      "train loss:0.00022745453395759947\n",
      "train loss:0.00021636236672450834\n",
      "train loss:0.0007473129507008326\n",
      "train loss:0.00019972407336012188\n",
      "train loss:0.00012868833913244222\n",
      "train loss:6.692657744360547e-05\n",
      "train loss:0.0004296352200939343\n",
      "train loss:0.00015353855764784028\n",
      "train loss:0.00012588178997100083\n",
      "train loss:0.00038021854686237324\n",
      "train loss:0.000203903905233558\n",
      "train loss:0.0003366854999356796\n",
      "train loss:0.000635790113814826\n",
      "train loss:0.0007120387813373822\n",
      "train loss:0.00021192590787687975\n",
      "train loss:0.00023716588529951288\n",
      "train loss:0.0008237972548389156\n",
      "train loss:0.000348474829601529\n",
      "train loss:0.0006908006866602266\n",
      "train loss:0.0004942066585811685\n",
      "train loss:0.002108594195286766\n",
      "train loss:0.00031351254136697745\n",
      "train loss:0.0009062633755186404\n",
      "train loss:0.0006905552191096119\n",
      "train loss:0.00027908174623800827\n",
      "train loss:0.00030597525328256186\n",
      "train loss:0.0006064430199962965\n",
      "train loss:0.00022126371416613812\n",
      "train loss:0.00039411056040639244\n",
      "train loss:0.00047553862539062645\n",
      "train loss:0.000908257750804893\n",
      "train loss:0.000499848376177697\n",
      "train loss:0.001038017092128143\n",
      "train loss:0.0004632550001174801\n",
      "train loss:0.00026096954558317765\n",
      "train loss:0.0004518249290482312\n",
      "train loss:0.000373212177011438\n",
      "train loss:0.00012840580626694397\n",
      "train loss:0.00031602564835758187\n",
      "train loss:0.0005917364298992108\n",
      "train loss:0.0004914738553914423\n",
      "train loss:0.0007683227634726862\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9995\n",
      "train loss:0.0004022258902709356\n",
      "=== epoch:1, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.0005167394665404043\n",
      "train loss:0.00011143529778646955\n",
      "train loss:0.00044284543900131553\n",
      "train loss:0.0007335180140412771\n",
      "train loss:0.00026304744101499213\n",
      "train loss:0.000377120318075723\n",
      "train loss:0.0010268593150227422\n",
      "train loss:0.00018424856268611824\n",
      "train loss:0.00027428042426671335\n",
      "train loss:0.00012339796288375203\n",
      "train loss:0.0005216096854351463\n",
      "train loss:0.00029812180218272496\n",
      "train loss:0.0005788831926001859\n",
      "train loss:0.00025778558180650994\n",
      "train loss:0.00032996494034183004\n",
      "train loss:0.0002931461244729487\n",
      "train loss:0.00022599840118479666\n",
      "train loss:0.000543549654892221\n",
      "train loss:0.0004826050509361396\n",
      "train loss:0.00033185805751168945\n",
      "train loss:0.0004548897506054906\n",
      "train loss:0.0007574947396834286\n",
      "train loss:0.0004939560206755814\n",
      "train loss:0.00023250744324617006\n",
      "train loss:0.0006358392983766156\n",
      "train loss:0.00011818916776630098\n",
      "train loss:0.0007253980343964819\n",
      "train loss:0.0003607608006453416\n",
      "train loss:0.00019876752061608199\n",
      "train loss:0.00014023238383433623\n",
      "train loss:0.0012451929817219674\n",
      "train loss:0.00024820554335042215\n",
      "train loss:0.0010907436809259351\n",
      "train loss:0.00036797967840271963\n",
      "train loss:0.000356597234258863\n",
      "train loss:0.0006269061320003822\n",
      "train loss:0.00022143009245200498\n",
      "train loss:0.00023288041709058807\n",
      "train loss:0.0005602863422667501\n",
      "train loss:0.00042654266051072703\n",
      "train loss:0.0003919668677745955\n",
      "train loss:0.00023954877359167522\n",
      "train loss:0.0001601801216332865\n",
      "train loss:0.0004726818349560468\n",
      "train loss:0.0002626485333746587\n",
      "train loss:0.0002670358089219867\n",
      "train loss:0.00031339391633662596\n",
      "train loss:0.0007770465669152696\n",
      "train loss:0.0001888243193588204\n",
      "train loss:0.0004276802501256349\n",
      "train loss:0.00052641652012841\n",
      "train loss:0.0002351620951638252\n",
      "train loss:0.0004101830895512412\n",
      "train loss:0.000296564839745448\n",
      "train loss:0.00018893591566189322\n",
      "train loss:0.0006972143077905803\n",
      "train loss:0.0020079032745872025\n",
      "train loss:0.00017412257496182744\n",
      "train loss:0.0005316657043675388\n",
      "train loss:0.00043458388822291525\n",
      "train loss:0.0003019154092525943\n",
      "train loss:0.000252271881798221\n",
      "train loss:0.00033719671646180975\n",
      "train loss:0.00046819912954916136\n",
      "train loss:0.0007044237772312903\n",
      "train loss:0.00010898413744684001\n",
      "train loss:0.000229403441492581\n",
      "train loss:9.959965421141744e-05\n",
      "train loss:0.000352065796402821\n",
      "train loss:0.0001745557529256454\n",
      "train loss:0.00033601221861821913\n",
      "train loss:0.00012185102108628532\n",
      "train loss:0.00019216987598768976\n",
      "train loss:0.0006834404087103041\n",
      "train loss:0.00018152776170681927\n",
      "train loss:0.0001566740313541403\n",
      "train loss:0.0009064671386207413\n",
      "train loss:0.00045649661327042574\n",
      "train loss:0.0003068693800523466\n",
      "train loss:0.000395868366512131\n",
      "=== epoch:2, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.0006956156823873902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0002038446182381187\n",
      "train loss:0.0004981278429009431\n",
      "train loss:0.0001343097013002796\n",
      "train loss:0.0003688596689617772\n",
      "train loss:0.0004094061584738901\n",
      "train loss:0.0002486198355199429\n",
      "train loss:0.0004991144641726235\n",
      "train loss:0.00036531346060285623\n",
      "train loss:0.0002610563117432775\n",
      "train loss:0.00039252364492473475\n",
      "train loss:0.00024096906549552096\n",
      "train loss:0.0005424116905883733\n",
      "train loss:0.0004082998538040359\n",
      "train loss:0.00032573787521135623\n",
      "train loss:0.00019585406289126031\n",
      "train loss:0.0004368849790305394\n",
      "train loss:0.00021077434586270322\n",
      "train loss:0.00044747805024133514\n",
      "train loss:0.00034890956045041834\n",
      "train loss:0.0012215496960657417\n",
      "train loss:0.00014726829992278405\n",
      "train loss:0.000547348220700525\n",
      "train loss:0.00040922734485686487\n",
      "train loss:0.0005113405756923691\n",
      "train loss:0.00020941935433973328\n",
      "train loss:7.894891770871977e-05\n",
      "train loss:0.0003002860433667628\n",
      "train loss:0.0003843170483612504\n",
      "train loss:0.00014749759096217735\n",
      "train loss:0.0004035561824351107\n",
      "train loss:0.0006786134084516078\n",
      "train loss:0.0003083521122808848\n",
      "train loss:0.00017609561631968618\n",
      "train loss:0.001126086109476374\n",
      "train loss:0.0002041377194497113\n",
      "train loss:0.00028727129401757187\n",
      "train loss:0.00047667570513295325\n",
      "train loss:0.0002669257301318109\n",
      "train loss:0.0007910207259340758\n",
      "train loss:0.00045331773524491005\n",
      "train loss:0.0009717735905628417\n",
      "train loss:0.0015375657867550457\n",
      "train loss:0.0003774247710866583\n",
      "train loss:0.0002834958680741182\n",
      "train loss:0.0005701282904050857\n",
      "train loss:0.000260286995197935\n",
      "train loss:0.000464984185554777\n",
      "train loss:0.0004653438116714781\n",
      "train loss:0.0004981136372490154\n",
      "train loss:0.0003117254510368435\n",
      "train loss:0.0004991926472408173\n",
      "train loss:0.00021651046414980832\n",
      "train loss:0.0003795633122430798\n",
      "train loss:0.0005437951497571171\n",
      "train loss:0.0006790118112973285\n",
      "train loss:0.0005674670571621069\n",
      "train loss:0.0005198400453497419\n",
      "train loss:0.00032115489737763144\n",
      "train loss:0.0003827911965250167\n",
      "train loss:0.0002715252507577861\n",
      "train loss:0.00024073344880584833\n",
      "train loss:0.000277762150675946\n",
      "train loss:0.0003669040835983945\n",
      "train loss:0.0003267644994778358\n",
      "train loss:0.00019366419420961125\n",
      "train loss:0.00017136220780440478\n",
      "train loss:0.0003500087755224468\n",
      "train loss:0.00022598964454472798\n",
      "train loss:0.0007126555340706099\n",
      "train loss:0.00015776380206451065\n",
      "train loss:0.00040576166043230876\n",
      "train loss:0.00026589692772336477\n",
      "train loss:0.0004914715971978424\n",
      "train loss:0.0004752037523282076\n",
      "train loss:0.00016626147198160128\n",
      "train loss:0.0002500307333356935\n",
      "train loss:0.0005777149415037855\n",
      "train loss:0.0002296869155325469\n",
      "train loss:0.0007127574022035472\n",
      "=== epoch:3, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.00026404699035023643\n",
      "train loss:0.00036333018868731705\n",
      "train loss:0.00019766381194211694\n",
      "train loss:0.0003034607366842989\n",
      "train loss:0.00048227682415202296\n",
      "train loss:0.00030290280934879314\n",
      "train loss:0.0002821978753650451\n",
      "train loss:0.0001875031713658373\n",
      "train loss:0.002304025348346039\n",
      "train loss:0.0008394310011304461\n",
      "train loss:0.00011094890466004865\n",
      "train loss:5.047520548782657e-05\n",
      "train loss:0.00027766214621873593\n",
      "train loss:0.00014008784674280578\n",
      "train loss:0.00021364607861426268\n",
      "train loss:0.00043582890496419833\n",
      "train loss:0.00039304019068841026\n",
      "train loss:0.00044250805813809355\n",
      "train loss:0.000824342545531899\n",
      "train loss:0.00031874439826532223\n",
      "train loss:0.0003225882369627335\n",
      "train loss:0.0001947988381867255\n",
      "train loss:0.0002936867013425168\n",
      "train loss:0.0003403614659253025\n",
      "train loss:0.00020695080676572306\n",
      "train loss:0.00010400637727748559\n",
      "train loss:0.0006292161246877604\n",
      "train loss:0.0006883529390141946\n",
      "train loss:0.0002111694998838287\n",
      "train loss:0.0003704093762166415\n",
      "train loss:0.00013697010786071972\n",
      "train loss:0.001192864117022822\n",
      "train loss:0.0004179557144458008\n",
      "train loss:0.000434652905472695\n",
      "train loss:7.441745822706876e-05\n",
      "train loss:0.0001972225540731459\n",
      "train loss:0.0006734389041911851\n",
      "train loss:0.00023161097463306364\n",
      "train loss:0.00025690426904353427\n",
      "train loss:0.0007960191291908345\n",
      "train loss:0.000134868808181541\n",
      "train loss:0.00021114586964972133\n",
      "train loss:0.00044051236177412205\n",
      "train loss:0.0003072279436476015\n",
      "train loss:0.00034629969058374237\n",
      "train loss:0.0005313808708945121\n",
      "train loss:0.00030674892570348373\n",
      "train loss:0.00011607612233787043\n",
      "train loss:0.00041423016132288633\n",
      "train loss:0.0017627238598960492\n",
      "train loss:0.00010029167120507089\n",
      "train loss:0.00046528466990262323\n",
      "train loss:0.0005838267910862062\n",
      "train loss:0.0003945315078917097\n",
      "train loss:0.00018238084723795828\n",
      "train loss:0.00024937096844047935\n",
      "train loss:0.0006268450422799623\n",
      "train loss:0.00022901238388560536\n",
      "train loss:0.0006509449013795911\n",
      "train loss:0.00020795249818695408\n",
      "train loss:0.00019734196625527213\n",
      "train loss:0.00019678760705312996\n",
      "train loss:6.752300930561964e-05\n",
      "train loss:0.0004833170910666427\n",
      "train loss:0.0002657910660474383\n",
      "train loss:0.00018479613830623936\n",
      "train loss:0.00019934200610721695\n",
      "train loss:0.00013773446397585872\n",
      "train loss:8.432820983123268e-05\n",
      "train loss:0.00015778687586473337\n",
      "train loss:0.0002558032276460522\n",
      "train loss:0.00016100699028894958\n",
      "train loss:0.00010604727847393644\n",
      "train loss:0.0006851983102365669\n",
      "train loss:0.0003746062984746831\n",
      "train loss:0.0006228353459852306\n",
      "train loss:0.00017532920304827424\n",
      "train loss:0.0010213545932630096\n",
      "train loss:0.0003771797864679184\n",
      "train loss:0.00014451492979924781\n",
      "=== epoch:4, train acc:1.0, test acc:0.999 ===\n",
      "train loss:0.00019801022226940133\n",
      "train loss:0.00026153183730732847\n",
      "train loss:0.00042618419255858775\n",
      "train loss:0.00038972468986395013\n",
      "train loss:0.00023008208090723295\n",
      "train loss:0.00015431866265964764\n",
      "train loss:0.00016824818637798126\n",
      "train loss:0.00023489866131425717\n",
      "train loss:0.0005165606015399204\n",
      "train loss:0.0005219770116252982\n",
      "train loss:0.0007858930412899531\n",
      "train loss:0.0001645242378071398\n",
      "train loss:0.0008780013095834658\n",
      "train loss:7.657738045939234e-05\n",
      "train loss:0.00035480223247192125\n",
      "train loss:0.0004489097559399283\n",
      "train loss:9.854476059912758e-05\n",
      "train loss:0.00025634159356024787\n",
      "train loss:0.0003456682114539406\n",
      "train loss:0.00018422511227271607\n",
      "train loss:0.0005344371153740754\n",
      "train loss:0.00029576220048438634\n",
      "train loss:0.0003282493761800164\n",
      "train loss:0.0002982403941422653\n",
      "train loss:4.418190724779672e-05\n",
      "train loss:0.0003233465865543538\n",
      "train loss:0.00036077010056658463\n",
      "train loss:0.000594377041093998\n",
      "train loss:0.00049539488055424\n",
      "train loss:0.000511611244844965\n",
      "train loss:0.0004248470634670717\n",
      "train loss:0.0004479777281617086\n",
      "train loss:0.00016532757444263086\n",
      "train loss:0.00017983198650642378\n",
      "train loss:0.0003490927328598539\n",
      "train loss:0.000492192576913212\n",
      "train loss:0.0003242418321809268\n",
      "train loss:0.0005022990013372681\n",
      "train loss:0.0007058330819872011\n",
      "train loss:0.00016426914986593418\n",
      "train loss:0.00024208302911189262\n",
      "train loss:0.00038594485573544555\n",
      "train loss:0.0003054244250394361\n",
      "train loss:0.0001665398546512845\n",
      "train loss:0.00031122094250878046\n",
      "train loss:0.0004636713707220771\n",
      "train loss:0.0002705752319430851\n",
      "train loss:0.00017157392755319078\n",
      "train loss:0.00011895334439903103\n",
      "train loss:0.0002521454142038008\n",
      "train loss:0.00039265438465275614\n",
      "train loss:0.00033277058281664556\n",
      "train loss:0.00010618016216318898\n",
      "train loss:0.0003568180605450873\n",
      "train loss:0.0006022431364372279\n",
      "train loss:0.00022404784512239134\n",
      "train loss:0.0003255829912482155\n",
      "train loss:0.0006717013317122188\n",
      "train loss:0.0003112053059157393\n",
      "train loss:0.00012543854752597038\n",
      "train loss:0.00024241271611774397\n",
      "train loss:0.0004071958743319792\n",
      "train loss:0.00017602834255035026\n",
      "train loss:0.0005714176532017012\n",
      "train loss:0.00038016920683968934\n",
      "train loss:0.000414526931615707\n",
      "train loss:0.0003798242503376176\n",
      "train loss:0.0006823557643054854\n",
      "train loss:0.0004389786200316928\n",
      "train loss:0.00011610843847910374\n",
      "train loss:0.0007223579612661738\n",
      "train loss:0.00017256024106955626\n",
      "train loss:0.0002726601556830291\n",
      "train loss:0.0001923849483712811\n",
      "train loss:0.0002272218645355755\n",
      "train loss:0.0003292694987742938\n",
      "train loss:8.865732540028205e-05\n",
      "train loss:0.00022552472439037083\n",
      "train loss:0.00020458452894192697\n",
      "train loss:0.00015894543927529134\n",
      "=== epoch:5, train acc:1.0, test acc:0.999 ===\n",
      "train loss:6.948074036854118e-05\n",
      "train loss:0.0003498193439576785\n",
      "train loss:0.0002817932366622398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0002643935623257824\n",
      "train loss:0.0002575896721918017\n",
      "train loss:0.00021222256123063564\n",
      "train loss:0.0002436823018523784\n",
      "train loss:0.0001917361818997188\n",
      "train loss:7.04040650445858e-05\n",
      "train loss:0.00023494762421628887\n",
      "train loss:0.00023483702679577754\n",
      "train loss:0.00017892227351298917\n",
      "train loss:0.0005415701700006304\n",
      "train loss:0.00042152510165779656\n",
      "train loss:4.344009231681047e-05\n",
      "train loss:0.00026094902373632075\n",
      "train loss:0.00024195081044971168\n",
      "train loss:0.0006230616603048835\n",
      "train loss:0.00021420376936554284\n",
      "train loss:0.000326183622644753\n",
      "train loss:0.0004987141491628055\n",
      "train loss:0.00015925924501436085\n",
      "train loss:0.00027932500264082677\n",
      "train loss:0.00017109555523654477\n",
      "train loss:0.0003584355668363768\n",
      "train loss:0.00017155401438006987\n",
      "train loss:5.7151322006281606e-05\n",
      "train loss:0.00021234409192124598\n",
      "train loss:4.358186560333479e-05\n",
      "train loss:0.0001373636599090336\n",
      "train loss:0.0001479037118517315\n",
      "train loss:0.00021260056669718914\n",
      "train loss:0.0002984904666310643\n",
      "train loss:0.00010989301594583362\n",
      "train loss:0.00020330317205091624\n",
      "train loss:0.00016883851355843193\n",
      "train loss:0.0004597817187947744\n",
      "train loss:0.0003425978260237355\n",
      "train loss:0.0003575921243214212\n",
      "train loss:0.00022140667478530953\n",
      "train loss:0.00020317159205517928\n",
      "train loss:0.0004590791433024269\n",
      "train loss:0.00013322123823070642\n",
      "train loss:0.0002809455822112099\n",
      "train loss:0.00025675244706504233\n",
      "train loss:0.00020954735592853565\n",
      "train loss:0.0005011928678998811\n",
      "train loss:6.735762356485551e-05\n",
      "train loss:0.00019642579933950807\n",
      "train loss:0.0003620012930807338\n",
      "train loss:2.4098644841143812e-05\n",
      "train loss:0.0005437436832679686\n",
      "train loss:0.00013059421374637825\n",
      "train loss:0.00014578514161306996\n",
      "train loss:0.00023967565363174355\n",
      "train loss:0.00023486900382836508\n",
      "train loss:8.450444668574387e-05\n",
      "train loss:0.00018249821180568482\n",
      "train loss:0.00024754004681886067\n",
      "train loss:0.00018129586786666172\n",
      "train loss:0.00020721523395024138\n",
      "train loss:0.00045166975796402426\n",
      "train loss:0.00033296468351673653\n",
      "train loss:0.00023584069062177073\n",
      "train loss:0.0002697677447711011\n",
      "train loss:0.0002855232880620845\n",
      "train loss:0.0002489340782716623\n",
      "train loss:0.00011577662676104013\n",
      "train loss:0.00012275427064397382\n",
      "train loss:0.000425476174648564\n",
      "train loss:0.0002466276945406576\n",
      "train loss:0.00013629909739095486\n",
      "train loss:0.00016705843375939796\n",
      "train loss:0.00037320820626113126\n",
      "train loss:9.411282730728709e-05\n",
      "train loss:0.0001030793143069671\n",
      "train loss:0.00021304974125468904\n",
      "train loss:0.0004602086978714463\n",
      "train loss:0.00037400449654482695\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9995\n"
     ]
    }
   ],
   "source": [
    "x_train=x_train_5\n",
    "x_test = x_test_5\n",
    "t_train=t_train_5\n",
    "t_test = t_test_5\n",
    "\n",
    "trainer_SGD = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='SGD', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_MMT = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Momentum', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_SGD.train()\n",
    "trainer_MMT.train()\n",
    "\n",
    "MMT_acc.append(trainer_MMT.test_acc_list[-1])\n",
    "SGD_acc.append(trainer_SGD.test_acc_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00033845771638510436\n",
      "=== epoch:1, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00010787364336635376\n",
      "train loss:0.00021763279545613485\n",
      "train loss:0.0003233013094770826\n",
      "train loss:0.0005894550245377834\n",
      "train loss:0.00028501474390179\n",
      "train loss:0.00014084987315223562\n",
      "train loss:0.00041617478912638924\n",
      "train loss:0.000249346841514048\n",
      "train loss:0.00036298319503112416\n",
      "train loss:0.0002015992170231731\n",
      "train loss:0.00020482063279430863\n",
      "train loss:9.549901532123664e-05\n",
      "train loss:5.0972446318914355e-05\n",
      "train loss:9.947742590582112e-05\n",
      "train loss:0.00019167556154751314\n",
      "train loss:0.000320532861992187\n",
      "train loss:0.00010669164540308351\n",
      "train loss:0.00018861107886659887\n",
      "train loss:0.00016675797777080764\n",
      "train loss:0.00010086255118844266\n",
      "train loss:0.00010213779972965122\n",
      "train loss:0.00020104506966612124\n",
      "train loss:0.00027949092639001567\n",
      "train loss:0.00047397448408453716\n",
      "train loss:0.001671912787249384\n",
      "train loss:2.633553602055739e-05\n",
      "train loss:0.0001362255070765568\n",
      "train loss:0.00028714806754503216\n",
      "train loss:0.00018952553079387116\n",
      "train loss:0.00022678301351598644\n",
      "train loss:0.00011834156828601084\n",
      "train loss:7.64955856158728e-05\n",
      "train loss:0.0001999526219535708\n",
      "train loss:0.00020262616641690412\n",
      "train loss:0.0023586318648826077\n",
      "train loss:0.00037730858598667224\n",
      "train loss:0.0002707759456734871\n",
      "train loss:0.00016370474068728668\n",
      "train loss:0.0006311947984454423\n",
      "train loss:0.00011472744390664487\n",
      "train loss:0.0003103395369001416\n",
      "train loss:0.00024575469982692275\n",
      "train loss:0.00015923296929343307\n",
      "train loss:0.0005378674548792712\n",
      "train loss:0.00043865163006813625\n",
      "train loss:0.00011553349880335982\n",
      "train loss:0.0001976865655150097\n",
      "train loss:0.0004077619971296882\n",
      "train loss:0.00015819561639615487\n",
      "train loss:0.0001782744611983083\n",
      "train loss:0.000250022161445532\n",
      "train loss:0.0005223737704064378\n",
      "train loss:0.0016665551282281617\n",
      "train loss:0.00029303745552015757\n",
      "train loss:0.00012160925610895042\n",
      "train loss:4.730783357391546e-05\n",
      "train loss:0.0002985451285508013\n",
      "train loss:0.00011912645935554817\n",
      "train loss:0.00022900563510860482\n",
      "train loss:0.0010515966184389365\n",
      "train loss:0.0002856602988794796\n",
      "train loss:0.00038478282560893513\n",
      "train loss:0.00013856672344791284\n",
      "train loss:0.00019854632672231377\n",
      "train loss:0.00018517453548228658\n",
      "train loss:0.0004404226508003547\n",
      "train loss:0.00025146542255267257\n",
      "train loss:0.00041109913813185627\n",
      "train loss:0.00022437272120132856\n",
      "train loss:0.00010931097881861344\n",
      "train loss:6.737548845802647e-05\n",
      "train loss:0.0002002436369425798\n",
      "train loss:0.00024230597144372134\n",
      "train loss:0.00013111799129856983\n",
      "train loss:0.00029672369892988146\n",
      "train loss:0.00030712143237396567\n",
      "train loss:0.0001092817527478757\n",
      "train loss:0.00035294282947625767\n",
      "train loss:0.0003139089568459328\n",
      "train loss:0.00019473440295927346\n",
      "=== epoch:2, train acc:1.0, test acc:1.0 ===\n",
      "train loss:7.987897929038271e-05\n",
      "train loss:0.00035166330474096987\n",
      "train loss:0.00020084359828522822\n",
      "train loss:0.00014872058241317\n",
      "train loss:0.0007595915616056614\n",
      "train loss:0.00023714299638569344\n",
      "train loss:0.00028631122981044783\n",
      "train loss:0.00011081447276255497\n",
      "train loss:0.00010091744699744313\n",
      "train loss:0.00027390804330472673\n",
      "train loss:8.590990131759171e-05\n",
      "train loss:0.00020133857408189442\n",
      "train loss:0.00026366663399129064\n",
      "train loss:4.556257791508816e-05\n",
      "train loss:0.0006915118785493507\n",
      "train loss:0.00030128687199355365\n",
      "train loss:0.0002324020280295237\n",
      "train loss:0.0004203241052807173\n",
      "train loss:0.0001724982463637841\n",
      "train loss:0.0004804672508993868\n",
      "train loss:0.00016333800676244774\n",
      "train loss:0.00016047377328932077\n",
      "train loss:0.0006874819439012707\n",
      "train loss:0.00010799584187501376\n",
      "train loss:9.746611023584135e-05\n",
      "train loss:7.506435532168482e-05\n",
      "train loss:0.0002834580697109415\n",
      "train loss:0.0003303669224513568\n",
      "train loss:0.0005581144923924897\n",
      "train loss:0.00031730810126779583\n",
      "train loss:0.00020368559455427156\n",
      "train loss:0.00015734189921099256\n",
      "train loss:0.0003266818485723174\n",
      "train loss:0.00020928030370615712\n",
      "train loss:0.00010347203502045212\n",
      "train loss:0.0013780179510383967\n",
      "train loss:0.00013450301474236055\n",
      "train loss:0.0004280208822532545\n",
      "train loss:0.00015788295120984383\n",
      "train loss:7.120826190706927e-05\n",
      "train loss:0.0002658943525716605\n",
      "train loss:0.0007359003973009813\n",
      "train loss:0.00015755304601519277\n",
      "train loss:0.00019090581466899126\n",
      "train loss:6.798930075547852e-05\n",
      "train loss:0.00016549301081726032\n",
      "train loss:0.00047079756231621796\n",
      "train loss:0.0001816215687411217\n",
      "train loss:0.0002468539180140336\n",
      "train loss:0.00025066845741950603\n",
      "train loss:0.00019006834835800856\n",
      "train loss:0.00030831787984948495\n",
      "train loss:0.00031759782704064514\n",
      "train loss:0.00041291267407795847\n",
      "train loss:0.00030365382404698454\n",
      "train loss:0.00016642328228926931\n",
      "train loss:0.0003897350833594617\n",
      "train loss:0.0003545052952636158\n",
      "train loss:0.00020236011991103594\n",
      "train loss:0.0003716725756100061\n",
      "train loss:9.313410538697857e-05\n",
      "train loss:0.00029800638179374453\n",
      "train loss:0.00016620069310857184\n",
      "train loss:0.00033655363899714606\n",
      "train loss:0.00020023885654201976\n",
      "train loss:0.00032557718428365807\n",
      "train loss:0.00033511346925089314\n",
      "train loss:0.0009621151154871203\n",
      "train loss:0.0001090105697208318\n",
      "train loss:9.129230911976332e-05\n",
      "train loss:0.00013480671093139344\n",
      "train loss:0.00016882748485376803\n",
      "train loss:0.00012465063269555772\n",
      "train loss:0.00029656109456223964\n",
      "train loss:0.00035812356258439483\n",
      "train loss:0.00043738043632539516\n",
      "train loss:0.0012701402912290317\n",
      "train loss:0.0004741549507917795\n",
      "train loss:0.0007464464214910256\n",
      "train loss:0.00017932485006517303\n",
      "=== epoch:3, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00044825399896378877\n",
      "train loss:0.00013437925531323112\n",
      "train loss:0.00024585217610743773\n",
      "train loss:2.570238407549017e-05\n",
      "train loss:0.00016057485326934556\n",
      "train loss:0.00014606275620594538\n",
      "train loss:0.00023155746088254954\n",
      "train loss:0.00010384122922103369\n",
      "train loss:0.00016540118812696626\n",
      "train loss:0.0003698210772376115\n",
      "train loss:0.00037275034530649643\n",
      "train loss:0.00037848653310855316\n",
      "train loss:0.00013055862618385777\n",
      "train loss:0.00045578014077168443\n",
      "train loss:0.0005506948783858053\n",
      "train loss:0.0003351894669744639\n",
      "train loss:0.00014444004621099322\n",
      "train loss:8.329660891528867e-05\n",
      "train loss:0.0005243593541199112\n",
      "train loss:0.0005918924757259906\n",
      "train loss:9.823447006602663e-05\n",
      "train loss:0.00020119814537766356\n",
      "train loss:0.00015540401246773254\n",
      "train loss:0.0003119348399008983\n",
      "train loss:0.00022689585369342312\n",
      "train loss:0.0002628633062522834\n",
      "train loss:0.00019267375595680174\n",
      "train loss:0.00018267665546192017\n",
      "train loss:0.0003736705704181105\n",
      "train loss:5.5590930935197346e-05\n",
      "train loss:0.0003032816656019635\n",
      "train loss:0.00016984250360396893\n",
      "train loss:0.0001745340423472624\n",
      "train loss:0.00013210037631709027\n",
      "train loss:0.00026052971407111497\n",
      "train loss:0.00025108110584769807\n",
      "train loss:0.0001589872982201857\n",
      "train loss:0.0003400285945897502\n",
      "train loss:0.0003565532880555676\n",
      "train loss:0.0012088901721679305\n",
      "train loss:6.733252315589976e-05\n",
      "train loss:0.0005808547135298022\n",
      "train loss:0.000993759013007376\n",
      "train loss:0.0002853785667446337\n",
      "train loss:0.0004432566506639686\n",
      "train loss:0.000282641356137614\n",
      "train loss:0.00033988789209113516\n",
      "train loss:0.00010958405475841067\n",
      "train loss:0.00019691984501540982\n",
      "train loss:0.0001909926415163227\n",
      "train loss:0.00017823419682786942\n",
      "train loss:0.0004568931708037192\n",
      "train loss:0.00020175532011096922\n",
      "train loss:0.0005688118582947049\n",
      "train loss:0.00032737082563519184\n",
      "train loss:0.000419145674896556\n",
      "train loss:0.0002910206043398352\n",
      "train loss:0.0006285822941288273\n",
      "train loss:0.0005015865663984937\n",
      "train loss:0.00023916959803446075\n",
      "train loss:0.0004498408979606285\n",
      "train loss:0.00030581482670449214\n",
      "train loss:0.00028843025795291294\n",
      "train loss:6.849459465036883e-05\n",
      "train loss:0.0003072470832879733\n",
      "train loss:0.00016769183663825225\n",
      "train loss:0.0002797684948605964\n",
      "train loss:0.0004835850959339997\n",
      "train loss:6.416214396169894e-05\n",
      "train loss:0.00015231047362646626\n",
      "train loss:0.00014543066232874995\n",
      "train loss:0.0003418381177897793\n",
      "train loss:0.0003776723118438755\n",
      "train loss:0.0010959399022329134\n",
      "train loss:0.00038697921734339965\n",
      "train loss:0.0005295784320353152\n",
      "train loss:0.0002240739714609568\n",
      "train loss:0.00020397569290631805\n",
      "train loss:0.0006522057831658309\n",
      "train loss:0.000325634516952027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:4, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00017969767224526703\n",
      "train loss:0.0011409427230096444\n",
      "train loss:0.0011194059557159903\n",
      "train loss:0.0001968620403959724\n",
      "train loss:0.00088228141532935\n",
      "train loss:0.00010437973878368454\n",
      "train loss:0.00021668737949661455\n",
      "train loss:0.0002048206157561399\n",
      "train loss:0.00028164141175873893\n",
      "train loss:0.0025831578060436533\n",
      "train loss:0.0013506194294262133\n",
      "train loss:0.00031123502549788927\n",
      "train loss:9.275706363255661e-05\n",
      "train loss:0.00014072241289558895\n",
      "train loss:0.0004390677277088556\n",
      "train loss:0.00028291982269937734\n",
      "train loss:0.00020623701898987785\n",
      "train loss:0.0003627121102734237\n",
      "train loss:0.00029217952802335464\n",
      "train loss:0.00023915916812987668\n",
      "train loss:9.482804221715992e-05\n",
      "train loss:0.0001480618234473291\n",
      "train loss:0.0003000221432929173\n",
      "train loss:0.000573317302023374\n",
      "train loss:3.119413708374599e-05\n",
      "train loss:0.0002633950166971729\n",
      "train loss:0.00018853726046397047\n",
      "train loss:0.00012476212014204336\n",
      "train loss:0.00023089841721970347\n",
      "train loss:0.00048137458701773327\n",
      "train loss:0.0001319384297693341\n",
      "train loss:0.000331709742445126\n",
      "train loss:0.0006168177487923918\n",
      "train loss:0.00017292964186806846\n",
      "train loss:0.0002669738386048077\n",
      "train loss:0.0004056807597041874\n",
      "train loss:0.00016249573205307923\n",
      "train loss:0.00030610759630729354\n",
      "train loss:0.0003524026494693388\n",
      "train loss:0.00017840007931060864\n",
      "train loss:0.00021920120018264092\n",
      "train loss:0.0005539858855552125\n",
      "train loss:0.000247417500613276\n",
      "train loss:0.0002467988832197278\n",
      "train loss:0.0005044799585068019\n",
      "train loss:0.00021190251932955622\n",
      "train loss:0.0007923842444891284\n",
      "train loss:8.645053423733578e-05\n",
      "train loss:0.00011989807284536645\n",
      "train loss:0.00044989233622030536\n",
      "train loss:6.584838699288985e-05\n",
      "train loss:0.00038326900503054694\n",
      "train loss:0.00025422071536765047\n",
      "train loss:2.7202006621183258e-05\n",
      "train loss:0.000231716514840816\n",
      "train loss:9.347602135980524e-05\n",
      "train loss:0.00023836008734559488\n",
      "train loss:0.00018503518943332\n",
      "train loss:0.00028378116261615696\n",
      "train loss:0.0005245459322589601\n",
      "train loss:0.0002365966017441738\n",
      "train loss:0.0007542428408503885\n",
      "train loss:0.00032177883781912153\n",
      "train loss:0.0002785021138736855\n",
      "train loss:8.303018012304207e-05\n",
      "train loss:0.00043849677017540335\n",
      "train loss:0.00013320562132586712\n",
      "train loss:0.00010483080650973746\n",
      "train loss:0.00020092637860843695\n",
      "train loss:0.0003223089640226267\n",
      "train loss:0.0005037521350405067\n",
      "train loss:0.00022826529036519213\n",
      "train loss:0.000330998462107822\n",
      "train loss:0.00010505676829569921\n",
      "train loss:0.0003235732885836125\n",
      "train loss:0.0004738610087677491\n",
      "train loss:0.00019323872380756516\n",
      "train loss:0.00023706086302314366\n",
      "train loss:0.0003048918290593319\n",
      "train loss:0.0001679649744125598\n",
      "=== epoch:5, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0003006756462083749\n",
      "train loss:0.00016987122960938287\n",
      "train loss:0.0001994145551362386\n",
      "train loss:9.373451115216327e-05\n",
      "train loss:0.0001625575619200878\n",
      "train loss:0.00023343699483533255\n",
      "train loss:0.0002651850992842731\n",
      "train loss:0.0001140366603225668\n",
      "train loss:0.0005321830464357875\n",
      "train loss:0.00016639366642965775\n",
      "train loss:0.00020968192612852226\n",
      "train loss:0.00022058699171342857\n",
      "train loss:0.00013476795799924029\n",
      "train loss:0.00033761951561329245\n",
      "train loss:0.0005123702983649101\n",
      "train loss:0.00014663344577887347\n",
      "train loss:0.0004588115293810062\n",
      "train loss:0.000293601536258206\n",
      "train loss:0.0005555132474543923\n",
      "train loss:0.00012919802883907793\n",
      "train loss:0.0002968513019926236\n",
      "train loss:0.000678445019914875\n",
      "train loss:0.00020903770282413976\n",
      "train loss:0.00015684243441624487\n",
      "train loss:0.00012031525983314654\n",
      "train loss:0.0001795404026230502\n",
      "train loss:0.000633104874597331\n",
      "train loss:0.0002505142201892315\n",
      "train loss:5.657041380739875e-05\n",
      "train loss:0.00047000678461393873\n",
      "train loss:0.00029958735353708327\n",
      "train loss:0.0003565037878076255\n",
      "train loss:0.00013648437054107206\n",
      "train loss:0.0002751644954881007\n",
      "train loss:0.0005607428629878297\n",
      "train loss:0.00040821217292272575\n",
      "train loss:0.0005049148706813537\n",
      "train loss:0.0001973389182586952\n",
      "train loss:0.00030074566481235685\n",
      "train loss:0.00035289944832153663\n",
      "train loss:0.00043038950628376116\n",
      "train loss:0.00023293127166056817\n",
      "train loss:0.00015430959842072903\n",
      "train loss:0.000227908888626936\n",
      "train loss:0.00040422837052628226\n",
      "train loss:0.00020651400019733874\n",
      "train loss:0.000642202283151705\n",
      "train loss:0.0001625505856575448\n",
      "train loss:0.00021329789212436574\n",
      "train loss:0.00041030926529001297\n",
      "train loss:0.00012749207216745413\n",
      "train loss:0.00017912646870270056\n",
      "train loss:0.0006461051592068977\n",
      "train loss:9.357233878113416e-05\n",
      "train loss:0.0002668516971641259\n",
      "train loss:0.0001856137832335651\n",
      "train loss:0.00012686629519001504\n",
      "train loss:0.00018331970857061647\n",
      "train loss:0.0001458602534099992\n",
      "train loss:0.00014692936217416483\n",
      "train loss:0.00018220951311935044\n",
      "train loss:0.00020804468978643782\n",
      "train loss:0.00039673616326976936\n",
      "train loss:0.00019725294344177447\n",
      "train loss:0.0003323772565112274\n",
      "train loss:0.0002922569051963502\n",
      "train loss:4.909600494010911e-05\n",
      "train loss:0.00022773074500059286\n",
      "train loss:0.000274663869220903\n",
      "train loss:0.0004834933208044406\n",
      "train loss:0.0001869281374402595\n",
      "train loss:0.0003914614540564277\n",
      "train loss:0.0009711855704722893\n",
      "train loss:0.00025321679035362674\n",
      "train loss:9.531636984277483e-05\n",
      "train loss:0.0003508760937423857\n",
      "train loss:0.00016039536878432355\n",
      "train loss:0.00032775564160044784\n",
      "train loss:0.00017888011099076342\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9995\n",
      "train loss:0.0003072030902847047\n",
      "=== epoch:1, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00031414144485757687\n",
      "train loss:0.000188530403701707\n",
      "train loss:0.00014474494733251458\n",
      "train loss:0.0003540437857665111\n",
      "train loss:0.00015172102086314696\n",
      "train loss:0.0003461575122802046\n",
      "train loss:0.0002207459286564219\n",
      "train loss:0.0002544283545285446\n",
      "train loss:9.994776826051636e-05\n",
      "train loss:0.0002817597988390615\n",
      "train loss:0.00020446841323110734\n",
      "train loss:0.00020138632716547238\n",
      "train loss:0.0003257388441459018\n",
      "train loss:0.0003625929639959848\n",
      "train loss:0.0002535653362753692\n",
      "train loss:0.0005461283959780653\n",
      "train loss:0.00010179046015847079\n",
      "train loss:0.00014768010790226497\n",
      "train loss:0.00018291431005699508\n",
      "train loss:0.00013840020975447678\n",
      "train loss:0.0003149193049635679\n",
      "train loss:0.0002959887961166126\n",
      "train loss:0.00011180808384895813\n",
      "train loss:0.00011842460663105296\n",
      "train loss:0.0002473961543623826\n",
      "train loss:0.00012523900456556068\n",
      "train loss:0.00026964376880895684\n",
      "train loss:0.0003256865786466173\n",
      "train loss:0.0004417346736157185\n",
      "train loss:0.0010080632782754604\n",
      "train loss:0.0008633425615959037\n",
      "train loss:9.033571043921785e-05\n",
      "train loss:0.00028149352867540213\n",
      "train loss:0.0003150070217355339\n",
      "train loss:0.0005113139719086419\n",
      "train loss:0.00016515991103299092\n",
      "train loss:0.0001461954552078543\n",
      "train loss:0.00015016344943208295\n",
      "train loss:0.0004889320555465965\n",
      "train loss:2.123814575290096e-05\n",
      "train loss:0.0002742173708929739\n",
      "train loss:7.790921408596772e-05\n",
      "train loss:0.0012398023957847276\n",
      "train loss:0.0006459723019228757\n",
      "train loss:0.00032747588267450166\n",
      "train loss:0.00018570018192044514\n",
      "train loss:0.00017622937466185896\n",
      "train loss:0.000339847457635627\n",
      "train loss:0.0004625994203156889\n",
      "train loss:0.00022181792677551185\n",
      "train loss:8.920779533066903e-05\n",
      "train loss:8.491676779319501e-05\n",
      "train loss:0.0005244670545936283\n",
      "train loss:0.00012238839536674888\n",
      "train loss:0.00025276218573065597\n",
      "train loss:0.0002068977474220971\n",
      "train loss:7.282330525148699e-05\n",
      "train loss:0.00034831177949591787\n",
      "train loss:0.00021587272482701095\n",
      "train loss:0.0012632233112719862\n",
      "train loss:0.0005072697557147364\n",
      "train loss:0.00023581710580527403\n",
      "train loss:0.00014289535592935967\n",
      "train loss:0.0002994381304651066\n",
      "train loss:0.0002797109727800239\n",
      "train loss:5.7542923589771596e-05\n",
      "train loss:7.785126351230392e-05\n",
      "train loss:0.0001500393073653102\n",
      "train loss:9.349401576092532e-05\n",
      "train loss:0.0003117949200807197\n",
      "train loss:0.00014142964289367462\n",
      "train loss:0.00042068830910222906\n",
      "train loss:0.00034138590309045424\n",
      "train loss:0.00016070640303435235\n",
      "train loss:0.00026406711779831385\n",
      "train loss:0.00011274159292144627\n",
      "train loss:0.000210066988019882\n",
      "train loss:0.0001331166846367378\n",
      "train loss:0.00025156813476457664\n",
      "train loss:0.00023804176950547324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:2, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0006583597449971534\n",
      "train loss:0.00030221199819846376\n",
      "train loss:0.0003689879156928206\n",
      "train loss:0.00019687631693940527\n",
      "train loss:0.00043143877919175173\n",
      "train loss:0.0003975671355943021\n",
      "train loss:0.00010445704633197155\n",
      "train loss:0.00022496583570002878\n",
      "train loss:0.00021493368594548493\n",
      "train loss:0.0001988109675200868\n",
      "train loss:0.0006634536071237673\n",
      "train loss:0.00020665542519123346\n",
      "train loss:0.0005753888861235448\n",
      "train loss:0.0002730204027808725\n",
      "train loss:0.0003154580368965231\n",
      "train loss:0.0002674460743632201\n",
      "train loss:7.334316007626463e-05\n",
      "train loss:0.0003052111075028369\n",
      "train loss:5.933478905354029e-05\n",
      "train loss:0.00015013250515952693\n",
      "train loss:0.0004864321703751463\n",
      "train loss:0.00015130947180152913\n",
      "train loss:0.00041037896484615537\n",
      "train loss:0.0003719873876614169\n",
      "train loss:0.0002212431784761402\n",
      "train loss:0.0003209953282143009\n",
      "train loss:0.0003783156616583049\n",
      "train loss:0.00015174575510383087\n",
      "train loss:0.00012654413546837808\n",
      "train loss:0.0003567213270878688\n",
      "train loss:0.00020985705288060927\n",
      "train loss:0.00023827282865360538\n",
      "train loss:0.0007322686108259773\n",
      "train loss:0.0005336385712026133\n",
      "train loss:0.0004236739527404225\n",
      "train loss:0.0003284100757477568\n",
      "train loss:0.00040021733210146885\n",
      "train loss:0.0001463673083728356\n",
      "train loss:0.00040541252739502464\n",
      "train loss:0.00020328515616835837\n",
      "train loss:0.0003896165315180026\n",
      "train loss:0.00013850966256101743\n",
      "train loss:0.00014412720089048008\n",
      "train loss:0.0006184554575542304\n",
      "train loss:0.00041527413521283597\n",
      "train loss:0.00019768843920598274\n",
      "train loss:0.0002986066869210054\n",
      "train loss:0.00010743525261830936\n",
      "train loss:0.0003586706762124572\n",
      "train loss:0.000223232535703548\n",
      "train loss:0.00013492753168209992\n",
      "train loss:0.0002957723039972334\n",
      "train loss:0.00024628451908286224\n",
      "train loss:0.0001312197608412094\n",
      "train loss:0.00020999394917266867\n",
      "train loss:0.00029072229194743773\n",
      "train loss:0.00014121652955800722\n",
      "train loss:0.00022785240776392762\n",
      "train loss:0.00028487044394048046\n",
      "train loss:0.00029769434781493526\n",
      "train loss:0.00014088551733697249\n",
      "train loss:0.00024651386700669497\n",
      "train loss:0.0001459782828121654\n",
      "train loss:0.0001462224850221057\n",
      "train loss:4.5436945889978204e-05\n",
      "train loss:0.0002352609419791791\n",
      "train loss:7.406548070204406e-05\n",
      "train loss:0.0002733838552061316\n",
      "train loss:0.0007038594997136185\n",
      "train loss:5.373695923644362e-05\n",
      "train loss:0.00024893168097842944\n",
      "train loss:0.0001084171831679814\n",
      "train loss:0.00042998670079892013\n",
      "train loss:0.0002792631639126282\n",
      "train loss:0.00038788862541125314\n",
      "train loss:0.000316765552169173\n",
      "train loss:0.0003453866408176228\n",
      "train loss:0.0007358280318949996\n",
      "train loss:0.00014785374703291373\n",
      "train loss:0.00020045941438339476\n",
      "=== epoch:3, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00014886745965632633\n",
      "train loss:0.0002657982421336925\n",
      "train loss:0.0003290264681971736\n",
      "train loss:0.00020368700870458954\n",
      "train loss:0.0003101966340260428\n",
      "train loss:0.00019890588221921026\n",
      "train loss:0.00016371352845139517\n",
      "train loss:0.0001125918876502803\n",
      "train loss:0.0007061517682328055\n",
      "train loss:0.000139630215199304\n",
      "train loss:0.0001825427724874839\n",
      "train loss:7.764830778888886e-05\n",
      "train loss:0.0005349149241594296\n",
      "train loss:7.8298521419137e-05\n",
      "train loss:0.00013045157011083418\n",
      "train loss:0.0002662674608501569\n",
      "train loss:0.0002868106594667768\n",
      "train loss:0.00019531355746850717\n",
      "train loss:0.0002965862162380207\n",
      "train loss:0.00025993834203452747\n",
      "train loss:7.923653835990536e-05\n",
      "train loss:0.00020047751803298596\n",
      "train loss:0.00010237480459517763\n",
      "train loss:0.00029912775489033806\n",
      "train loss:0.0002882840632081725\n",
      "train loss:0.0002803659529596125\n",
      "train loss:0.000573086411317133\n",
      "train loss:0.00011440994395460572\n",
      "train loss:0.0003128053754756197\n",
      "train loss:0.0002813077493451643\n",
      "train loss:0.00025987830973142715\n",
      "train loss:0.00017094487838911063\n",
      "train loss:0.00024120714008094512\n",
      "train loss:0.00025603868274474967\n",
      "train loss:0.0005371808924805253\n",
      "train loss:7.863479076441229e-05\n",
      "train loss:0.00013126896858671182\n",
      "train loss:0.00041393305062806604\n",
      "train loss:6.787653137445027e-05\n",
      "train loss:0.00044248102483642073\n",
      "train loss:0.0002361924982004338\n",
      "train loss:0.00018096573995437894\n",
      "train loss:0.00016743050892619623\n",
      "train loss:0.00038480207528185785\n",
      "train loss:0.001340869803902188\n",
      "train loss:0.0002653122039109178\n",
      "train loss:0.00010813002100460455\n",
      "train loss:0.00013957955310894759\n",
      "train loss:0.00027993450150563213\n",
      "train loss:0.0004118246289004564\n",
      "train loss:0.00018247315751654936\n",
      "train loss:0.00024154290599372964\n",
      "train loss:0.0001180294398508983\n",
      "train loss:0.0001663766454196462\n",
      "train loss:0.0001410043965769219\n",
      "train loss:0.00013987553478383706\n",
      "train loss:0.0004750066633973961\n",
      "train loss:0.00030533636215340337\n",
      "train loss:0.00014233645480816329\n",
      "train loss:0.00016782334707356024\n",
      "train loss:0.00014079081290531954\n",
      "train loss:0.0015263413340457616\n",
      "train loss:0.00020975025014655934\n",
      "train loss:0.00039509832099950283\n",
      "train loss:0.00011192323975946917\n",
      "train loss:0.00017761839584984562\n",
      "train loss:0.00014419474496050254\n",
      "train loss:0.0005657223449157885\n",
      "train loss:0.00022602335467920617\n",
      "train loss:0.00015901086098674702\n",
      "train loss:0.00018170346375804013\n",
      "train loss:7.086063887544252e-05\n",
      "train loss:0.00024074493095864994\n",
      "train loss:0.00019372349624649496\n",
      "train loss:0.00018902721393920417\n",
      "train loss:0.0002646698663136365\n",
      "train loss:0.00013237877085831487\n",
      "train loss:0.0003800559273010162\n",
      "train loss:0.00036773494067484454\n",
      "train loss:0.00017063947414243605\n",
      "=== epoch:4, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0001370655920656957\n",
      "train loss:0.00044333392142436744\n",
      "train loss:0.00021837859035676825\n",
      "train loss:9.390237151324031e-05\n",
      "train loss:0.00013503962006730397\n",
      "train loss:0.0002233932448727214\n",
      "train loss:0.0002981944583448801\n",
      "train loss:0.00032802290081272267\n",
      "train loss:0.00021467829664700692\n",
      "train loss:0.00026213647883810117\n",
      "train loss:0.00041133964779723994\n",
      "train loss:9.183118928070009e-05\n",
      "train loss:0.00017301766990458355\n",
      "train loss:0.0003446775416213998\n",
      "train loss:0.0002175536540032583\n",
      "train loss:0.0007501124383282362\n",
      "train loss:0.00017428481303825093\n",
      "train loss:0.00017536921640351466\n",
      "train loss:0.0003923624503799506\n",
      "train loss:0.0001573489942936853\n",
      "train loss:0.0002183852990425039\n",
      "train loss:0.0002893940478878782\n",
      "train loss:0.00012824459826405666\n",
      "train loss:7.750084271314826e-05\n",
      "train loss:4.49578210343335e-05\n",
      "train loss:0.0002167086676385477\n",
      "train loss:0.0001147901474889923\n",
      "train loss:0.00040380048304279007\n",
      "train loss:0.00015879019948388779\n",
      "train loss:8.179159203845142e-05\n",
      "train loss:0.0002716360374038243\n",
      "train loss:0.00037007164754494776\n",
      "train loss:8.702527474986156e-05\n",
      "train loss:0.0001249718365859612\n",
      "train loss:0.00015324348555137464\n",
      "train loss:8.60402298760165e-05\n",
      "train loss:0.00019442421085039217\n",
      "train loss:0.0002598491993685579\n",
      "train loss:0.00037107425946679944\n",
      "train loss:6.449776133462619e-05\n",
      "train loss:8.423754362148161e-05\n",
      "train loss:0.00013926861137575865\n",
      "train loss:0.00017885140050908965\n",
      "train loss:0.00033140403444374315\n",
      "train loss:0.0005339068228666479\n",
      "train loss:0.00016155499529135848\n",
      "train loss:0.00022111560681775108\n",
      "train loss:0.0002547520053145085\n",
      "train loss:0.00013083605985748958\n",
      "train loss:3.589975810030837e-05\n",
      "train loss:9.80766353306133e-05\n",
      "train loss:0.0002923606002869421\n",
      "train loss:0.0005572040508460191\n",
      "train loss:0.00010396132874697404\n",
      "train loss:0.00012069535346445343\n",
      "train loss:9.9911300937716e-05\n",
      "train loss:0.0003638425238896402\n",
      "train loss:9.285049881235921e-05\n",
      "train loss:0.002284173933443382\n",
      "train loss:0.0001505269753924612\n",
      "train loss:0.00031415682113945803\n",
      "train loss:0.00035525774947062715\n",
      "train loss:6.960798826049918e-05\n",
      "train loss:0.00013311706076431579\n",
      "train loss:0.0001211139859723102\n",
      "train loss:0.0002796642095604773\n",
      "train loss:0.00018366824296862786\n",
      "train loss:0.0003243224757027804\n",
      "train loss:3.158141803009273e-05\n",
      "train loss:0.00016098347715696974\n",
      "train loss:0.00014204031235965023\n",
      "train loss:0.000439249778030444\n",
      "train loss:0.00022663013461112446\n",
      "train loss:0.0002557121611625973\n",
      "train loss:0.0001393157872669938\n",
      "train loss:0.00013285931658663954\n",
      "train loss:0.00043720192854101157\n",
      "train loss:0.00020206148723639773\n",
      "train loss:0.0002737849392636086\n",
      "train loss:0.00034856969043425135\n",
      "=== epoch:5, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00027782710495661395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0005685596626545008\n",
      "train loss:0.00025747093029276203\n",
      "train loss:0.00037347470535121775\n",
      "train loss:0.00021188909346239005\n",
      "train loss:0.00022967899492786764\n",
      "train loss:0.00030853563881827505\n",
      "train loss:0.000286668515534262\n",
      "train loss:0.00045084415386539075\n",
      "train loss:8.513024858323414e-05\n",
      "train loss:7.987226842666848e-05\n",
      "train loss:9.515689005251938e-05\n",
      "train loss:4.9551417695769696e-05\n",
      "train loss:0.00012194308969087798\n",
      "train loss:0.00010667259950479098\n",
      "train loss:0.00018577967727257373\n",
      "train loss:0.00014675724091658986\n",
      "train loss:0.00040169610411882337\n",
      "train loss:0.0001809915757520218\n",
      "train loss:0.0001764613848179531\n",
      "train loss:0.00022339937930927485\n",
      "train loss:0.00033000603500978493\n",
      "train loss:0.0003054639280267484\n",
      "train loss:0.00021375938711734314\n",
      "train loss:0.00012031382262879741\n",
      "train loss:0.00023839614672623667\n",
      "train loss:0.00027277929996901313\n",
      "train loss:0.0002821938624512807\n",
      "train loss:7.309312074289588e-05\n",
      "train loss:0.00023586177373467211\n",
      "train loss:9.48122092775578e-05\n",
      "train loss:0.00010347497041004067\n",
      "train loss:0.00010524341680112859\n",
      "train loss:0.0001261089637714906\n",
      "train loss:0.00027645631298331674\n",
      "train loss:8.319767373621308e-05\n",
      "train loss:8.53880088355507e-05\n",
      "train loss:0.00031581742200578104\n",
      "train loss:0.00031081958441118816\n",
      "train loss:0.00018988394125457687\n",
      "train loss:0.0004660687624844704\n",
      "train loss:0.00024389819288668873\n",
      "train loss:0.00010047947210770437\n",
      "train loss:0.00014434639250076952\n",
      "train loss:0.00010440210934050554\n",
      "train loss:8.041003625438304e-05\n",
      "train loss:0.0004268141872942781\n",
      "train loss:0.0001604392806873157\n",
      "train loss:0.0002527325711135674\n",
      "train loss:0.00044565279604711835\n",
      "train loss:0.00022557365741098762\n",
      "train loss:0.00039882689296029676\n",
      "train loss:9.528735457401914e-05\n",
      "train loss:0.0001861684418925231\n",
      "train loss:0.0001463075160087874\n",
      "train loss:0.0002041551959437273\n",
      "train loss:0.00011790638735531836\n",
      "train loss:0.0002505220381614896\n",
      "train loss:0.00010541470694313552\n",
      "train loss:0.00031466427874218876\n",
      "train loss:0.000490724232661701\n",
      "train loss:0.00015185780599659567\n",
      "train loss:0.00015767716333514616\n",
      "train loss:0.0002318283862293901\n",
      "train loss:0.00021102330556570692\n",
      "train loss:9.521918870978398e-05\n",
      "train loss:0.0004487756276938663\n",
      "train loss:0.000251857155152976\n",
      "train loss:0.000417774129412469\n",
      "train loss:0.0008187522612442054\n",
      "train loss:7.561686313655953e-05\n",
      "train loss:0.00047212817852766633\n",
      "train loss:0.00023823743512086843\n",
      "train loss:0.0001598024564300834\n",
      "train loss:0.0002472868495199295\n",
      "train loss:0.0002209489700634229\n",
      "train loss:0.00044515934670556333\n",
      "train loss:7.635061243637796e-05\n",
      "train loss:0.0006549118404306454\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:1.0\n"
     ]
    }
   ],
   "source": [
    "x_train=x_train_6\n",
    "x_test = x_test_6\n",
    "t_train=t_train_6\n",
    "t_test = t_test_6\n",
    "\n",
    "trainer_SGD = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='SGD', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_MMT = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Momentum', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_SGD.train()\n",
    "trainer_MMT.train()\n",
    "\n",
    "MMT_acc.append(trainer_MMT.test_acc_list[-1])\n",
    "SGD_acc.append(trainer_SGD.test_acc_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:4.326762482734885e-05\n",
      "=== epoch:1, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00012232827102315072\n",
      "train loss:8.551616950889622e-05\n",
      "train loss:0.0001653110658540962\n",
      "train loss:0.00018900467380087666\n",
      "train loss:0.00023470855735368477\n",
      "train loss:0.0005063469748363941\n",
      "train loss:8.409532515995494e-05\n",
      "train loss:0.0013790675416873485\n",
      "train loss:0.00010930375395281799\n",
      "train loss:0.0005268391760139263\n",
      "train loss:0.00029150342523776403\n",
      "train loss:0.00018729448796294272\n",
      "train loss:0.00011085782974170205\n",
      "train loss:0.00031212041084387036\n",
      "train loss:8.04264842425178e-05\n",
      "train loss:0.00033030881099818486\n",
      "train loss:0.0002959584161644762\n",
      "train loss:5.0088233855584595e-05\n",
      "train loss:0.00023785774597369003\n",
      "train loss:0.00019839557144348953\n",
      "train loss:0.0004739417015802145\n",
      "train loss:0.00037850386113878564\n",
      "train loss:0.00012009438114861985\n",
      "train loss:0.0004207793191676215\n",
      "train loss:0.000350108242641565\n",
      "train loss:0.00021494608109287196\n",
      "train loss:0.0003378168638733511\n",
      "train loss:0.00037414411052268966\n",
      "train loss:0.00033442112238684713\n",
      "train loss:0.00023498735745569808\n",
      "train loss:0.0002276569480301674\n",
      "train loss:0.00022220862456533367\n",
      "train loss:0.00018706090792939854\n",
      "train loss:0.00010943102248023073\n",
      "train loss:0.00018242259004713123\n",
      "train loss:0.0004561601683605631\n",
      "train loss:0.00016381003132636158\n",
      "train loss:0.0001629511175796242\n",
      "train loss:0.00021822896546817464\n",
      "train loss:0.00047714783987255906\n",
      "train loss:0.0003128191867760824\n",
      "train loss:0.00015013175332610046\n",
      "train loss:7.046457584031988e-05\n",
      "train loss:0.00048338798494531037\n",
      "train loss:0.00030448416108419665\n",
      "train loss:0.0006734739206392536\n",
      "train loss:6.771089970686675e-05\n",
      "train loss:0.00024060355139900218\n",
      "train loss:0.00039462504076300225\n",
      "train loss:4.2644723183473284e-05\n",
      "train loss:0.0002980995951215929\n",
      "train loss:0.0001833519984563569\n",
      "train loss:7.310035727834215e-05\n",
      "train loss:0.0001469808327803041\n",
      "train loss:0.0005002907728233934\n",
      "train loss:4.3789696811793796e-05\n",
      "train loss:6.594113575700457e-05\n",
      "train loss:0.0004590979530987032\n",
      "train loss:0.001142938546224387\n",
      "train loss:0.00022021014639234826\n",
      "train loss:0.00033120695721610186\n",
      "train loss:0.000138015421405973\n",
      "train loss:6.174611245328326e-05\n",
      "train loss:0.0003028992946839559\n",
      "train loss:0.00019516399097836473\n",
      "train loss:0.0003018490603965605\n",
      "train loss:0.0003025467673112487\n",
      "train loss:0.00010139822752194152\n",
      "train loss:0.00015692243346243816\n",
      "train loss:0.00039688424350084385\n",
      "train loss:0.00027829096785262784\n",
      "train loss:0.0003969587002448201\n",
      "train loss:0.0003155897034131105\n",
      "train loss:0.00032605899444479095\n",
      "train loss:0.0008127070372150312\n",
      "train loss:0.00026514895792300783\n",
      "train loss:0.00017807562071054964\n",
      "train loss:0.00029634122143984773\n",
      "train loss:0.00018593019342938222\n",
      "train loss:0.00021287851388884842\n",
      "=== epoch:2, train acc:1.0, test acc:1.0 ===\n",
      "train loss:6.799071176034395e-05\n",
      "train loss:6.183487930581903e-05\n",
      "train loss:0.00026210495517372157\n",
      "train loss:0.00030587500339362115\n",
      "train loss:0.00045273671521895375\n",
      "train loss:0.000450756988691451\n",
      "train loss:0.0005072526779618824\n",
      "train loss:0.00036202374523829105\n",
      "train loss:0.0001828218360725063\n",
      "train loss:0.00026660329813672437\n",
      "train loss:0.0004455258667093004\n",
      "train loss:0.0004121843381047871\n",
      "train loss:0.0005793843642205096\n",
      "train loss:0.0002800847582130772\n",
      "train loss:0.0005166109137373836\n",
      "train loss:0.00028390499522004307\n",
      "train loss:6.780707389700697e-05\n",
      "train loss:0.0005640343431470401\n",
      "train loss:9.414579452689359e-05\n",
      "train loss:0.00020535613991023326\n",
      "train loss:0.0001999781578734216\n",
      "train loss:0.0005148198349964793\n",
      "train loss:0.00040076681680716844\n",
      "train loss:0.0001158363191987733\n",
      "train loss:5.2293980044256655e-05\n",
      "train loss:0.00012490987926942293\n",
      "train loss:0.0004304288076765006\n",
      "train loss:0.00012981489777931542\n",
      "train loss:0.00013361428065511612\n",
      "train loss:0.000195064613431515\n",
      "train loss:8.614027872551654e-05\n",
      "train loss:0.00019709142450681836\n",
      "train loss:0.00028747897825041256\n",
      "train loss:0.0001089515899203312\n",
      "train loss:0.00013002477662734963\n",
      "train loss:9.73880011137878e-05\n",
      "train loss:0.0004140370194588302\n",
      "train loss:0.0001268978152071149\n",
      "train loss:0.00022128981007241854\n",
      "train loss:0.00043327055430289447\n",
      "train loss:0.00039469089828098795\n",
      "train loss:0.00026545183290587913\n",
      "train loss:0.00015700879918901405\n",
      "train loss:0.00037103273696338754\n",
      "train loss:0.0010711202568473009\n",
      "train loss:0.0001481495619206997\n",
      "train loss:0.00025172488198219907\n",
      "train loss:0.00031667602181633894\n",
      "train loss:4.57927306924273e-05\n",
      "train loss:0.00012175249864619405\n",
      "train loss:0.0003334952219675519\n",
      "train loss:0.00013523488705892585\n",
      "train loss:0.00024753462290669643\n",
      "train loss:5.5948058707862025e-05\n",
      "train loss:0.00014575638680802724\n",
      "train loss:0.0004663291387788923\n",
      "train loss:0.0005440655100023297\n",
      "train loss:0.0005348615749380871\n",
      "train loss:0.0004382019853257956\n",
      "train loss:9.151083678615536e-05\n",
      "train loss:0.0003042549111187959\n",
      "train loss:0.00031375587400346095\n",
      "train loss:0.00016642717577594311\n",
      "train loss:3.761436824213391e-05\n",
      "train loss:0.00020720766026291137\n",
      "train loss:0.00023835759409702399\n",
      "train loss:0.00039492138696458674\n",
      "train loss:8.859319838966156e-05\n",
      "train loss:0.00017134327208369142\n",
      "train loss:0.0006797467223445952\n",
      "train loss:0.00023214120084957994\n",
      "train loss:0.00043654965370198897\n",
      "train loss:0.00034776419201671175\n",
      "train loss:0.0002895254044732288\n",
      "train loss:0.0004058927566946023\n",
      "train loss:4.333601265800043e-05\n",
      "train loss:0.00029296292153911224\n",
      "train loss:0.001063102047200872\n",
      "train loss:0.00010962387382812221\n",
      "train loss:0.00041506031737213034\n",
      "=== epoch:3, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00016482548413494454\n",
      "train loss:0.00014778858743264925\n",
      "train loss:5.352886336577598e-05\n",
      "train loss:0.00024326667268286979\n",
      "train loss:0.00045194418816443767\n",
      "train loss:0.0001643739200171377\n",
      "train loss:0.0005789006925722671\n",
      "train loss:0.0003192173499726021\n",
      "train loss:0.0004795833546966866\n",
      "train loss:0.00014207512291736457\n",
      "train loss:0.00014127733203396365\n",
      "train loss:0.00022725017712015003\n",
      "train loss:0.00040758654023044017\n",
      "train loss:0.0001511735576572808\n",
      "train loss:6.748169473764247e-05\n",
      "train loss:0.0002106030363081296\n",
      "train loss:0.00013445921621155595\n",
      "train loss:0.0001544059731383468\n",
      "train loss:7.78193800816656e-05\n",
      "train loss:9.417739021273179e-05\n",
      "train loss:0.00015106008001602756\n",
      "train loss:0.0003397846667043033\n",
      "train loss:0.0006657995127947237\n",
      "train loss:0.00018438899354731832\n",
      "train loss:0.0003411216914266731\n",
      "train loss:0.00034259386183061743\n",
      "train loss:0.0004160442959143039\n",
      "train loss:0.0002766848432708301\n",
      "train loss:0.0004206843387681979\n",
      "train loss:0.0002093867080096971\n",
      "train loss:0.00036749343317926046\n",
      "train loss:4.516286124638885e-05\n",
      "train loss:0.00042892272072556927\n",
      "train loss:0.00012496395808489193\n",
      "train loss:0.00011619068184769825\n",
      "train loss:0.001480327898359294\n",
      "train loss:0.000270444369086667\n",
      "train loss:0.00010263888272264429\n",
      "train loss:0.00014534657340755763\n",
      "train loss:4.151520025088346e-05\n",
      "train loss:0.0002967174379635193\n",
      "train loss:0.0001701881582730976\n",
      "train loss:0.00023157215144419617\n",
      "train loss:0.000426653415623473\n",
      "train loss:9.373365077585752e-05\n",
      "train loss:0.0001516997330091713\n",
      "train loss:0.00012860442674845373\n",
      "train loss:0.0002538538725003438\n",
      "train loss:0.00015950779218975165\n",
      "train loss:0.0003576189353179739\n",
      "train loss:0.00024181630791519617\n",
      "train loss:9.93218422994392e-05\n",
      "train loss:0.0003208677030018541\n",
      "train loss:5.203095527376841e-05\n",
      "train loss:0.00018365509321786638\n",
      "train loss:0.0002769366880044935\n",
      "train loss:5.739153940771645e-05\n",
      "train loss:0.0012054466769662218\n",
      "train loss:0.00017301609661123148\n",
      "train loss:5.63397398859886e-05\n",
      "train loss:9.490127918544743e-05\n",
      "train loss:0.00032171066234336126\n",
      "train loss:0.00023006773777228023\n",
      "train loss:0.00023137737893204796\n",
      "train loss:0.0005664800232102929\n",
      "train loss:0.00010305699863376961\n",
      "train loss:0.0005229031161862215\n",
      "train loss:0.00013674853757224688\n",
      "train loss:0.00018337754654828265\n",
      "train loss:0.00017777963981767338\n",
      "train loss:0.00033722012407324365\n",
      "train loss:0.0001673119774168208\n",
      "train loss:0.00018866486922756352\n",
      "train loss:0.0002386565790829253\n",
      "train loss:0.0001307047919195021\n",
      "train loss:5.244097032580845e-05\n",
      "train loss:0.0001031888267694759\n",
      "train loss:0.0003777614015371979\n",
      "train loss:4.9534160474325035e-05\n",
      "train loss:0.00036630625343050396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:4, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00038683599858857823\n",
      "train loss:0.0003332475827282596\n",
      "train loss:8.885084736403089e-05\n",
      "train loss:7.04091863803886e-05\n",
      "train loss:0.00011335102447654733\n",
      "train loss:0.00020972499846013337\n",
      "train loss:0.0002190230396351612\n",
      "train loss:0.00034184371076872067\n",
      "train loss:7.602212454224091e-05\n",
      "train loss:0.00017858035432436798\n",
      "train loss:0.00023988558368271673\n",
      "train loss:0.00032196487331127545\n",
      "train loss:0.0004141962505784774\n",
      "train loss:0.0005896697219563182\n",
      "train loss:0.0002155465652456761\n",
      "train loss:0.00023926970883833632\n",
      "train loss:0.0002349690530560499\n",
      "train loss:0.00026694671738883\n",
      "train loss:0.00033124903792696727\n",
      "train loss:0.00016177299200433005\n",
      "train loss:0.0003986571823497025\n",
      "train loss:0.0004284079157114402\n",
      "train loss:0.00021949866978433687\n",
      "train loss:9.208588818002787e-05\n",
      "train loss:0.00010367778249580638\n",
      "train loss:0.00014596031279567214\n",
      "train loss:0.00016403416155602195\n",
      "train loss:0.00024385467040521273\n",
      "train loss:0.00026946869499989886\n",
      "train loss:0.00019515352152153724\n",
      "train loss:0.00043599629059627395\n",
      "train loss:2.913949621684117e-05\n",
      "train loss:0.00011747187701047264\n",
      "train loss:0.0004128521127008489\n",
      "train loss:8.951022159295704e-05\n",
      "train loss:0.0003246120514656281\n",
      "train loss:9.798141462026256e-05\n",
      "train loss:0.0004054418740561858\n",
      "train loss:0.00025810216376897214\n",
      "train loss:0.0002431120541272266\n",
      "train loss:0.00021469671161784511\n",
      "train loss:5.3291522715703426e-05\n",
      "train loss:0.00018553044672960927\n",
      "train loss:6.45764831197752e-05\n",
      "train loss:9.567169238027246e-05\n",
      "train loss:7.263263889867276e-05\n",
      "train loss:7.315659018592623e-05\n",
      "train loss:0.00021408740418068486\n",
      "train loss:0.00045780711609885055\n",
      "train loss:0.0009364942744825681\n",
      "train loss:0.00014338368490536083\n",
      "train loss:0.0002742820189104327\n",
      "train loss:0.00011528908660553425\n",
      "train loss:0.00031500900530846725\n",
      "train loss:2.3923713115898988e-05\n",
      "train loss:0.0005861057485196431\n",
      "train loss:9.34201669850784e-05\n",
      "train loss:0.00012416951765428967\n",
      "train loss:0.00028831677680946283\n",
      "train loss:0.0008805122290586664\n",
      "train loss:8.447238224661087e-05\n",
      "train loss:0.00033811970384283385\n",
      "train loss:0.0003871171829754783\n",
      "train loss:0.00013110815561524934\n",
      "train loss:0.00038250384697660573\n",
      "train loss:0.00017577346644987704\n",
      "train loss:0.0001575898884664723\n",
      "train loss:0.00039904673308504223\n",
      "train loss:0.00016172557118748098\n",
      "train loss:0.00019508062878473097\n",
      "train loss:0.0005321809759904494\n",
      "train loss:0.00026229335472483667\n",
      "train loss:7.13720894959406e-05\n",
      "train loss:0.00040044105697365206\n",
      "train loss:0.00045966999829956514\n",
      "train loss:5.8150981680595866e-05\n",
      "train loss:0.00078474131413745\n",
      "train loss:0.00011396652242966435\n",
      "train loss:0.0002101110280544201\n",
      "train loss:0.0002470590564695165\n",
      "=== epoch:5, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00031801862940357244\n",
      "train loss:0.0002372487610834148\n",
      "train loss:0.00015197606032226081\n",
      "train loss:0.0003744694247437766\n",
      "train loss:0.00014638980899012077\n",
      "train loss:0.00015914512886431567\n",
      "train loss:0.00014109898763324137\n",
      "train loss:0.00024238300166185968\n",
      "train loss:0.00025811877613158814\n",
      "train loss:0.00011302197543422948\n",
      "train loss:0.0002232198576712223\n",
      "train loss:0.0004787264936644876\n",
      "train loss:9.02405745967837e-05\n",
      "train loss:0.0001136984817931652\n",
      "train loss:0.0001710941869412258\n",
      "train loss:0.00029340200481914146\n",
      "train loss:5.1132779030686556e-05\n",
      "train loss:0.00011300615528600937\n",
      "train loss:0.0005076536650717407\n",
      "train loss:0.00015678368150966914\n",
      "train loss:5.3345905212129394e-05\n",
      "train loss:0.00018261668230366043\n",
      "train loss:0.00038525654466968934\n",
      "train loss:0.00019550163763885662\n",
      "train loss:6.286113548623967e-05\n",
      "train loss:0.00010755701801702735\n",
      "train loss:0.0006409067443257866\n",
      "train loss:7.465940628611003e-05\n",
      "train loss:0.00013805751802699846\n",
      "train loss:0.0004129785044660135\n",
      "train loss:0.00021910392427751233\n",
      "train loss:0.00013849460938693367\n",
      "train loss:0.0003785112484993937\n",
      "train loss:0.0003043062575837903\n",
      "train loss:0.00029472695152254515\n",
      "train loss:0.00045421038063862465\n",
      "train loss:0.00015608036361300124\n",
      "train loss:0.0002115933007664477\n",
      "train loss:7.369008758306956e-05\n",
      "train loss:6.303733983288221e-05\n",
      "train loss:0.0001419317315411426\n",
      "train loss:0.00014609854505101644\n",
      "train loss:0.0003946256314611106\n",
      "train loss:0.00014577761995237818\n",
      "train loss:0.0001082052831398214\n",
      "train loss:6.014531036749798e-05\n",
      "train loss:0.0002072485487833461\n",
      "train loss:0.00017205936222765904\n",
      "train loss:9.026977248468048e-05\n",
      "train loss:0.0001919280923118523\n",
      "train loss:0.00019093439526409372\n",
      "train loss:0.00018712512553430876\n",
      "train loss:0.00016688006195802893\n",
      "train loss:0.00037297530463913884\n",
      "train loss:0.00031572320739478484\n",
      "train loss:8.309105414942619e-05\n",
      "train loss:0.00030993042576208323\n",
      "train loss:0.0001451608535543143\n",
      "train loss:0.0003096875313156711\n",
      "train loss:0.00019508940431777063\n",
      "train loss:0.00032892447673177404\n",
      "train loss:0.0001823110688345545\n",
      "train loss:0.0001544500747391151\n",
      "train loss:0.0002257187016827398\n",
      "train loss:0.00013108871619202343\n",
      "train loss:0.0001951873269586309\n",
      "train loss:0.00013561566909646246\n",
      "train loss:7.776760772871513e-05\n",
      "train loss:8.834413024679816e-05\n",
      "train loss:9.578291659348393e-05\n",
      "train loss:0.00012190877440237889\n",
      "train loss:0.00015871820678641394\n",
      "train loss:0.00017603260114587322\n",
      "train loss:0.00019915984065722287\n",
      "train loss:0.00021474557673682962\n",
      "train loss:0.00020114874057862426\n",
      "train loss:0.00039571748026915254\n",
      "train loss:0.00013815311241876648\n",
      "train loss:0.00020815508849071062\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:1.0\n",
      "train loss:0.0001508844558536087\n",
      "=== epoch:1, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00015074943896549955\n",
      "train loss:8.242168519921292e-05\n",
      "train loss:7.575547280346253e-05\n",
      "train loss:0.00015181714622105938\n",
      "train loss:0.0009316694710191356\n",
      "train loss:0.00017951548209270256\n",
      "train loss:0.0002472465949824683\n",
      "train loss:0.0002131456431639316\n",
      "train loss:0.0003854432403399991\n",
      "train loss:0.0001620071796634249\n",
      "train loss:0.00023670176343727463\n",
      "train loss:7.835651534099142e-05\n",
      "train loss:0.00010101706943330099\n",
      "train loss:0.00022218657992244706\n",
      "train loss:7.837475195219003e-05\n",
      "train loss:0.00012096629932715838\n",
      "train loss:0.00014879871973297703\n",
      "train loss:0.00014343223147714136\n",
      "train loss:0.0002859192599563578\n",
      "train loss:2.7719064448577507e-05\n",
      "train loss:0.00012510280611157778\n",
      "train loss:6.369209227403364e-05\n",
      "train loss:0.0005154210075223976\n",
      "train loss:0.0002385825898246888\n",
      "train loss:0.00017957936037897473\n",
      "train loss:0.00012910577422244065\n",
      "train loss:0.0016633056437030669\n",
      "train loss:0.000514118053729311\n",
      "train loss:0.0001270171493947531\n",
      "train loss:0.0004037404981437959\n",
      "train loss:7.7909010948125e-05\n",
      "train loss:0.00019045546637033947\n",
      "train loss:0.0006730502744038308\n",
      "train loss:0.00047224931622633326\n",
      "train loss:0.00013915938465965125\n",
      "train loss:0.00026235742931830346\n",
      "train loss:0.00016023863946249619\n",
      "train loss:0.0014652109277821677\n",
      "train loss:0.0003613776899718465\n",
      "train loss:0.0003481951307700337\n",
      "train loss:0.0002888636324019879\n",
      "train loss:0.00020232564533920468\n",
      "train loss:0.0008365518336276689\n",
      "train loss:8.893737264799652e-05\n",
      "train loss:0.0003010363860030455\n",
      "train loss:0.000374731901095812\n",
      "train loss:0.0004811765394873844\n",
      "train loss:8.274665881475306e-05\n",
      "train loss:0.0003725416488525338\n",
      "train loss:0.0004578896707444415\n",
      "train loss:0.00014012670126857296\n",
      "train loss:0.0001814965211918272\n",
      "train loss:0.0001890308415305693\n",
      "train loss:0.00015530222600376704\n",
      "train loss:0.000602535419195518\n",
      "train loss:0.00043591225090614395\n",
      "train loss:0.00019877083471585129\n",
      "train loss:0.00023903107796025358\n",
      "train loss:0.0001603410492034595\n",
      "train loss:0.0002913118806504301\n",
      "train loss:0.0003125571595886575\n",
      "train loss:0.00025428202810222415\n",
      "train loss:0.0006975994688966413\n",
      "train loss:0.00012714205079252482\n",
      "train loss:0.00021733361638606033\n",
      "train loss:4.629588290266127e-05\n",
      "train loss:0.00023634972300992936\n",
      "train loss:0.00013607995189752732\n",
      "train loss:0.0004502095663979347\n",
      "train loss:4.861157541080658e-05\n",
      "train loss:0.00023378609149277244\n",
      "train loss:0.0006576945571645039\n",
      "train loss:0.00014571927828767396\n",
      "train loss:0.0002422882973843827\n",
      "train loss:0.00035002202175610675\n",
      "train loss:0.0005258438333687283\n",
      "train loss:0.00038178246711986275\n",
      "train loss:0.00043087935437487797\n",
      "train loss:0.00010451211882445397\n",
      "train loss:0.00019735310212145332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:2, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0002688753997013629\n",
      "train loss:9.904232681757458e-05\n",
      "train loss:0.00012436882113034967\n",
      "train loss:0.00025892370998404555\n",
      "train loss:0.00032208692660187636\n",
      "train loss:0.00010557456792596979\n",
      "train loss:0.0016094787499798464\n",
      "train loss:0.00033172604742632596\n",
      "train loss:0.00015284191876632547\n",
      "train loss:0.0002042035254275096\n",
      "train loss:0.00011679094382547312\n",
      "train loss:0.00013591256654927678\n",
      "train loss:0.00037499422639374313\n",
      "train loss:0.000539757434659748\n",
      "train loss:9.574201785474941e-05\n",
      "train loss:0.00022087199051043426\n",
      "train loss:0.0008987399083513638\n",
      "train loss:0.0002038047094608951\n",
      "train loss:0.00039515689678614575\n",
      "train loss:0.00017277668429345903\n",
      "train loss:0.0003250480024408901\n",
      "train loss:0.00013922039881858394\n",
      "train loss:5.772006705498901e-05\n",
      "train loss:8.933836171901603e-05\n",
      "train loss:0.0005501694297613143\n",
      "train loss:0.00017218093226622014\n",
      "train loss:0.0003494612602232554\n",
      "train loss:0.00011893858041908024\n",
      "train loss:0.0005303396686933451\n",
      "train loss:0.00045037977682984995\n",
      "train loss:0.0002630093209144354\n",
      "train loss:0.00024064908838004239\n",
      "train loss:0.0002474600203555012\n",
      "train loss:0.00020029019175516602\n",
      "train loss:0.00021373863516619359\n",
      "train loss:0.00015347554769640632\n",
      "train loss:0.00048693939722773165\n",
      "train loss:0.00019671176666437532\n",
      "train loss:0.00028193947319539767\n",
      "train loss:0.0007770754794388715\n",
      "train loss:0.00014217484841710508\n",
      "train loss:9.856575749664942e-05\n",
      "train loss:0.0001772053456710534\n",
      "train loss:0.0003239743017829412\n",
      "train loss:0.00014514299463653583\n",
      "train loss:7.276979275748126e-05\n",
      "train loss:0.00021777648808386638\n",
      "train loss:0.00011498068125269618\n",
      "train loss:0.0003319478768859202\n",
      "train loss:0.00022602842380609837\n",
      "train loss:0.0002834998560127364\n",
      "train loss:0.00037510799259097205\n",
      "train loss:2.028767797802125e-05\n",
      "train loss:0.00018218622394722628\n",
      "train loss:0.00026526990325738615\n",
      "train loss:0.00019247319393262723\n",
      "train loss:0.00012361660160750795\n",
      "train loss:6.728009447232526e-05\n",
      "train loss:0.0007776551133871026\n",
      "train loss:0.00047236086957963796\n",
      "train loss:0.0001665237623773522\n",
      "train loss:0.0003432329294874713\n",
      "train loss:0.00038405754856272783\n",
      "train loss:0.00013742658757144986\n",
      "train loss:8.974331219375404e-05\n",
      "train loss:0.0007910608735065168\n",
      "train loss:0.0001296279825301942\n",
      "train loss:0.00010509597057244687\n",
      "train loss:0.00025756954064169556\n",
      "train loss:0.00013870976016886464\n",
      "train loss:0.0002366773697182385\n",
      "train loss:0.0002823337327248646\n",
      "train loss:0.00013980183843126558\n",
      "train loss:0.0003636968653753806\n",
      "train loss:0.00012821517191821544\n",
      "train loss:0.00010773164411863972\n",
      "train loss:0.0003444062209585292\n",
      "train loss:0.00020356424986228534\n",
      "train loss:0.00015638039030394071\n",
      "train loss:0.00032525162444988496\n",
      "=== epoch:3, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0002964211832481114\n",
      "train loss:0.0001886906928424032\n",
      "train loss:0.000195042787441271\n",
      "train loss:0.00021824146172443146\n",
      "train loss:9.408126337668004e-05\n",
      "train loss:0.00021838446164973313\n",
      "train loss:0.00014917974899106135\n",
      "train loss:0.0002964287508187464\n",
      "train loss:5.373649513565104e-05\n",
      "train loss:0.00013691102899089538\n",
      "train loss:4.9027102506398216e-05\n",
      "train loss:0.00015103171368177761\n",
      "train loss:0.00047086199145187176\n",
      "train loss:5.069145048297815e-05\n",
      "train loss:0.0001618722959546122\n",
      "train loss:0.00013452856811161862\n",
      "train loss:9.42696142960001e-05\n",
      "train loss:0.00023691335354156186\n",
      "train loss:7.721766644200112e-05\n",
      "train loss:0.0001293309787906053\n",
      "train loss:0.00014170810255345576\n",
      "train loss:0.00018352253222270895\n",
      "train loss:6.611519806312676e-05\n",
      "train loss:0.00016959555773676032\n",
      "train loss:6.0517979882936656e-05\n",
      "train loss:6.626781408751587e-05\n",
      "train loss:0.00021026685851932746\n",
      "train loss:0.00010425084072605214\n",
      "train loss:0.00020855773673674822\n",
      "train loss:0.000143432285976347\n",
      "train loss:0.00026416063647688925\n",
      "train loss:0.00015503806931453597\n",
      "train loss:0.0005421305131531638\n",
      "train loss:0.00027887837087008853\n",
      "train loss:0.00010179479625355389\n",
      "train loss:6.234780227758072e-05\n",
      "train loss:0.000302039722633527\n",
      "train loss:3.9696940805523676e-05\n",
      "train loss:0.00019114412211627144\n",
      "train loss:0.00022102124075153005\n",
      "train loss:0.00023235769450581184\n",
      "train loss:8.227795146410428e-05\n",
      "train loss:6.60372946934325e-05\n",
      "train loss:0.00046389529112787715\n",
      "train loss:0.0003803671926239378\n",
      "train loss:0.0001546971516027443\n",
      "train loss:4.998089776535137e-05\n",
      "train loss:0.00012662064694351875\n",
      "train loss:9.964523428037356e-05\n",
      "train loss:0.00022515102256317462\n",
      "train loss:5.2689794999436314e-05\n",
      "train loss:0.00018075557380611482\n",
      "train loss:0.00014705683679818352\n",
      "train loss:0.0003621911747837616\n",
      "train loss:4.591544776406821e-05\n",
      "train loss:0.00038868754046631154\n",
      "train loss:0.00032988710470139714\n",
      "train loss:0.00021693536834485936\n",
      "train loss:0.0004432410682238353\n",
      "train loss:0.0002062117827806292\n",
      "train loss:0.00014710784636892576\n",
      "train loss:0.00021301396497025435\n",
      "train loss:0.00016373634174516856\n",
      "train loss:6.372290110257211e-05\n",
      "train loss:0.0001420790594530543\n",
      "train loss:0.00019791535021170605\n",
      "train loss:0.0001716028200126378\n",
      "train loss:0.00023552949017149598\n",
      "train loss:8.246333879363045e-05\n",
      "train loss:0.0001745524168625553\n",
      "train loss:0.0002585591281709281\n",
      "train loss:0.0003303132054411547\n",
      "train loss:0.0001095719405033054\n",
      "train loss:0.00012489593331837175\n",
      "train loss:0.00012836868231850572\n",
      "train loss:0.0002628438667909847\n",
      "train loss:9.89920427642667e-05\n",
      "train loss:0.00011082653335029473\n",
      "train loss:0.0003145414792982433\n",
      "train loss:7.220058790328953e-05\n",
      "=== epoch:4, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0001386153475914729\n",
      "train loss:0.0001537095802143964\n",
      "train loss:0.00022387519887184584\n",
      "train loss:0.0002756896259388169\n",
      "train loss:0.0003841514308531463\n",
      "train loss:5.408443127403412e-05\n",
      "train loss:0.0002583769556776955\n",
      "train loss:0.00010867220767403362\n",
      "train loss:0.0002582345490599514\n",
      "train loss:0.00011169582233560196\n",
      "train loss:5.2748699841469746e-05\n",
      "train loss:0.00011827717836882326\n",
      "train loss:0.00015544991933902772\n",
      "train loss:0.00035686445691381736\n",
      "train loss:0.0001562047467451562\n",
      "train loss:0.0005075268559882671\n",
      "train loss:7.829634286350656e-05\n",
      "train loss:0.0006091500436392264\n",
      "train loss:0.0002732937987871\n",
      "train loss:0.000158693246367694\n",
      "train loss:0.0002897081370924686\n",
      "train loss:0.000237210474696849\n",
      "train loss:8.058852735154189e-05\n",
      "train loss:9.11988035054637e-05\n",
      "train loss:0.00023019639627149113\n",
      "train loss:7.476963411164813e-05\n",
      "train loss:8.466756033085178e-05\n",
      "train loss:4.06450627045662e-05\n",
      "train loss:2.9853240504683633e-05\n",
      "train loss:0.0007073332524267121\n",
      "train loss:0.00016778449618601134\n",
      "train loss:0.00015806712947658878\n",
      "train loss:5.070204134608739e-05\n",
      "train loss:0.00011911854195334359\n",
      "train loss:3.692796548325268e-05\n",
      "train loss:0.0002603689866432642\n",
      "train loss:0.0004003388890611198\n",
      "train loss:1.5385192467339615e-05\n",
      "train loss:0.00026661024076960653\n",
      "train loss:0.00020263999440238228\n",
      "train loss:0.0003012882876953364\n",
      "train loss:0.0002502181683748359\n",
      "train loss:0.00011636984961396932\n",
      "train loss:0.0002274121368622436\n",
      "train loss:0.00010249264683250755\n",
      "train loss:0.00038760229876086774\n",
      "train loss:0.00081812262306197\n",
      "train loss:0.0004597219391339192\n",
      "train loss:0.00010986223805265533\n",
      "train loss:0.0002766627547379871\n",
      "train loss:3.531678866634742e-05\n",
      "train loss:0.0002780332682837096\n",
      "train loss:0.00017021800852224644\n",
      "train loss:0.0001549360100852109\n",
      "train loss:9.433198215670669e-05\n",
      "train loss:0.00020295176822240774\n",
      "train loss:0.00010385685044497494\n",
      "train loss:0.00029062282182517143\n",
      "train loss:0.00016668965014083242\n",
      "train loss:0.00017749791137411453\n",
      "train loss:0.00019563331283788886\n",
      "train loss:0.0003171163507049992\n",
      "train loss:0.00011592053096160574\n",
      "train loss:0.0003662420721885409\n",
      "train loss:0.00011700903569540298\n",
      "train loss:0.0002533712963362103\n",
      "train loss:7.307235909856304e-05\n",
      "train loss:0.0004152467974777912\n",
      "train loss:0.00022824146190303382\n",
      "train loss:0.00021234088250890096\n",
      "train loss:0.00019168527697924795\n",
      "train loss:0.00014592805108650474\n",
      "train loss:0.0002038927927285397\n",
      "train loss:8.799636122854042e-05\n",
      "train loss:8.997697017491712e-05\n",
      "train loss:0.00040329620318829327\n",
      "train loss:5.345360853590218e-05\n",
      "train loss:0.0001971224581102099\n",
      "train loss:0.00011493846932431096\n",
      "train loss:0.00025216460060313405\n",
      "=== epoch:5, train acc:1.0, test acc:1.0 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00022337331844424175\n",
      "train loss:0.0004023276403239847\n",
      "train loss:3.567517611375816e-05\n",
      "train loss:0.0001230416588346119\n",
      "train loss:9.795244598426186e-05\n",
      "train loss:0.0003010661319930708\n",
      "train loss:0.00013156079852805287\n",
      "train loss:0.0002214523276307128\n",
      "train loss:4.801504333955067e-05\n",
      "train loss:0.00022866822367673242\n",
      "train loss:0.00033223215347641117\n",
      "train loss:7.001859641464825e-05\n",
      "train loss:0.000327836710190241\n",
      "train loss:0.0003828635858921256\n",
      "train loss:0.00010076508837851397\n",
      "train loss:0.0005458751159512974\n",
      "train loss:8.55991031411882e-05\n",
      "train loss:0.0001332851799047945\n",
      "train loss:0.0002373307560028676\n",
      "train loss:0.0005905324799606046\n",
      "train loss:0.00010274423527240318\n",
      "train loss:8.16485547133289e-05\n",
      "train loss:0.00021934075813699692\n",
      "train loss:0.0001757770042141004\n",
      "train loss:0.00024702735449041264\n",
      "train loss:0.00011129490754365058\n",
      "train loss:0.00020797059632969694\n",
      "train loss:0.00011852640707028873\n",
      "train loss:0.0001815760181145349\n",
      "train loss:0.0002301715307039706\n",
      "train loss:0.00013102842169105246\n",
      "train loss:0.00015306603395664998\n",
      "train loss:7.33941816739436e-05\n",
      "train loss:0.0001526488477864932\n",
      "train loss:0.00010720063545070229\n",
      "train loss:0.00011140759541738987\n",
      "train loss:0.00011404861729368643\n",
      "train loss:0.00028104580934188616\n",
      "train loss:3.634682452509848e-05\n",
      "train loss:0.0002983097269651203\n",
      "train loss:9.4349107909845e-05\n",
      "train loss:6.432883186161568e-05\n",
      "train loss:0.0005137706805113099\n",
      "train loss:0.00013538930728239042\n",
      "train loss:7.956414337470791e-05\n",
      "train loss:0.00010052763146562413\n",
      "train loss:0.00012834252274086466\n",
      "train loss:0.00010513240069244348\n",
      "train loss:0.0002535721490528908\n",
      "train loss:0.00015327917652332965\n",
      "train loss:0.00011993081391888892\n",
      "train loss:0.00017640476370675537\n",
      "train loss:0.0002618448363049526\n",
      "train loss:0.0001314129060223957\n",
      "train loss:0.0002425464954193624\n",
      "train loss:0.0003493994053748558\n",
      "train loss:9.163204947386108e-05\n",
      "train loss:0.00015339182592702996\n",
      "train loss:0.00020443484719442914\n",
      "train loss:0.0001858811227850683\n",
      "train loss:0.00025163323915916076\n",
      "train loss:6.0134914384934845e-05\n",
      "train loss:0.00018745443046423564\n",
      "train loss:8.968579798595018e-05\n",
      "train loss:9.287204055002409e-05\n",
      "train loss:5.057694461505801e-05\n",
      "train loss:0.00032745492884367073\n",
      "train loss:0.0001943988985157893\n",
      "train loss:9.121379079492833e-05\n",
      "train loss:0.00019756126336882278\n",
      "train loss:0.0004603038472584836\n",
      "train loss:0.000277595335437875\n",
      "train loss:0.0001188623689167942\n",
      "train loss:0.00020414514010283426\n",
      "train loss:7.715753534234997e-05\n",
      "train loss:0.00010149099757532431\n",
      "train loss:0.00011249756784979684\n",
      "train loss:0.000184014088364859\n",
      "train loss:0.0001621015245709702\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:1.0\n"
     ]
    }
   ],
   "source": [
    "x_train=x_train_7\n",
    "x_test = x_test_7\n",
    "t_train=t_train_7\n",
    "t_test = t_test_7\n",
    "\n",
    "trainer_SGD = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='SGD', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_MMT = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Momentum', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_SGD.train()\n",
    "trainer_MMT.train()\n",
    "\n",
    "MMT_acc.append(trainer_MMT.test_acc_list[-1])\n",
    "SGD_acc.append(trainer_SGD.test_acc_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00017596676971868202\n",
      "=== epoch:1, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00010856376549712349\n",
      "train loss:0.00010360016447502523\n",
      "train loss:0.00023043385568379538\n",
      "train loss:0.00016671814583519808\n",
      "train loss:0.00023532110145305744\n",
      "train loss:0.0004025412993640281\n",
      "train loss:0.00024251568127039828\n",
      "train loss:0.0003052610157328776\n",
      "train loss:0.0002558583033320787\n",
      "train loss:0.0002450714952821229\n",
      "train loss:0.00013712281005435518\n",
      "train loss:0.000334262191974068\n",
      "train loss:0.00019050841456904078\n",
      "train loss:0.00019483929992802733\n",
      "train loss:0.00033333586336747206\n",
      "train loss:3.259983659814191e-05\n",
      "train loss:0.000250645956384831\n",
      "train loss:0.0001897843288580331\n",
      "train loss:7.476694920753895e-05\n",
      "train loss:0.00011337809268250662\n",
      "train loss:6.468665647983373e-05\n",
      "train loss:0.00024438492526088576\n",
      "train loss:0.00014071809487344823\n",
      "train loss:0.0002517234986531469\n",
      "train loss:0.00025422894696451104\n",
      "train loss:0.00020840554595235355\n",
      "train loss:0.00035642988459069975\n",
      "train loss:0.0002799475470769263\n",
      "train loss:7.535820287387245e-05\n",
      "train loss:0.00015045611596098356\n",
      "train loss:7.783131292955501e-05\n",
      "train loss:0.0002527286284061759\n",
      "train loss:0.0001256011418784949\n",
      "train loss:7.809146923002615e-05\n",
      "train loss:8.599784618180178e-05\n",
      "train loss:0.0006093260287474949\n",
      "train loss:9.960071688574817e-05\n",
      "train loss:0.0002293007728342604\n",
      "train loss:0.0002532902801826854\n",
      "train loss:0.0001484564308030919\n",
      "train loss:0.00028629010563362904\n",
      "train loss:0.00019817849124689258\n",
      "train loss:0.0003034775231565278\n",
      "train loss:0.0004894656770877501\n",
      "train loss:0.00018051355780115006\n",
      "train loss:4.416571442580996e-05\n",
      "train loss:4.8152764530058886e-05\n",
      "train loss:0.00014996163171801088\n",
      "train loss:0.00022344636403534985\n",
      "train loss:0.0002540399491649896\n",
      "train loss:0.00024111271304594936\n",
      "train loss:5.470317908056455e-05\n",
      "train loss:0.00028422435258212313\n",
      "train loss:0.00019732066371130652\n",
      "train loss:3.406771338592322e-05\n",
      "train loss:0.00011990235034813755\n",
      "train loss:0.00015156694583544973\n",
      "train loss:0.00013151344341156545\n",
      "train loss:0.0001981527415158183\n",
      "train loss:0.0003152635179712149\n",
      "train loss:5.299615800563317e-05\n",
      "train loss:0.00024808398398446565\n",
      "train loss:0.00027081174617142373\n",
      "train loss:7.629971680669776e-05\n",
      "train loss:0.00017213793651682443\n",
      "train loss:0.00013860519769461548\n",
      "train loss:0.0003048020222007959\n",
      "train loss:0.00024877255173663215\n",
      "train loss:0.0001262342884288773\n",
      "train loss:6.226325773132151e-05\n",
      "train loss:0.0003711971529201135\n",
      "train loss:0.00010786189213220099\n",
      "train loss:0.00026719590344409234\n",
      "train loss:0.00019054396156102817\n",
      "train loss:0.0003135614152874961\n",
      "train loss:0.0001637177481888531\n",
      "train loss:0.0003302972940730136\n",
      "train loss:0.00012160756717305352\n",
      "train loss:0.00029053263308621515\n",
      "train loss:0.00020440281747886448\n",
      "=== epoch:2, train acc:1.0, test acc:1.0 ===\n",
      "train loss:5.250596885786029e-05\n",
      "train loss:0.00014718901758471904\n",
      "train loss:0.0004148489975799224\n",
      "train loss:0.00014355948967143786\n",
      "train loss:0.00047269788417899034\n",
      "train loss:0.0005373802913430741\n",
      "train loss:0.00014621089191762217\n",
      "train loss:0.0002887116230665846\n",
      "train loss:3.632046272219904e-05\n",
      "train loss:0.0002530646677224309\n",
      "train loss:0.00028572270103168005\n",
      "train loss:0.00010590245123043624\n",
      "train loss:6.730191096011116e-05\n",
      "train loss:0.00016389960105002695\n",
      "train loss:6.486081727402393e-05\n",
      "train loss:8.803340792170026e-05\n",
      "train loss:0.0001288944320548639\n",
      "train loss:0.00014292208858395194\n",
      "train loss:0.00010789432186305234\n",
      "train loss:0.00013583723208400954\n",
      "train loss:0.00011619748339007456\n",
      "train loss:0.00015135692679376985\n",
      "train loss:6.750127123611449e-05\n",
      "train loss:0.00047084942102040546\n",
      "train loss:0.0002753673250739919\n",
      "train loss:0.00022760317834567484\n",
      "train loss:0.00030305210037228644\n",
      "train loss:0.0002924325650080686\n",
      "train loss:0.00024835135027811994\n",
      "train loss:4.3822207208125015e-05\n",
      "train loss:0.00021817644250183936\n",
      "train loss:0.00012237981656000656\n",
      "train loss:0.0003670283789137306\n",
      "train loss:6.754001050574724e-05\n",
      "train loss:0.0002518051864025303\n",
      "train loss:0.0002598924519520989\n",
      "train loss:8.825806644390106e-05\n",
      "train loss:0.00027419199265458073\n",
      "train loss:0.00014460731529673102\n",
      "train loss:0.0003889055961146104\n",
      "train loss:0.0002263362469139611\n",
      "train loss:0.00017741443672257047\n",
      "train loss:0.000274543478404125\n",
      "train loss:0.0001807771570850678\n",
      "train loss:0.00021812118166516118\n",
      "train loss:0.00013012530048216254\n",
      "train loss:0.00019998869100460996\n",
      "train loss:0.00010787684885025937\n",
      "train loss:6.639334610915529e-05\n",
      "train loss:0.00013920231224004076\n",
      "train loss:6.95489836215106e-05\n",
      "train loss:0.0004915612478191971\n",
      "train loss:0.00016426273801145415\n",
      "train loss:3.952189410436618e-05\n",
      "train loss:0.00031458062098583\n",
      "train loss:0.00045315101394285463\n",
      "train loss:0.00011354662661808451\n",
      "train loss:0.0002062299992815417\n",
      "train loss:0.00047187193337005535\n",
      "train loss:0.00017601514300858698\n",
      "train loss:0.00015009447343912325\n",
      "train loss:0.00018854749451417498\n",
      "train loss:0.00017278479120432053\n",
      "train loss:8.824620089978587e-05\n",
      "train loss:0.00029243937297343943\n",
      "train loss:0.0005437463740149609\n",
      "train loss:0.0002329620524043394\n",
      "train loss:0.0001921853589335439\n",
      "train loss:0.0001506975299027893\n",
      "train loss:0.00018030945489124677\n",
      "train loss:0.0001657894510438742\n",
      "train loss:0.0005901619078374306\n",
      "train loss:6.70541083640049e-05\n",
      "train loss:0.00030830501167268337\n",
      "train loss:9.008225566963811e-05\n",
      "train loss:0.0003312103212987846\n",
      "train loss:0.0001443670022456365\n",
      "train loss:0.00011839135888793043\n",
      "train loss:0.00010190414500367766\n",
      "train loss:0.00018482736011556322\n",
      "=== epoch:3, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00010401734631684757\n",
      "train loss:8.867703184220017e-05\n",
      "train loss:0.00024609836495509353\n",
      "train loss:0.00015480770634461073\n",
      "train loss:5.668408981771777e-05\n",
      "train loss:6.426455617384176e-05\n",
      "train loss:0.00028149203648254864\n",
      "train loss:0.00016720636534605203\n",
      "train loss:0.00011661634407799876\n",
      "train loss:0.00010336265962270728\n",
      "train loss:0.00025371331566846454\n",
      "train loss:8.714791971634938e-05\n",
      "train loss:9.12105001229775e-05\n",
      "train loss:8.787413916180006e-05\n",
      "train loss:0.0001521438549372597\n",
      "train loss:0.00017908803437221226\n",
      "train loss:0.0002446143603926294\n",
      "train loss:0.0002768406758245916\n",
      "train loss:0.00010205234584840377\n",
      "train loss:0.0004459766727539434\n",
      "train loss:0.0003017754906478837\n",
      "train loss:2.5532950068364847e-05\n",
      "train loss:5.248147018766613e-05\n",
      "train loss:0.00027580988365371554\n",
      "train loss:0.0004191562073547493\n",
      "train loss:6.786616334280105e-05\n",
      "train loss:0.00027981506923663513\n",
      "train loss:0.0002954947449074811\n",
      "train loss:6.071023002766972e-05\n",
      "train loss:0.0002424644576732893\n",
      "train loss:0.00031161093054480954\n",
      "train loss:0.00023955277783189742\n",
      "train loss:0.0001422583729492834\n",
      "train loss:0.0004876958052746381\n",
      "train loss:4.9650028823492435e-05\n",
      "train loss:0.00010756223376173447\n",
      "train loss:0.00021159048921637414\n",
      "train loss:0.00014797357229369518\n",
      "train loss:0.00031852527347177024\n",
      "train loss:0.00016098194468259477\n",
      "train loss:0.00010968200554578922\n",
      "train loss:0.00021874028540206934\n",
      "train loss:0.0005176726566958306\n",
      "train loss:0.0006069987529665322\n",
      "train loss:0.00016444016144941253\n",
      "train loss:0.00015006124914047547\n",
      "train loss:0.00016392733400589655\n",
      "train loss:0.0003755388617589432\n",
      "train loss:8.838196703870678e-05\n",
      "train loss:8.130325445814846e-05\n",
      "train loss:4.319252110666275e-05\n",
      "train loss:0.00024888978020748393\n",
      "train loss:0.00011149999880987789\n",
      "train loss:0.00016982611631524477\n",
      "train loss:0.00040766476470303377\n",
      "train loss:0.0001367909196384637\n",
      "train loss:0.0001327904441605235\n",
      "train loss:0.0002215878234266582\n",
      "train loss:0.000435431018572659\n",
      "train loss:8.012422183165353e-05\n",
      "train loss:0.00046521329297461285\n",
      "train loss:2.6385900606608897e-05\n",
      "train loss:0.0001044907008457458\n",
      "train loss:9.476175572587482e-05\n",
      "train loss:0.00012723168313346343\n",
      "train loss:0.00014086602710792212\n",
      "train loss:0.00023523334333315057\n",
      "train loss:0.0004474832073622921\n",
      "train loss:9.508084412283387e-05\n",
      "train loss:0.00010990810783986833\n",
      "train loss:0.0006046592219246141\n",
      "train loss:5.81995575998507e-05\n",
      "train loss:0.0003379006342722625\n",
      "train loss:0.00019164339795524606\n",
      "train loss:0.00013405186727514463\n",
      "train loss:7.12082700025114e-05\n",
      "train loss:0.0002568383351310286\n",
      "train loss:0.0001573545766184636\n",
      "train loss:0.00038307062826630926\n",
      "train loss:0.00011955528574434288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:4, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00014711842767316368\n",
      "train loss:0.00023866849694985544\n",
      "train loss:9.539588705133233e-05\n",
      "train loss:0.0003642668945347486\n",
      "train loss:0.00035005341840653996\n",
      "train loss:0.0001911396021465712\n",
      "train loss:0.00019322484267049898\n",
      "train loss:0.00025419635560505266\n",
      "train loss:4.694823776636203e-05\n",
      "train loss:0.00015524087251379785\n",
      "train loss:0.00012537171489288232\n",
      "train loss:2.5590836371096306e-05\n",
      "train loss:2.596123023270493e-05\n",
      "train loss:0.00010227833077955879\n",
      "train loss:0.000259661795896373\n",
      "train loss:4.3323726893867606e-05\n",
      "train loss:0.0001512373490828764\n",
      "train loss:0.00025186213389473996\n",
      "train loss:0.0005631584996823505\n",
      "train loss:0.00023937557970782783\n",
      "train loss:0.0001355915325234093\n",
      "train loss:0.0004954898699732923\n",
      "train loss:0.00036066440396491546\n",
      "train loss:8.932799200338394e-05\n",
      "train loss:7.512636245603876e-05\n",
      "train loss:0.00014206987293388437\n",
      "train loss:0.00029178701036537463\n",
      "train loss:0.00019604715022364052\n",
      "train loss:0.0004917186993738037\n",
      "train loss:0.00028613007129467676\n",
      "train loss:7.580219310555389e-05\n",
      "train loss:4.8670086906159305e-05\n",
      "train loss:0.00022402064912017944\n",
      "train loss:2.3357123781499245e-05\n",
      "train loss:0.0005068472028064998\n",
      "train loss:9.695423207574409e-05\n",
      "train loss:0.00031382119107356934\n",
      "train loss:0.00036008274337811336\n",
      "train loss:0.0005090050721254643\n",
      "train loss:0.00016223632996936163\n",
      "train loss:0.00015217301799150352\n",
      "train loss:0.00018816463197967296\n",
      "train loss:9.019123908616533e-05\n",
      "train loss:0.00012492591481030914\n",
      "train loss:0.0001746054925683009\n",
      "train loss:0.00019747594959088473\n",
      "train loss:0.00041051714586366137\n",
      "train loss:0.00015779724103139503\n",
      "train loss:0.00010724548754093321\n",
      "train loss:3.2076972477112695e-05\n",
      "train loss:0.000355125018001632\n",
      "train loss:0.00022435301314129737\n",
      "train loss:0.00032369658736390117\n",
      "train loss:7.752756879780888e-05\n",
      "train loss:0.0004202216443330687\n",
      "train loss:0.00011798817449460889\n",
      "train loss:0.00021682464756804488\n",
      "train loss:0.0002936841911561079\n",
      "train loss:0.00011940536390108042\n",
      "train loss:0.0002699488297718682\n",
      "train loss:0.00010699737634371951\n",
      "train loss:9.296216535109009e-05\n",
      "train loss:0.0003230656589446155\n",
      "train loss:4.473036213945645e-05\n",
      "train loss:0.00047725398728696146\n",
      "train loss:0.00019367660625168207\n",
      "train loss:0.0001129507811259542\n",
      "train loss:0.00019244514622667982\n",
      "train loss:0.000195276270108439\n",
      "train loss:9.06209274420276e-05\n",
      "train loss:7.961792162076701e-05\n",
      "train loss:0.00026761824620236954\n",
      "train loss:0.0001430608241859441\n",
      "train loss:0.0002730979033664021\n",
      "train loss:0.00024173632537181165\n",
      "train loss:0.00016231125331095725\n",
      "train loss:0.000210684580097595\n",
      "train loss:0.00017579377813476248\n",
      "train loss:4.927248265759364e-05\n",
      "train loss:0.0002074075387182674\n",
      "=== epoch:5, train acc:1.0, test acc:1.0 ===\n",
      "train loss:9.912714361006216e-05\n",
      "train loss:0.00026871710481945034\n",
      "train loss:3.9030612831795046e-05\n",
      "train loss:0.0003874068317136003\n",
      "train loss:0.0002712378384265406\n",
      "train loss:0.00010190102850423716\n",
      "train loss:0.00010160185303001729\n",
      "train loss:0.00011663871616570553\n",
      "train loss:4.8947346528063444e-05\n",
      "train loss:1.6278101723601292e-05\n",
      "train loss:9.771228489275694e-05\n",
      "train loss:2.8901304413869464e-05\n",
      "train loss:0.00016013293221133038\n",
      "train loss:0.00014515340558963043\n",
      "train loss:0.0002774398958740853\n",
      "train loss:5.728061166624236e-05\n",
      "train loss:9.985651687642351e-05\n",
      "train loss:0.00031517440456766267\n",
      "train loss:0.00039700658427738807\n",
      "train loss:0.0001672264650475373\n",
      "train loss:0.00020093671926089753\n",
      "train loss:0.00027002416270630156\n",
      "train loss:0.00024524330772376186\n",
      "train loss:0.0002746847746713313\n",
      "train loss:0.0005551842639540052\n",
      "train loss:0.00016869532260382057\n",
      "train loss:0.00021031484853756342\n",
      "train loss:0.00017604625247191204\n",
      "train loss:0.0001051572652356763\n",
      "train loss:3.985719344862554e-05\n",
      "train loss:0.00020741167153487768\n",
      "train loss:0.00028184912798160093\n",
      "train loss:0.0001452295672254129\n",
      "train loss:8.844881662593184e-05\n",
      "train loss:0.00013722952283843143\n",
      "train loss:0.00012143515153830995\n",
      "train loss:0.00015727637701990618\n",
      "train loss:0.00012908544300058993\n",
      "train loss:5.7944322649556134e-05\n",
      "train loss:7.618921940161768e-05\n",
      "train loss:7.421913733332501e-05\n",
      "train loss:0.00026878438971338786\n",
      "train loss:0.0002502466259610696\n",
      "train loss:0.000148407221474107\n",
      "train loss:0.0003510545094972993\n",
      "train loss:0.0007461782610231689\n",
      "train loss:0.0002731945469433433\n",
      "train loss:7.720964722248992e-05\n",
      "train loss:0.00011636622884446745\n",
      "train loss:0.0001384675318882895\n",
      "train loss:0.00025932071812377003\n",
      "train loss:0.0002691780870496146\n",
      "train loss:7.862128517965392e-05\n",
      "train loss:0.00018387300829346442\n",
      "train loss:0.00022945810467111738\n",
      "train loss:0.0003438956900362153\n",
      "train loss:6.0114462574955995e-05\n",
      "train loss:0.000237115786143971\n",
      "train loss:0.0003261969575430291\n",
      "train loss:9.237494277357749e-05\n",
      "train loss:0.00016315632460951965\n",
      "train loss:7.467819851595604e-05\n",
      "train loss:0.00010269438094067851\n",
      "train loss:0.0001476266857162305\n",
      "train loss:8.606483771180312e-05\n",
      "train loss:0.00022645509080953082\n",
      "train loss:0.00022735325976905072\n",
      "train loss:0.000436624137582151\n",
      "train loss:7.332736002684687e-05\n",
      "train loss:0.00012503062418802626\n",
      "train loss:0.00020825431885904456\n",
      "train loss:4.773642061997586e-05\n",
      "train loss:0.00043256945533549993\n",
      "train loss:7.891420484756438e-05\n",
      "train loss:0.00011593788281482746\n",
      "train loss:0.00035657757293493513\n",
      "train loss:0.0002752818944552424\n",
      "train loss:0.0001070020183908391\n",
      "train loss:0.0002528534567445421\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:1.0\n",
      "train loss:0.00013151274744164116\n",
      "=== epoch:1, train acc:1.0, test acc:1.0 ===\n",
      "train loss:9.152044130315497e-05\n",
      "train loss:0.00015600436653057505\n",
      "train loss:0.0001717990471125765\n",
      "train loss:0.0001210922791505791\n",
      "train loss:0.0003143920656805749\n",
      "train loss:8.412807593239162e-05\n",
      "train loss:0.00021315954437457142\n",
      "train loss:4.5700584532356525e-05\n",
      "train loss:0.00014295004981651255\n",
      "train loss:0.00041830328452609753\n",
      "train loss:0.0001258665581091343\n",
      "train loss:0.00021568997532378956\n",
      "train loss:0.00017929846606990738\n",
      "train loss:0.0004846670949816645\n",
      "train loss:0.0002605991417730964\n",
      "train loss:3.797930757296945e-05\n",
      "train loss:0.0001676056747762811\n",
      "train loss:9.030299948497536e-05\n",
      "train loss:0.00022226145555739548\n",
      "train loss:0.00011021577783875397\n",
      "train loss:0.0002186937024842503\n",
      "train loss:0.00012315853994393893\n",
      "train loss:0.00022304129084442192\n",
      "train loss:0.00017028459387058526\n",
      "train loss:0.0005022425083714175\n",
      "train loss:0.00028804613433002413\n",
      "train loss:0.00016617579197438192\n",
      "train loss:0.0003014616340122975\n",
      "train loss:0.00012856964039513762\n",
      "train loss:0.00033075906406699647\n",
      "train loss:7.579517272083258e-05\n",
      "train loss:9.96717097369294e-05\n",
      "train loss:0.00015788881820042892\n",
      "train loss:0.00013340478684613965\n",
      "train loss:0.0005023411074187622\n",
      "train loss:0.0001378911849098775\n",
      "train loss:0.00010257615261605646\n",
      "train loss:0.00018762217260360594\n",
      "train loss:0.00012713497060824394\n",
      "train loss:0.0005840197744123458\n",
      "train loss:0.00016895957997401287\n",
      "train loss:0.00023447606939788746\n",
      "train loss:8.195272105351107e-05\n",
      "train loss:0.00038788642576223996\n",
      "train loss:2.7226782955260508e-05\n",
      "train loss:0.00016235104191198262\n",
      "train loss:0.00013348843944591574\n",
      "train loss:0.00013298357356842025\n",
      "train loss:4.286439728962584e-05\n",
      "train loss:0.00010044612448788168\n",
      "train loss:0.00011801980309811116\n",
      "train loss:0.00018725110187666448\n",
      "train loss:0.00014210076334366458\n",
      "train loss:5.2582057877749406e-05\n",
      "train loss:0.0004179778200634276\n",
      "train loss:0.00011128530122950167\n",
      "train loss:0.00032780082088836336\n",
      "train loss:0.000374501880747155\n",
      "train loss:0.00023114732954783288\n",
      "train loss:4.5249851499824806e-05\n",
      "train loss:0.0002128984561377539\n",
      "train loss:9.128097201999735e-05\n",
      "train loss:7.405639116395044e-05\n",
      "train loss:5.590216982180795e-05\n",
      "train loss:0.000180857436293171\n",
      "train loss:0.00012736236947248023\n",
      "train loss:0.00018457587183699222\n",
      "train loss:0.00034496483758933135\n",
      "train loss:0.00012896666127435583\n",
      "train loss:0.00041089527717434954\n",
      "train loss:0.00023232537099239788\n",
      "train loss:0.0002223206441765802\n",
      "train loss:0.00014040940604737384\n",
      "train loss:2.6237786228262396e-05\n",
      "train loss:0.00030377904260839647\n",
      "train loss:0.00014656469688883826\n",
      "train loss:0.0003144892922653395\n",
      "train loss:0.00023340528642793406\n",
      "train loss:0.00019827074205700572\n",
      "train loss:0.00035750550818508515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:2, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0004607345923572413\n",
      "train loss:0.00012532728986880967\n",
      "train loss:0.0002490698965860099\n",
      "train loss:0.00010906871604792654\n",
      "train loss:0.0003076575952233786\n",
      "train loss:0.00015782385230785423\n",
      "train loss:0.00012473205170865763\n",
      "train loss:0.00021061746184226036\n",
      "train loss:0.00018900912111661464\n",
      "train loss:0.00034147744577146257\n",
      "train loss:0.00013518683244455603\n",
      "train loss:8.925892531455297e-05\n",
      "train loss:0.00017651534506626097\n",
      "train loss:0.00020523207872117418\n",
      "train loss:4.983410155360945e-05\n",
      "train loss:0.0001710429609002127\n",
      "train loss:0.000141457882515456\n",
      "train loss:0.00011217732592827878\n",
      "train loss:0.00015461559106736322\n",
      "train loss:0.000236717994717032\n",
      "train loss:0.00025308358726618824\n",
      "train loss:0.00010713044086662425\n",
      "train loss:0.0005145596372332189\n",
      "train loss:0.00021771971166213355\n",
      "train loss:0.00017146269961409598\n",
      "train loss:4.5120939347479414e-05\n",
      "train loss:0.00020727259893597774\n",
      "train loss:0.0001379516838662614\n",
      "train loss:0.00017090655133174753\n",
      "train loss:0.00017781431698249777\n",
      "train loss:0.00017582681139678217\n",
      "train loss:0.00015171179732148706\n",
      "train loss:9.034591435546587e-05\n",
      "train loss:0.00016266762715654747\n",
      "train loss:0.00016326401270327833\n",
      "train loss:0.0001406948629637251\n",
      "train loss:0.00013536853633206385\n",
      "train loss:0.00010408765669858072\n",
      "train loss:0.00023838956058941472\n",
      "train loss:0.00031677277065077556\n",
      "train loss:0.00018726772414182888\n",
      "train loss:0.00010064657939711159\n",
      "train loss:9.472708139296449e-05\n",
      "train loss:0.00025763671722377207\n",
      "train loss:5.900979843021802e-05\n",
      "train loss:0.00019821677008705988\n",
      "train loss:8.389964232437375e-05\n",
      "train loss:4.3592972376259324e-05\n",
      "train loss:5.590903051437064e-05\n",
      "train loss:0.00015438063077200532\n",
      "train loss:0.0004065336494797727\n",
      "train loss:0.00011598597286498908\n",
      "train loss:0.00021183200443835752\n",
      "train loss:0.0002725577788377308\n",
      "train loss:0.0002552946486044313\n",
      "train loss:0.00011382650573333302\n",
      "train loss:6.515995243494549e-05\n",
      "train loss:0.00017985158280682127\n",
      "train loss:0.0003589739815413432\n",
      "train loss:0.0005233949594371749\n",
      "train loss:0.0002569824276795241\n",
      "train loss:0.00018213542212016908\n",
      "train loss:0.00013069955787421678\n",
      "train loss:0.000311265931946128\n",
      "train loss:5.6583535549924113e-05\n",
      "train loss:0.0006233687946412604\n",
      "train loss:0.000268373242389912\n",
      "train loss:0.00013290702272926468\n",
      "train loss:0.00012708952402305237\n",
      "train loss:0.00017874252873142736\n",
      "train loss:0.00016085473509425878\n",
      "train loss:0.00034170126801692227\n",
      "train loss:0.00019609464608335471\n",
      "train loss:0.00029249813435107077\n",
      "train loss:0.00014461542585732638\n",
      "train loss:0.00012855902596269856\n",
      "train loss:0.00026879171648709204\n",
      "train loss:0.00017300741117751893\n",
      "train loss:0.0001380356431161759\n",
      "train loss:0.0001266471047695291\n",
      "=== epoch:3, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00011391490424150968\n",
      "train loss:8.903381526646062e-05\n",
      "train loss:0.0001285908174262588\n",
      "train loss:0.00016758745438143568\n",
      "train loss:7.566067284686522e-05\n",
      "train loss:7.808373126911608e-05\n",
      "train loss:0.00017907465844850906\n",
      "train loss:0.00010636600717783208\n",
      "train loss:0.00010933855166955015\n",
      "train loss:0.00010151381854181457\n",
      "train loss:0.00042889904489644816\n",
      "train loss:0.00022092343891915903\n",
      "train loss:0.00016335340981591523\n",
      "train loss:5.664709858608085e-05\n",
      "train loss:0.00027455368385222433\n",
      "train loss:0.00010904219925351571\n",
      "train loss:0.0002223823430383083\n",
      "train loss:0.00013452510514104022\n",
      "train loss:6.858139116189027e-05\n",
      "train loss:9.835578735812818e-05\n",
      "train loss:0.00011388264598887173\n",
      "train loss:0.00012738397067176251\n",
      "train loss:0.0003707753375885377\n",
      "train loss:0.0002587238121791837\n",
      "train loss:0.0001565268599421981\n",
      "train loss:6.937345795393893e-05\n",
      "train loss:0.0003404450666774272\n",
      "train loss:0.00012830449709823663\n",
      "train loss:0.00020158140235533524\n",
      "train loss:0.0001972525607893748\n",
      "train loss:0.00023541868013925767\n",
      "train loss:0.0004127262835624123\n",
      "train loss:0.0004947546332304123\n",
      "train loss:0.0004259296597797849\n",
      "train loss:7.952543770345834e-05\n",
      "train loss:0.00015188134158020865\n",
      "train loss:0.0002932381264719359\n",
      "train loss:0.00015237284785435726\n",
      "train loss:0.00029583161563337374\n",
      "train loss:0.0003781173866289646\n",
      "train loss:0.00020545621416671279\n",
      "train loss:0.00010899096875344835\n",
      "train loss:0.0003202639999503773\n",
      "train loss:0.00010823393808937978\n",
      "train loss:0.0002425518052719974\n",
      "train loss:0.00024056970715126899\n",
      "train loss:9.441994412892995e-05\n",
      "train loss:0.0002618440250602793\n",
      "train loss:0.00022972894232788066\n",
      "train loss:0.00027830271963862204\n",
      "train loss:0.00010945584620517166\n",
      "train loss:0.0001487085234154385\n",
      "train loss:0.0004133140125210135\n",
      "train loss:0.00011700998646989046\n",
      "train loss:0.0005031176879201539\n",
      "train loss:0.00014704266296372613\n",
      "train loss:0.00010156463953084677\n",
      "train loss:0.0002624217164757731\n",
      "train loss:0.00014466807271546148\n",
      "train loss:0.00018845015708314562\n",
      "train loss:3.150668630185565e-05\n",
      "train loss:0.0002355741888930512\n",
      "train loss:0.0002110897516709575\n",
      "train loss:0.0001373421190483802\n",
      "train loss:0.00026987626949492757\n",
      "train loss:0.0003324579568924632\n",
      "train loss:0.00043932461995708274\n",
      "train loss:4.1000168154626434e-05\n",
      "train loss:0.00014678825202687378\n",
      "train loss:0.00013827788939647673\n",
      "train loss:6.006271253152533e-05\n",
      "train loss:0.00016083108572354995\n",
      "train loss:0.0002958318155814147\n",
      "train loss:0.00022578625520183114\n",
      "train loss:5.509154386545179e-05\n",
      "train loss:0.00014045695007587543\n",
      "train loss:0.00032595434025052225\n",
      "train loss:4.898301919441529e-05\n",
      "train loss:0.0004204194744119527\n",
      "train loss:0.0002606528432086785\n",
      "=== epoch:4, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00020915355149750265\n",
      "train loss:6.407784763452796e-05\n",
      "train loss:0.00010211449464364021\n",
      "train loss:0.0002938008697832681\n",
      "train loss:0.00011976168566426722\n",
      "train loss:0.00017389017728069862\n",
      "train loss:0.0001415714081098499\n",
      "train loss:0.00013021808455311983\n",
      "train loss:0.00016245050467231616\n",
      "train loss:0.0001605210578062446\n",
      "train loss:0.00017829907116492344\n",
      "train loss:0.00015394171207284488\n",
      "train loss:0.0001307504920142518\n",
      "train loss:0.0004434117034208554\n",
      "train loss:0.0003229174937569557\n",
      "train loss:8.552574264333006e-05\n",
      "train loss:0.0001842811584542083\n",
      "train loss:0.00014104307316165424\n",
      "train loss:5.358084317769361e-05\n",
      "train loss:0.00013865733369495604\n",
      "train loss:0.00019633195915504147\n",
      "train loss:7.046948194261958e-05\n",
      "train loss:0.00016554804889687224\n",
      "train loss:0.00011384128664375228\n",
      "train loss:4.962959544433113e-05\n",
      "train loss:0.00019510857359345365\n",
      "train loss:0.00018252447469018358\n",
      "train loss:4.297212529208051e-05\n",
      "train loss:0.00016624403366493343\n",
      "train loss:0.000203147331235802\n",
      "train loss:0.00026898523284938215\n",
      "train loss:6.122627277420986e-05\n",
      "train loss:0.00022180886838793108\n",
      "train loss:0.00020121387237516806\n",
      "train loss:0.0003904366657409206\n",
      "train loss:0.00047238723623193305\n",
      "train loss:7.257552051651292e-05\n",
      "train loss:3.9382519773055736e-05\n",
      "train loss:0.0003103734577730685\n",
      "train loss:0.0002617280010587305\n",
      "train loss:0.0001629796885613993\n",
      "train loss:5.8826493738868606e-05\n",
      "train loss:0.0001981759351873879\n",
      "train loss:4.135821788392064e-05\n",
      "train loss:0.00024509766236175757\n",
      "train loss:0.0001646510701201929\n",
      "train loss:0.00015005372266172162\n",
      "train loss:0.00013997893320089036\n",
      "train loss:0.00011744240543641653\n",
      "train loss:0.00013524846768857068\n",
      "train loss:0.000402959905199014\n",
      "train loss:7.739877845959291e-05\n",
      "train loss:0.00017769557106397284\n",
      "train loss:0.00013593360017052705\n",
      "train loss:0.0001736286537921355\n",
      "train loss:0.00010851062655589072\n",
      "train loss:6.480111413640027e-05\n",
      "train loss:8.978675387569036e-05\n",
      "train loss:0.00036536204951862985\n",
      "train loss:0.00025196953330834795\n",
      "train loss:0.00026366376307721266\n",
      "train loss:0.0001649670852142374\n",
      "train loss:0.00012070429057938708\n",
      "train loss:8.217182970520729e-05\n",
      "train loss:0.00010142793684544428\n",
      "train loss:0.00016130295486196274\n",
      "train loss:4.101382065196721e-05\n",
      "train loss:0.00014611417196039918\n",
      "train loss:0.0001459241022141025\n",
      "train loss:0.000467176672970815\n",
      "train loss:0.00019384846345381183\n",
      "train loss:0.00021425626293476615\n",
      "train loss:0.0001794616553250367\n",
      "train loss:8.022499639217878e-05\n",
      "train loss:0.00019246805320598956\n",
      "train loss:9.218531563424599e-05\n",
      "train loss:0.000231223485331664\n",
      "train loss:4.332072678710972e-05\n",
      "train loss:9.726837185729722e-05\n",
      "train loss:8.821437852314948e-05\n",
      "=== epoch:5, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0002953591179278701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0003232802164972423\n",
      "train loss:4.743771862869859e-05\n",
      "train loss:0.00024385392032518112\n",
      "train loss:0.0001401396907493441\n",
      "train loss:0.00024213725434649118\n",
      "train loss:0.00017120009250446412\n",
      "train loss:1.698426229581299e-05\n",
      "train loss:0.00012891549061052704\n",
      "train loss:0.0002087927718914973\n",
      "train loss:0.00019000708901387156\n",
      "train loss:4.410900913586485e-05\n",
      "train loss:0.00012936991688074594\n",
      "train loss:0.00024118985254583978\n",
      "train loss:0.00021425173654763958\n",
      "train loss:0.000174521194477502\n",
      "train loss:7.746322102273207e-05\n",
      "train loss:0.00010477163662587111\n",
      "train loss:0.00016471963879903824\n",
      "train loss:0.00028899108427203135\n",
      "train loss:0.00017960591001658138\n",
      "train loss:8.768653765879902e-05\n",
      "train loss:0.00017798430312966702\n",
      "train loss:8.12807734715106e-05\n",
      "train loss:0.00023194636391252668\n",
      "train loss:0.00021369375498665708\n",
      "train loss:0.0001306857735756501\n",
      "train loss:0.00015481437977978257\n",
      "train loss:0.00017038744675024987\n",
      "train loss:0.00021190847690795358\n",
      "train loss:9.335852908322336e-05\n",
      "train loss:5.391410586115209e-05\n",
      "train loss:6.632400292823953e-05\n",
      "train loss:0.00018721267331813868\n",
      "train loss:0.00014297310440034215\n",
      "train loss:5.927995568309228e-05\n",
      "train loss:0.00010301958801620747\n",
      "train loss:0.00011143704745208993\n",
      "train loss:0.00024217453175987878\n",
      "train loss:0.00020561924445508192\n",
      "train loss:8.778523613525523e-05\n",
      "train loss:0.00014672509437936197\n",
      "train loss:0.00039485415090564094\n",
      "train loss:8.180823838988887e-05\n",
      "train loss:7.73919388802258e-05\n",
      "train loss:0.00011666167433415065\n",
      "train loss:0.00013322095066789364\n",
      "train loss:5.37800159646699e-05\n",
      "train loss:9.548901938163603e-05\n",
      "train loss:7.269695868921681e-05\n",
      "train loss:0.00018754565182636644\n",
      "train loss:5.232345882801435e-05\n",
      "train loss:0.00015841981118707585\n",
      "train loss:7.517125131522281e-05\n",
      "train loss:0.00023438777555495022\n",
      "train loss:0.0003248848861609327\n",
      "train loss:0.00018611842007763504\n",
      "train loss:0.00018989072348209076\n",
      "train loss:0.00011616634356402667\n",
      "train loss:0.00017055586841006128\n",
      "train loss:0.0001343725060190794\n",
      "train loss:8.45954313740877e-05\n",
      "train loss:9.436368983384206e-05\n",
      "train loss:0.0001895399915730527\n",
      "train loss:9.225303642493601e-05\n",
      "train loss:0.00017450743065709274\n",
      "train loss:0.0001366694796210783\n",
      "train loss:0.00013759432984975176\n",
      "train loss:0.00023470800143471802\n",
      "train loss:0.000179596345913956\n",
      "train loss:5.389450117839604e-05\n",
      "train loss:0.0004763900240620984\n",
      "train loss:7.460318381727468e-05\n",
      "train loss:0.00018496241268962948\n",
      "train loss:4.3140069287045445e-05\n",
      "train loss:0.00035124913884552404\n",
      "train loss:0.00018014446485047458\n",
      "train loss:4.075189935307671e-05\n",
      "train loss:0.00018683406535515212\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:1.0\n"
     ]
    }
   ],
   "source": [
    "x_train=x_train_8\n",
    "x_test = x_test_8\n",
    "t_train=t_train_8\n",
    "t_test = t_test_8\n",
    "\n",
    "trainer_SGD = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='SGD', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_MMT = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Momentum', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_SGD.train()\n",
    "trainer_MMT.train()\n",
    "\n",
    "MMT_acc.append(trainer_MMT.test_acc_list[-1])\n",
    "SGD_acc.append(trainer_SGD.test_acc_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00015906105686072967\n",
      "=== epoch:1, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00014069445251766594\n",
      "train loss:0.00010170937832248576\n",
      "train loss:0.0002242572459733547\n",
      "train loss:6.58089047927087e-05\n",
      "train loss:2.361533479886407e-05\n",
      "train loss:8.107620354068183e-05\n",
      "train loss:0.00018590846252530227\n",
      "train loss:5.850213210444947e-05\n",
      "train loss:0.0001810256408973303\n",
      "train loss:0.00019222175989734873\n",
      "train loss:0.0001966379358932693\n",
      "train loss:5.583609352996381e-05\n",
      "train loss:0.00014348495496399458\n",
      "train loss:0.00031875207344445744\n",
      "train loss:0.0003951171655957885\n",
      "train loss:0.00015879112474890514\n",
      "train loss:5.663842288381391e-05\n",
      "train loss:0.00014346146014458986\n",
      "train loss:7.024509123715222e-05\n",
      "train loss:7.084856189106141e-05\n",
      "train loss:0.00020407939719068266\n",
      "train loss:0.0001568643000211625\n",
      "train loss:8.61482740112475e-05\n",
      "train loss:0.00021262568177064016\n",
      "train loss:0.00017406408551545345\n",
      "train loss:1.8079521741842974e-05\n",
      "train loss:0.0001601344594081075\n",
      "train loss:7.220291945126762e-05\n",
      "train loss:0.00024407268486532996\n",
      "train loss:0.0002479400452180514\n",
      "train loss:0.00019648486393264208\n",
      "train loss:5.344164307993352e-05\n",
      "train loss:0.0001834510255856056\n",
      "train loss:0.0001213934719548302\n",
      "train loss:0.0002190716893361955\n",
      "train loss:9.228750885245073e-05\n",
      "train loss:0.00023179792354479585\n",
      "train loss:0.00035180360827314066\n",
      "train loss:0.0001784910964796477\n",
      "train loss:0.00011015066004439777\n",
      "train loss:0.0002792903783712711\n",
      "train loss:9.593893440701364e-05\n",
      "train loss:0.0003726956396817606\n",
      "train loss:0.0002041263836561005\n",
      "train loss:0.00024270477954772254\n",
      "train loss:0.00011735457773274736\n",
      "train loss:0.00016673316905145485\n",
      "train loss:5.2924456769326345e-05\n",
      "train loss:0.0003954454771188638\n",
      "train loss:0.0001444603520003256\n",
      "train loss:0.00023610973100320895\n",
      "train loss:4.1746891322561795e-05\n",
      "train loss:0.00012344282065397278\n",
      "train loss:0.000210602894377459\n",
      "train loss:0.0001557610902917877\n",
      "train loss:0.00024422086290361577\n",
      "train loss:0.000144538008423949\n",
      "train loss:0.00017696554771897446\n",
      "train loss:0.0001973980557535609\n",
      "train loss:0.00026563981754718785\n",
      "train loss:0.0002724976599083191\n",
      "train loss:0.00018984179953521305\n",
      "train loss:0.00013876195642335655\n",
      "train loss:0.00017196163396084464\n",
      "train loss:0.00011731131486485087\n",
      "train loss:0.00013647718086355327\n",
      "train loss:0.00034101310566329065\n",
      "train loss:0.0003021034805234907\n",
      "train loss:7.694557442687189e-05\n",
      "train loss:7.037286680070245e-05\n",
      "train loss:8.346968960294248e-05\n",
      "train loss:0.00024283741587811814\n",
      "train loss:3.2122449706734676e-05\n",
      "train loss:1.1313339858785007e-05\n",
      "train loss:8.896494648596615e-05\n",
      "train loss:0.00012538686067247717\n",
      "train loss:0.000503763048502697\n",
      "train loss:0.00015894991667235293\n",
      "train loss:8.823858883132443e-05\n",
      "train loss:0.00019208522430150696\n",
      "=== epoch:2, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00014173532123356472\n",
      "train loss:0.00011831512968309154\n",
      "train loss:0.0002277672681909687\n",
      "train loss:0.0003134978380097082\n",
      "train loss:0.0002470333189803614\n",
      "train loss:0.00015058356298809837\n",
      "train loss:5.214279416128106e-05\n",
      "train loss:0.000170515628728745\n",
      "train loss:0.00028762398549390386\n",
      "train loss:0.0003413825221402727\n",
      "train loss:0.000297777186452879\n",
      "train loss:9.974126148982845e-05\n",
      "train loss:0.00017723228659065587\n",
      "train loss:4.077689640739681e-05\n",
      "train loss:0.00013331006381399522\n",
      "train loss:0.0001631487563573588\n",
      "train loss:0.00011678557186111928\n",
      "train loss:0.00017752757169987572\n",
      "train loss:3.385597021230285e-05\n",
      "train loss:0.00011349164599792787\n",
      "train loss:0.00014357285483891768\n",
      "train loss:0.00013010440582134056\n",
      "train loss:0.00016584977076774938\n",
      "train loss:0.00018861006705334465\n",
      "train loss:0.00020381471070584644\n",
      "train loss:0.0002848094741808689\n",
      "train loss:0.00025504468242590155\n",
      "train loss:0.00018640441364527966\n",
      "train loss:0.0001633514397744738\n",
      "train loss:0.00014183257069245116\n",
      "train loss:0.00017868484203321404\n",
      "train loss:0.00021161617385686185\n",
      "train loss:0.00012647110232199882\n",
      "train loss:8.977929575983146e-05\n",
      "train loss:0.00010045561770578661\n",
      "train loss:5.2302272020117235e-05\n",
      "train loss:0.0005340542153191822\n",
      "train loss:0.00016851357758621428\n",
      "train loss:0.00018573732752670544\n",
      "train loss:0.00010409774668060904\n",
      "train loss:0.00018800894658297046\n",
      "train loss:0.00025385911039018776\n",
      "train loss:0.00016461475156143575\n",
      "train loss:0.00014038883339818827\n",
      "train loss:8.380387817158925e-05\n",
      "train loss:4.9869728900582185e-05\n",
      "train loss:0.0001703964771552106\n",
      "train loss:0.00014442792877059128\n",
      "train loss:0.00028943983637115084\n",
      "train loss:0.00033790063715786995\n",
      "train loss:0.00021231764228612078\n",
      "train loss:0.00025325405401078544\n",
      "train loss:0.00019307880750949277\n",
      "train loss:7.622580916397345e-05\n",
      "train loss:0.00021475915842966234\n",
      "train loss:9.526878220468022e-05\n",
      "train loss:0.00023211096340074644\n",
      "train loss:0.00017405264546396825\n",
      "train loss:0.00010200106048545878\n",
      "train loss:7.212345761816118e-05\n",
      "train loss:0.0003947827181565482\n",
      "train loss:0.00032549040928401906\n",
      "train loss:0.00018019144746502243\n",
      "train loss:9.344403320661406e-05\n",
      "train loss:4.400144387808696e-05\n",
      "train loss:0.0003509998527695507\n",
      "train loss:0.0002744348213206064\n",
      "train loss:8.312528312590228e-05\n",
      "train loss:0.00031959154969419466\n",
      "train loss:0.00015250438745746146\n",
      "train loss:0.00045592540569796056\n",
      "train loss:0.00025290555660126556\n",
      "train loss:0.00019947077255979447\n",
      "train loss:0.00033506416038863905\n",
      "train loss:0.00018981975831823411\n",
      "train loss:0.00020394286990722227\n",
      "train loss:0.00024445038867326524\n",
      "train loss:0.00021161653702787765\n",
      "train loss:0.00017338109867945196\n",
      "train loss:6.193743413924423e-05\n",
      "=== epoch:3, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00021768931065298483\n",
      "train loss:0.00021692336209353828\n",
      "train loss:0.00020253071296857367\n",
      "train loss:0.00017097963742645694\n",
      "train loss:0.00012401315985904527\n",
      "train loss:0.00032263472492263275\n",
      "train loss:0.00018588336184921741\n",
      "train loss:0.000373718236151697\n",
      "train loss:0.00028856552951990634\n",
      "train loss:1.6879470894902453e-05\n",
      "train loss:5.7336235687087246e-05\n",
      "train loss:1.9375947484538113e-05\n",
      "train loss:0.00010876357921872286\n",
      "train loss:3.7247119852548124e-05\n",
      "train loss:0.00014626303633620617\n",
      "train loss:9.143200766178902e-05\n",
      "train loss:0.00022217688301650763\n",
      "train loss:0.00014680523666277202\n",
      "train loss:0.0001918299279616806\n",
      "train loss:7.95244340806718e-05\n",
      "train loss:0.00029735326902653866\n",
      "train loss:0.00030418178802186856\n",
      "train loss:0.00015956126661152616\n",
      "train loss:0.00018572258105850105\n",
      "train loss:8.271813016748895e-05\n",
      "train loss:0.00020208408921976875\n",
      "train loss:0.00013448892811006614\n",
      "train loss:0.00021492642009964368\n",
      "train loss:6.824177204530671e-05\n",
      "train loss:7.762168857553242e-05\n",
      "train loss:8.908096428691905e-05\n",
      "train loss:0.00019705416209024086\n",
      "train loss:0.0001379535038085746\n",
      "train loss:0.0001566414942866369\n",
      "train loss:0.00041619974166434075\n",
      "train loss:0.0002978754503666688\n",
      "train loss:0.00010545369137362536\n",
      "train loss:0.00011468310186685123\n",
      "train loss:9.064074343585167e-05\n",
      "train loss:0.0003359894928085775\n",
      "train loss:0.0004359895296463793\n",
      "train loss:0.0002227974795694018\n",
      "train loss:0.00012710020538781616\n",
      "train loss:0.00016853406501555844\n",
      "train loss:6.035868263828989e-05\n",
      "train loss:0.00014954257777988135\n",
      "train loss:0.00017764788725384273\n",
      "train loss:7.790194326125902e-05\n",
      "train loss:0.00019655176629388606\n",
      "train loss:0.00012240505805313255\n",
      "train loss:9.938907966323485e-05\n",
      "train loss:0.00012662869594584142\n",
      "train loss:0.0002329786700590271\n",
      "train loss:0.00024322876418784626\n",
      "train loss:8.574682253252746e-05\n",
      "train loss:0.00028975890939149997\n",
      "train loss:4.245865245781889e-05\n",
      "train loss:0.00036608776264440544\n",
      "train loss:0.00025596469521363917\n",
      "train loss:5.003041684164566e-05\n",
      "train loss:4.27994480201485e-05\n",
      "train loss:0.00014590793322000073\n",
      "train loss:0.00011288340524337871\n",
      "train loss:5.097879302463902e-05\n",
      "train loss:0.00011687035931984751\n",
      "train loss:0.00010202052055402674\n",
      "train loss:0.00023466317533790235\n",
      "train loss:0.00014547275008145808\n",
      "train loss:0.00012019008055821236\n",
      "train loss:0.00027095913167334424\n",
      "train loss:3.7650923469619974e-05\n",
      "train loss:1.0006924861977386e-05\n",
      "train loss:0.00034343479689337924\n",
      "train loss:5.700158328884388e-05\n",
      "train loss:0.0002752034410671541\n",
      "train loss:0.00014788216507641898\n",
      "train loss:0.00012704722612195146\n",
      "train loss:0.00010358453622547472\n",
      "train loss:8.387639828472097e-05\n",
      "train loss:0.00015669161091348571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:4, train acc:1.0, test acc:1.0 ===\n",
      "train loss:8.373895572382503e-05\n",
      "train loss:0.00020598423412001462\n",
      "train loss:0.00024614390898667954\n",
      "train loss:0.00016787537146524054\n",
      "train loss:4.9574973507577946e-05\n",
      "train loss:0.00024327439814220999\n",
      "train loss:5.07560291358684e-05\n",
      "train loss:0.0001245265359486246\n",
      "train loss:0.00036488155320727714\n",
      "train loss:0.0001288082118551444\n",
      "train loss:0.0001106870305893772\n",
      "train loss:0.00011805928046027327\n",
      "train loss:0.00023982325225644168\n",
      "train loss:6.657811901510155e-05\n",
      "train loss:0.00016843618231209473\n",
      "train loss:0.00016208809298049964\n",
      "train loss:0.00013369016236162483\n",
      "train loss:0.00010026690951641165\n",
      "train loss:4.5941333577936425e-05\n",
      "train loss:0.00012624972104527767\n",
      "train loss:0.00018242326451184661\n",
      "train loss:0.0001476825599510827\n",
      "train loss:9.171330973772973e-05\n",
      "train loss:0.00014303393426287828\n",
      "train loss:6.983188194679107e-05\n",
      "train loss:0.00019220740013416632\n",
      "train loss:0.00017901919293804305\n",
      "train loss:0.0001447339287498723\n",
      "train loss:0.00013953445547109348\n",
      "train loss:0.00015635261631944671\n",
      "train loss:0.00016227357790617452\n",
      "train loss:0.00011540962594066398\n",
      "train loss:0.00016118846128030255\n",
      "train loss:0.00016534038272239535\n",
      "train loss:8.722425621902746e-05\n",
      "train loss:0.00010620529509046442\n",
      "train loss:6.0693642364826495e-05\n",
      "train loss:8.38304030257939e-05\n",
      "train loss:0.0003149594337045393\n",
      "train loss:9.248165733658527e-05\n",
      "train loss:0.00018678044420103324\n",
      "train loss:0.0001206975062048424\n",
      "train loss:0.0001996508622530917\n",
      "train loss:0.00014701233840541477\n",
      "train loss:0.00037235701206569965\n",
      "train loss:0.00013247659986763785\n",
      "train loss:0.0003866272256566436\n",
      "train loss:5.5983423592660516e-05\n",
      "train loss:0.0002546373704534656\n",
      "train loss:0.0002709134338554867\n",
      "train loss:8.557409512241491e-05\n",
      "train loss:0.00020026333971244172\n",
      "train loss:0.00018006138430004868\n",
      "train loss:0.00011654817905215527\n",
      "train loss:0.0001386877195547098\n",
      "train loss:0.0003067886893493505\n",
      "train loss:0.00012414286796246852\n",
      "train loss:0.00021028617952507274\n",
      "train loss:0.00014345168657031265\n",
      "train loss:0.000208488118149343\n",
      "train loss:0.00018067616354036355\n",
      "train loss:0.000239716759790687\n",
      "train loss:0.0003266862193901195\n",
      "train loss:0.00011241478979758558\n",
      "train loss:0.00012644544939230634\n",
      "train loss:9.350447484336217e-05\n",
      "train loss:0.0002341570705733134\n",
      "train loss:0.00010162231569086483\n",
      "train loss:8.061149288309915e-05\n",
      "train loss:0.00010768516557703595\n",
      "train loss:0.00027082154513231144\n",
      "train loss:0.0003357560077296349\n",
      "train loss:0.00018492201058663299\n",
      "train loss:0.00011270421249919009\n",
      "train loss:0.00022199916489524092\n",
      "train loss:0.00016496221605555577\n",
      "train loss:0.00015481110162257383\n",
      "train loss:4.203315650056233e-05\n",
      "train loss:0.0001889069074708385\n",
      "train loss:0.0001689970704776552\n",
      "=== epoch:5, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00017865972943867899\n",
      "train loss:0.0003519203368584789\n",
      "train loss:0.00020537771106088798\n",
      "train loss:0.00010530889271602774\n",
      "train loss:0.00021607557823321165\n",
      "train loss:0.00018787328226997944\n",
      "train loss:0.0001566542672477522\n",
      "train loss:0.00012463545197685124\n",
      "train loss:0.0002549752429004174\n",
      "train loss:5.967286626201663e-05\n",
      "train loss:5.254487008346294e-05\n",
      "train loss:0.0001547749748290503\n",
      "train loss:4.658741079506974e-05\n",
      "train loss:0.00010416124422987322\n",
      "train loss:0.00011819803857081394\n",
      "train loss:0.00013767140211805607\n",
      "train loss:3.661922320764033e-05\n",
      "train loss:0.00014857448873748977\n",
      "train loss:0.00011869001280008394\n",
      "train loss:6.883149850822951e-05\n",
      "train loss:0.00019867154920544196\n",
      "train loss:0.00015552523293546516\n",
      "train loss:6.76716798018243e-05\n",
      "train loss:0.00015546987834747829\n",
      "train loss:7.7364578763642e-05\n",
      "train loss:0.00018239033620716188\n",
      "train loss:0.00013074514845629664\n",
      "train loss:9.654755362752008e-05\n",
      "train loss:0.00029778282552224124\n",
      "train loss:0.0001408526443678327\n",
      "train loss:0.00032337852315143014\n",
      "train loss:0.00014899985600513323\n",
      "train loss:0.00011222989746746993\n",
      "train loss:0.00014299293372963006\n",
      "train loss:0.00013961975377245163\n",
      "train loss:0.0002499214237256629\n",
      "train loss:0.0001908043173315529\n",
      "train loss:0.00031298409415428385\n",
      "train loss:0.00016404204914149525\n",
      "train loss:0.0002209756110851874\n",
      "train loss:0.00021318964351976972\n",
      "train loss:0.00023843604049368912\n",
      "train loss:0.0002461020135955859\n",
      "train loss:0.0001110853713230926\n",
      "train loss:0.00035590537486955814\n",
      "train loss:0.0002604378035593279\n",
      "train loss:0.00013063263934402053\n",
      "train loss:0.0001468769344936315\n",
      "train loss:9.870533963342673e-05\n",
      "train loss:0.00016133967288333155\n",
      "train loss:5.7022272528445175e-05\n",
      "train loss:7.195853470627639e-05\n",
      "train loss:0.00020602609534139976\n",
      "train loss:1.7445062402362208e-05\n",
      "train loss:0.00018722791116102726\n",
      "train loss:0.00013927806683609433\n",
      "train loss:0.00010953890534898334\n",
      "train loss:0.00012806457628729702\n",
      "train loss:0.00022370534798428143\n",
      "train loss:8.654722672126391e-05\n",
      "train loss:0.0002335963307295395\n",
      "train loss:6.10878330756426e-05\n",
      "train loss:7.20610877418716e-05\n",
      "train loss:4.68070116422729e-05\n",
      "train loss:0.00010784659018194936\n",
      "train loss:0.00012992782477784357\n",
      "train loss:6.297625911522289e-05\n",
      "train loss:0.00013935359148179585\n",
      "train loss:0.0001454394628741416\n",
      "train loss:0.0003821917743325716\n",
      "train loss:0.00019304236779576058\n",
      "train loss:0.00012971448473778326\n",
      "train loss:0.00013973547270342308\n",
      "train loss:0.00018580119750280998\n",
      "train loss:0.00019280166120016435\n",
      "train loss:0.00023050470899099805\n",
      "train loss:0.000308695175644906\n",
      "train loss:0.0002560860981793899\n",
      "train loss:0.00025724478960632293\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:1.0\n",
      "train loss:0.00031822836214176947\n",
      "=== epoch:1, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00017836030267307393\n",
      "train loss:8.60383013898931e-05\n",
      "train loss:0.00015024883448672046\n",
      "train loss:6.0764019605905945e-05\n",
      "train loss:0.0002331147714692427\n",
      "train loss:7.274851816459504e-05\n",
      "train loss:9.207818962443109e-05\n",
      "train loss:2.7409308497065632e-05\n",
      "train loss:0.00018327513846712193\n",
      "train loss:7.725293006070204e-05\n",
      "train loss:0.00011893518162788565\n",
      "train loss:0.00011141947680956835\n",
      "train loss:0.00019900917300638826\n",
      "train loss:0.00016454468256383122\n",
      "train loss:0.0004334831786101288\n",
      "train loss:0.00016631437033351698\n",
      "train loss:0.00012340027059312938\n",
      "train loss:0.0001432956835910887\n",
      "train loss:0.00038132691206569215\n",
      "train loss:0.00018426814433647622\n",
      "train loss:0.00020373212878490633\n",
      "train loss:0.00010110170476322282\n",
      "train loss:0.0001237287542766142\n",
      "train loss:0.00023850349986321141\n",
      "train loss:0.0001934767573065186\n",
      "train loss:0.0002396044509677019\n",
      "train loss:7.596912132959634e-05\n",
      "train loss:0.0003908550858516378\n",
      "train loss:0.0001124264264823349\n",
      "train loss:0.0001408667027861994\n",
      "train loss:3.5134981054907863e-05\n",
      "train loss:0.00025336514041121214\n",
      "train loss:0.00032221512587720893\n",
      "train loss:0.00020837424316003415\n",
      "train loss:0.00010542789470931791\n",
      "train loss:5.82154342671658e-05\n",
      "train loss:7.694628078270902e-05\n",
      "train loss:0.00024379341010363442\n",
      "train loss:6.169490678242513e-05\n",
      "train loss:0.0002429953466274426\n",
      "train loss:9.519851968711388e-05\n",
      "train loss:0.00039395282145075853\n",
      "train loss:8.314805378768368e-05\n",
      "train loss:0.0001757278158542504\n",
      "train loss:0.00012003448164629777\n",
      "train loss:0.0003634715191735815\n",
      "train loss:8.05962847547429e-05\n",
      "train loss:9.212247436044393e-05\n",
      "train loss:0.00017991654222894197\n",
      "train loss:0.00014727004506766926\n",
      "train loss:0.00011585299447861362\n",
      "train loss:0.00011277942041028991\n",
      "train loss:0.00016855565820856393\n",
      "train loss:0.00018762175243865808\n",
      "train loss:0.00060124460538325\n",
      "train loss:0.00011738565781944883\n",
      "train loss:7.110439262096114e-05\n",
      "train loss:0.00021147328404982516\n",
      "train loss:0.00019972443345757398\n",
      "train loss:2.5996460636795726e-05\n",
      "train loss:0.00013688782504992495\n",
      "train loss:0.00028352978958855977\n",
      "train loss:0.0002537224032714425\n",
      "train loss:7.166076038764607e-05\n",
      "train loss:0.00016749476626797498\n",
      "train loss:0.0003206126584113093\n",
      "train loss:8.434091102320122e-05\n",
      "train loss:0.0005739422018721053\n",
      "train loss:0.00030939082711743475\n",
      "train loss:0.00014972088316350795\n",
      "train loss:0.00019393140891793393\n",
      "train loss:0.00017070372932755556\n",
      "train loss:0.00014955759810488577\n",
      "train loss:0.00012659393593229026\n",
      "train loss:0.00017280270314622842\n",
      "train loss:0.00010285386507813124\n",
      "train loss:8.18551553999738e-05\n",
      "train loss:9.598382217890119e-05\n",
      "train loss:0.0002268136840261717\n",
      "train loss:7.711872324195678e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:2, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.0003165304035328429\n",
      "train loss:6.895337996674627e-05\n",
      "train loss:7.844753525281606e-05\n",
      "train loss:0.0003709297123774283\n",
      "train loss:0.00015233171525070278\n",
      "train loss:0.00016789079458226196\n",
      "train loss:0.00015489003238855798\n",
      "train loss:0.000256685129943609\n",
      "train loss:0.0001448107039346449\n",
      "train loss:0.00012925342671874488\n",
      "train loss:0.0002195494014412953\n",
      "train loss:0.00017509121973276046\n",
      "train loss:0.00022303141002518595\n",
      "train loss:0.00012032608122125464\n",
      "train loss:9.233759315733643e-05\n",
      "train loss:6.412058056694549e-05\n",
      "train loss:0.0002682611242876637\n",
      "train loss:0.00023558411681318453\n",
      "train loss:0.0004985441234385445\n",
      "train loss:0.0001541691796806288\n",
      "train loss:3.283035428068948e-05\n",
      "train loss:0.00018990117991090393\n",
      "train loss:0.00014626870452923767\n",
      "train loss:0.0001999523012794657\n",
      "train loss:7.476754505704358e-05\n",
      "train loss:0.0001601797535008249\n",
      "train loss:0.0001521181746441332\n",
      "train loss:7.41812544695621e-05\n",
      "train loss:0.0001108372010791172\n",
      "train loss:0.00013011801159866523\n",
      "train loss:3.120186771419843e-05\n",
      "train loss:0.0001812276901889808\n",
      "train loss:0.0003211310949897833\n",
      "train loss:0.0001472353072436929\n",
      "train loss:0.00013830405920723903\n",
      "train loss:0.0002785240747378116\n",
      "train loss:8.59650095082206e-05\n",
      "train loss:7.082177700963454e-05\n",
      "train loss:5.2123463824625906e-05\n",
      "train loss:0.00013667561265211467\n",
      "train loss:7.460868946117294e-05\n",
      "train loss:0.00016806672208283585\n",
      "train loss:0.00027663466619994877\n",
      "train loss:0.00031340684408082776\n",
      "train loss:0.000110808319129712\n",
      "train loss:0.00023982192811276197\n",
      "train loss:0.0002832347044063324\n",
      "train loss:0.00015421581755045003\n",
      "train loss:0.0001209207480960957\n",
      "train loss:0.00011091589171862948\n",
      "train loss:0.00012407834227201394\n",
      "train loss:0.000267637066446927\n",
      "train loss:0.0001620582726079677\n",
      "train loss:6.842656483632997e-05\n",
      "train loss:6.135280571989628e-05\n",
      "train loss:3.4100734511704736e-05\n",
      "train loss:0.0001591310808134955\n",
      "train loss:0.00035176470823287037\n",
      "train loss:0.00024505854039885163\n",
      "train loss:0.00016193584571456743\n",
      "train loss:0.00011532512388747817\n",
      "train loss:0.0001393592595021168\n",
      "train loss:0.00019140831667021903\n",
      "train loss:8.502555991475652e-05\n",
      "train loss:0.00013501124651423723\n",
      "train loss:0.00018074665744821275\n",
      "train loss:9.786047488868339e-05\n",
      "train loss:6.841307517221831e-05\n",
      "train loss:0.000295837685876762\n",
      "train loss:9.469554913342925e-05\n",
      "train loss:8.012641896470166e-05\n",
      "train loss:6.952934251292163e-05\n",
      "train loss:0.00011944976760938265\n",
      "train loss:5.418571811076487e-05\n",
      "train loss:0.00012101094913308495\n",
      "train loss:0.00025135280795550983\n",
      "train loss:2.523782759971566e-05\n",
      "train loss:0.0001511472744872616\n",
      "train loss:8.265406079224408e-05\n",
      "train loss:0.00016603011774614823\n",
      "=== epoch:3, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00023357206726428527\n",
      "train loss:9.444304834401413e-05\n",
      "train loss:0.000195060903404002\n",
      "train loss:0.00015430084278655545\n",
      "train loss:2.909507674433917e-05\n",
      "train loss:0.00020747559250457453\n",
      "train loss:0.00016480320571522517\n",
      "train loss:0.00016565830249455518\n",
      "train loss:0.0005020769541743953\n",
      "train loss:0.00019944535645764725\n",
      "train loss:0.0002488091853180932\n",
      "train loss:2.156958293690639e-05\n",
      "train loss:0.00011386453141460794\n",
      "train loss:8.876326552811828e-05\n",
      "train loss:0.00026340686375145636\n",
      "train loss:0.0002831795538935835\n",
      "train loss:0.000137629719627909\n",
      "train loss:4.780174736860852e-05\n",
      "train loss:0.00012141012577504005\n",
      "train loss:0.00010640006967536077\n",
      "train loss:0.00012284803757983776\n",
      "train loss:0.00020933936023000705\n",
      "train loss:7.275103921446172e-05\n",
      "train loss:0.00016047282603946244\n",
      "train loss:4.111957669044736e-05\n",
      "train loss:0.00012000044397408654\n",
      "train loss:0.00011652326471838791\n",
      "train loss:0.00016940245501145275\n",
      "train loss:3.892146331293298e-05\n",
      "train loss:7.912862987689213e-05\n",
      "train loss:6.688950176805686e-05\n",
      "train loss:0.00025477823356792525\n",
      "train loss:5.156895585728306e-05\n",
      "train loss:0.00023588915063604444\n",
      "train loss:0.00014141646726228946\n",
      "train loss:0.00012308016816138225\n",
      "train loss:5.051711770627638e-05\n",
      "train loss:4.870927743676031e-05\n",
      "train loss:0.0002351326633383576\n",
      "train loss:0.000210106416340258\n",
      "train loss:0.0003017752462474713\n",
      "train loss:0.00019839508572903886\n",
      "train loss:2.6063393524190838e-05\n",
      "train loss:7.907148465847653e-05\n",
      "train loss:6.052223844039595e-05\n",
      "train loss:0.00014947793367756757\n",
      "train loss:4.239843478251773e-05\n",
      "train loss:0.00013714128970058854\n",
      "train loss:0.00011944526336612996\n",
      "train loss:4.3707482547913825e-05\n",
      "train loss:0.00018155539102996746\n",
      "train loss:0.00024415678506164415\n",
      "train loss:0.00016766971551626926\n",
      "train loss:9.073629675829298e-05\n",
      "train loss:8.49832287549634e-05\n",
      "train loss:0.00010135785154300149\n",
      "train loss:3.0111119866844474e-05\n",
      "train loss:0.0003059635418058182\n",
      "train loss:0.00016726854249077532\n",
      "train loss:0.00017481913544257512\n",
      "train loss:0.0002513920638224637\n",
      "train loss:0.00021872937944547382\n",
      "train loss:0.00024368306384427274\n",
      "train loss:6.539499808231977e-05\n",
      "train loss:0.00019172926158382144\n",
      "train loss:0.0001041334721824992\n",
      "train loss:0.00018946967965705362\n",
      "train loss:0.00012858259076350842\n",
      "train loss:0.00018084940675426142\n",
      "train loss:0.0003635946513807073\n",
      "train loss:0.0002794978512659397\n",
      "train loss:0.00012200371701800201\n",
      "train loss:0.0004056186475166345\n",
      "train loss:0.00014299025703749097\n",
      "train loss:0.00020361832424270381\n",
      "train loss:9.240544740620057e-05\n",
      "train loss:0.00012627265990235168\n",
      "train loss:3.133319103171201e-05\n",
      "train loss:6.944796385682979e-05\n",
      "train loss:0.00010020388854045768\n",
      "=== epoch:4, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.00024205531625283826\n",
      "train loss:4.5902055910975965e-05\n",
      "train loss:0.0001161961936881433\n",
      "train loss:0.0001125453103818607\n",
      "train loss:4.9551591308264526e-05\n",
      "train loss:0.000154448949431903\n",
      "train loss:0.00015885567667287497\n",
      "train loss:0.0001363382186290644\n",
      "train loss:0.0002501569386670891\n",
      "train loss:0.0002610002257132909\n",
      "train loss:6.729303633705004e-05\n",
      "train loss:0.0001476244957247969\n",
      "train loss:0.0003587616408675848\n",
      "train loss:0.00021341273090072044\n",
      "train loss:7.383458988362667e-05\n",
      "train loss:5.4231587993228315e-05\n",
      "train loss:8.327074708592574e-05\n",
      "train loss:0.0001820239557406252\n",
      "train loss:6.335593320918052e-05\n",
      "train loss:0.00033374505601385543\n",
      "train loss:0.00021217442954904526\n",
      "train loss:0.00017337736796574604\n",
      "train loss:0.0002456320865193101\n",
      "train loss:4.743141949876593e-05\n",
      "train loss:0.00018186850160015322\n",
      "train loss:0.00036351490819332545\n",
      "train loss:0.00011732339028268894\n",
      "train loss:0.00013007286038368775\n",
      "train loss:0.00020872436201401873\n",
      "train loss:8.024024974092838e-05\n",
      "train loss:1.8151047333573404e-05\n",
      "train loss:0.00010221874836447959\n",
      "train loss:0.00016653088791448761\n",
      "train loss:0.0001734453161812413\n",
      "train loss:0.0001883043728342522\n",
      "train loss:0.0001740638207716608\n",
      "train loss:0.00018381037276780693\n",
      "train loss:0.00019623678218797254\n",
      "train loss:0.00012004905757313826\n",
      "train loss:0.0005239290480849126\n",
      "train loss:0.00010203795855260319\n",
      "train loss:0.00012069949832678834\n",
      "train loss:0.00017774309530037052\n",
      "train loss:6.0152976102033934e-05\n",
      "train loss:2.6603547405392204e-05\n",
      "train loss:0.00019715271809213507\n",
      "train loss:8.793742028397042e-05\n",
      "train loss:5.495740820146096e-05\n",
      "train loss:0.00014327521953090547\n",
      "train loss:0.0002756768990547266\n",
      "train loss:0.00026904721387338814\n",
      "train loss:3.371351202397566e-05\n",
      "train loss:0.00013958020625406384\n",
      "train loss:0.0001289990863339831\n",
      "train loss:8.75233147587212e-05\n",
      "train loss:6.960099444892499e-05\n",
      "train loss:0.00028244383404367587\n",
      "train loss:0.00025809272244287665\n",
      "train loss:0.00011920010955632649\n",
      "train loss:0.000248412606404921\n",
      "train loss:9.44134492395073e-05\n",
      "train loss:0.0001246670130260287\n",
      "train loss:0.0001442442700005375\n",
      "train loss:0.00016729436926706296\n",
      "train loss:0.0001640335080115735\n",
      "train loss:0.0002682392724124995\n",
      "train loss:4.05762350956808e-05\n",
      "train loss:0.00014515240023153022\n",
      "train loss:0.00010879853672412075\n",
      "train loss:0.00015705868681996783\n",
      "train loss:0.00014582103276158267\n",
      "train loss:7.460422229675797e-05\n",
      "train loss:0.00015071522636879992\n",
      "train loss:0.0001241000886179867\n",
      "train loss:0.00014966617860077958\n",
      "train loss:0.00018669867348283586\n",
      "train loss:0.0001162290589067451\n",
      "train loss:0.000192757426459195\n",
      "train loss:0.00020766615846964893\n",
      "train loss:0.00023295228735891943\n",
      "=== epoch:5, train acc:1.0, test acc:1.0 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0002284319381257019\n",
      "train loss:9.01021114871531e-05\n",
      "train loss:0.00015430015864419032\n",
      "train loss:0.00013776704502850196\n",
      "train loss:0.0003829490755266555\n",
      "train loss:5.325806806074424e-05\n",
      "train loss:0.00035437788465959993\n",
      "train loss:4.697013621872962e-05\n",
      "train loss:6.360215987559489e-05\n",
      "train loss:0.000295576306562664\n",
      "train loss:9.280213562493471e-05\n",
      "train loss:0.00022031409567991667\n",
      "train loss:3.5024483768231206e-05\n",
      "train loss:7.61490393657351e-05\n",
      "train loss:0.00013027376795683192\n",
      "train loss:0.00026413016259154787\n",
      "train loss:0.00025235197093786754\n",
      "train loss:7.141484685649827e-05\n",
      "train loss:0.00013252167304809244\n",
      "train loss:8.639432226094135e-05\n",
      "train loss:0.0002587578376280601\n",
      "train loss:0.00011139633367907875\n",
      "train loss:5.708426795058695e-05\n",
      "train loss:0.0002638687936172251\n",
      "train loss:0.0001034287682334157\n",
      "train loss:0.00022424377773983773\n",
      "train loss:8.444628714155421e-05\n",
      "train loss:3.5227971876707896e-05\n",
      "train loss:0.0002670378252755927\n",
      "train loss:0.00028223425231098544\n",
      "train loss:0.00016713309675368772\n",
      "train loss:0.0002267040322089463\n",
      "train loss:0.00010773281339194228\n",
      "train loss:0.0001329443130711769\n",
      "train loss:0.00018418633249326355\n",
      "train loss:0.0001802505965169461\n",
      "train loss:0.00012072406799559851\n",
      "train loss:0.00012153337987368396\n",
      "train loss:7.636352351790734e-05\n",
      "train loss:0.0002042800040491021\n",
      "train loss:9.782896582639308e-05\n",
      "train loss:0.0003077193172208234\n",
      "train loss:0.0001233093643794174\n",
      "train loss:9.246885020117942e-05\n",
      "train loss:0.0003306768851886632\n",
      "train loss:2.5941025211639286e-05\n",
      "train loss:0.00023753100395848038\n",
      "train loss:9.176109559135314e-05\n",
      "train loss:0.0003435885380913753\n",
      "train loss:0.00017520519098106871\n",
      "train loss:0.00015888422069479006\n",
      "train loss:0.0003129541637334479\n",
      "train loss:0.00010791801261142722\n",
      "train loss:5.6240558456257085e-05\n",
      "train loss:0.0001888302280692215\n",
      "train loss:8.545072171989281e-05\n",
      "train loss:9.76190740417384e-05\n",
      "train loss:9.155813344112911e-05\n",
      "train loss:8.698780776881504e-05\n",
      "train loss:0.00036822307258747996\n",
      "train loss:0.00011926506721134919\n",
      "train loss:0.0001374134134417505\n",
      "train loss:0.00010587057302290239\n",
      "train loss:0.0002748149579286967\n",
      "train loss:9.944279368678017e-05\n",
      "train loss:0.00011654628576545815\n",
      "train loss:3.308626418621659e-05\n",
      "train loss:0.0002663904999484549\n",
      "train loss:0.0001939794689036551\n",
      "train loss:0.00010095350346080826\n",
      "train loss:3.2563166681234644e-05\n",
      "train loss:5.7875193026714794e-05\n",
      "train loss:0.0001298362555225327\n",
      "train loss:9.968268089927057e-05\n",
      "train loss:8.877839526103356e-05\n",
      "train loss:3.843537947790349e-05\n",
      "train loss:0.00013108700923417536\n",
      "train loss:6.857505186203313e-05\n",
      "train loss:0.0001697003410783079\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:1.0\n"
     ]
    }
   ],
   "source": [
    "x_train=x_train_9\n",
    "x_test = x_test_9\n",
    "t_train=t_train_9\n",
    "t_test = t_test_9\n",
    "\n",
    "trainer_SGD = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='SGD', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_MMT = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Momentum', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer_SGD.train()\n",
    "trainer_MMT.train()\n",
    "\n",
    "MMT_acc.append(trainer_MMT.test_acc_list[-1])\n",
    "SGD_acc.append(trainer_SGD.test_acc_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.974, 0.986, 0.996, 0.999, 0.999, 0.999, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MMT_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.923, 0.99, 0.997, 0.999, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
